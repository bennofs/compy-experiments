
@article{yao_learning_2020,
	title = {Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks},
	url = {http://arxiv.org/abs/2003.07959v3},
	abstract = {Verifying real-world programs often requires inferring loop invariants with nonlinear constraints. This is especially true in programs that perform many numerical operations, such as control systems for avionics or industrial plants. Recently, data-driven methods for loop invariant inference have shown promise, especially on linear invariants. However, applying data-driven inference to nonlinear loop invariants is challenging due to the large numbers of and magnitudes of high-order terms, the potential for overfitting on a small number of samples, and the large space of possible inequality bounds. In this paper, we introduce a new neural architecture for general {SMT} learning, the Gated Continuous Logic Network (G-{CLN}), and apply it to nonlinear loop invariant learning. G-{CLNs} extend the Continuous Logic Network ({CLN}) architecture with gating units and dropout, which allow the model to robustly learn general invariants over large numbers of terms. To address overfitting that arises from finite program sampling, we introduce fractional sampling—a sound relaxation of loop semantics to continuous functions that facilitates unbounded sampling on real domain. We additionally design a new {CLN} activation function, the Piecewise Biased Quadratic Unit ({PBQU}), for naturally learning tight inequality bounds. We incorporate these methods into a nonlinear loop invariant inference system that can learn general nonlinear loop invariants. We evaluate our system on a benchmark of nonlinear loop invariants and show it solves 26 out of 27 problems, 3 more than prior work, with an average runtime of 53.3 seconds. We further demonstrate the generic learning ability of G-{CLNs} by solving all 124 problems in the linear Code2Inv benchmark. We also perform a quantitative stability evaluation and show G-{CLNs} have a convergence rate of \$97.5\%\$ on quadratic problems, a \$39.2\%\$ improvement over {CLN} models.},
	journaltitle = {{CoRR}},
	author = {Yao, Jianan and Ryan, Gabriel and Wong, Justin and Jana, Suman and Gu, Ronghui},
	date = {2020},
	note = {\_eprint: 2003.07959v3},
	keywords = {read, ⛔ No {DOI} found, \_tablet},
	file = {Yao et al_2020_Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks.pdf:/data/zotero/storage/P9ET2BJF/Yao et al_2020_Learning Nonlinear Loop Invariants With Gated Continuous Logic Networks.pdf:application/pdf}
}

@inproceedings{menendez_alive-infer_2017,
	title = {Alive-Infer: data-driven precondition inference for peephole optimizations in {LLVM}},
	url = {https://doi.org/10.1145/3062341.3062372},
	doi = {10/gg3j54},
	abstract = {Peephole optimizations are a common source of compiler bugs. Compiler developers typically transform an incorrect peephole optimization into a valid one by strengthening the precondition. This process is challenging and tedious. This paper proposes Alive-Infer, a data-driven approach that infers preconditions for peephole optimizations expressed in Alive. Alive-Infer generates positive and negative examples for an optimization, enumerates predicates on-demand, and learns a set of predicates that separate the positive and negative examples. Alive-Infer repeats this process until it finds a precondition that ensures the validity of the optimization. Alive-Infer reports both a weakest precondition and a set of succinct partial preconditions to the developer. Our prototype generates preconditions that are weaker than {LLVM}’s preconditions for 73 optimizations in the Alive suite. We also demonstrate the applicability of this technique to generalize 54 optimization patterns generated by Souper, an {LLVM} {IR}–based superoptimizer.},
	pages = {nil},
	booktitle = {Proceedings of the 38th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation - {PLDI} 2017},
	author = {Menendez, David and Nagarakatte, Santosh},
	date = {2017},
	keywords = {read},
	file = {Menendez_Nagarakatte_2017_Alive-Infer.pdf:/data/zotero/storage/TGNNP2VB/Menendez_Nagarakatte_2017_Alive-Infer.pdf:application/pdf}
}

@article{raychev_probabilistic_2016,
	title = {Probabilistic model for code with decision trees},
	url = {http://dx.doi.org/10.1145/2983990.2984041},
	doi = {10/gg3j67},
	abstract = {In this paper we introduce a new approach for learning precise and general probabilistic models of code based on decision tree learning. Our approach directly benefits an emerging class of statistical programming tools which leverage probabilistic models of code learned over large codebases (e.g., {GitHub}) to make predictions about new programs (e.g., code completion, repair, etc).

The key idea is to phrase the problem of learning a probabilistic model of code as learning a decision tree in a domain specific language over abstract syntax trees (called {TGen}). This allows us to condition the prediction of a program element on a dynamically computed context. Further, our problem formulation enables us to easily instantiate known decision tree learning algorithms such as {ID}3, but also to obtain new variants we refer to as {ID}3+ and E13, not previously explored and ones that outperform {ID}3 in prediction accuracy.

Our approach is general and can be used to learn a probabilistic model of any programming language. We implemented our approach in a system called Deep3 and evaluated it for the challenging task of learning probabilistic models of {JavaScript} and Python. Our experimental results indicate that Deep3 predicts elements of {JavaScript} and Python code with precision above 82\% and 69\%, respectively. Further, Deep3 often significantly outperforms state-of-the-art approaches in overall prediction accuracy.},
	journaltitle = {Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Object-Oriented Programming, Systems, Languages, and Applications - {OOPSLA} 2016},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin},
	date = {2016},
	note = {{ISBN}: 9781450344449
Publisher: {ACM} Press},
	keywords = {to-read},
	file = {Raychev et al_2016_Probabilistic model for code with decision trees.pdf:/data/zotero/storage/FHCA5GTY/Raychev et al_2016_Probabilistic model for code with decision trees.pdf:application/pdf}
}

@article{raychev_predicting_2019,
	title = {Predicting program properties from “big code”},
	volume = {62},
	issn = {1557-7317},
	url = {http://dx.doi.org/10.1145/3306204},
	doi = {10/gg3j66},
	pages = {99--107},
	number = {3},
	journaltitle = {Communications of the {ACM}},
	author = {Raychev, Veselin and Vechev, Martin and Krause, Andreas},
	date = {2019-02},
	note = {Publisher: Association for Computing Machinery ({ACM})},
	keywords = {read, \_tablet},
	file = {Raychev et al_2019_Predicting program properties from “big code”.pdf:/data/zotero/storage/VQRPALTZ/Raychev et al_2019_Predicting program properties from “big code”.pdf:application/pdf}
}

@article{allamanis_typilus_2020,
	title = {Typilus: Neural Type Hints},
	volume = {abs/2004.10657},
	url = {https://arxiv.org/abs/2004.10657},
	abstract = {Type inference over partial contexts in dynamically typed languages is challenging. In this work, we present a graph neural network model that predicts types by probabilistically reasoning over a program's structure, names, and patterns. The network uses deep similarity learning to learn a {TypeSpace} -- a continuous relaxation of the discrete space of types -- and how to embed the type properties of a symbol (i.e. identifier) into it. Importantly, our model can employ one-shot learning to predict an open vocabulary of types, including rare and user-defined ones. We realise our approach in Typilus for Python that combines the {TypeSpace} with an optional type checker. We show that Typilus accurately predicts types. Typilus confidently predicts types for 70\% of all annotatable symbols; when it predicts a type, that type optionally type checks 95\% of the time. Typilus can also find incorrect type annotations; two important and popular open source libraries, fairseq and allennlp, accepted our pull requests that fixed the annotation errors Typilus discovered.},
	journaltitle = {{CoRR}},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Ducousso, Soline and Gao, Zheng},
	date = {2020},
	note = {\_eprint: 2004.10657},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Allamanis et al_2020_Typilus.pdf:/data/zotero/storage/HBN6BK8X/Allamanis et al_2020_Typilus.pdf:application/pdf}
}

@article{ernst_daikon_2007,
	title = {The Daikon system for dynamic detection of likely invariants},
	volume = {69},
	doi = {10/drc63v},
	pages = {35--45},
	number = {1},
	journaltitle = {Science of computer programming},
	author = {Ernst, Michael D. and Perkins, Jeff H. and Guo, Philip J. and {McCamant}, Stephen and Pacheco, Carlos and Tschantz, Matthew S. and Xiao, Chen},
	date = {2007},
	note = {Publisher: Elsevier},
	file = {Ernst et al_2007_The Daikon system for dynamic detection of likely invariants.pdf:/data/zotero/storage/3CP3ANAY/Ernst et al_2007_The Daikon system for dynamic detection of likely invariants.pdf:application/pdf}
}

@inproceedings{ernst_finding_2004,
	title = {Finding latent code errors via machine learning over program executions},
	doi = {10/dndgmg},
	abstract = {This paper proposes a technique for identifying program properties that indicate errors. The technique generates machine learning models of program properties known to result from errors, and applies these models to program properties of user-written code to classify and rank properties that may lead the user to errors. Given a set of properties produced by the program analysis, the technique selects a subset of properties that are most likely to reveal an error. An implementation, the fault invariant classifier, demonstrates the efficacy of the technique. The implementation uses dynamic invariant detection to generate program properties. It uses support vector machine and decision tree learning tools to classify those properties. In our experimental evaluation, the technique increases the relevance (the concentration of fault-revealing properties) by a factor of 50 on average for the C programs, and 4.8 for the Java programs. Preliminary experience suggests that most of the fault-revealing properties do lead a programmer to an error.},
	eventtitle = {Proceedings. 26th International Conference on Software Engineering},
	pages = {480--490},
	booktitle = {Proceedings. 26th International Conference on Software Engineering},
	author = {Ernst, M.D and Brun, Yuriy},
	date = {2004-05},
	note = {{ISSN}: 0270-5257},
	keywords = {skimmed},
	file = {Ernst_Brun_2004_Finding latent code errors via machine learning over program executions.pdf:/data/zotero/storage/8TARWUHP/Ernst_Brun_2004_Finding latent code errors via machine learning over program executions.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/FNM2EDXY/1317470.html:text/html}
}

@article{alon_general_2018,
	title = {A general path-based representation for predicting program properties},
	volume = {53},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3296979.3192412},
	doi = {10/gg3j5v},
	abstract = {Predicting program properties such as names or expression types has a wide range of applications. It can ease the task of programming, and increase programmer productivity. A major challenge when learning from programs is how to represent programs in a way that facilitates effective learning. We present a general path-based representation for learning from programs. Our representation is purely syntactic and extracted automatically. The main idea is to represent a program using paths in its abstract syntax tree ({AST}). This allows a learning model to leverage the structured nature of code rather than treating it as a flat sequence of tokens. We show that this representation is general and can: (i) cover different prediction tasks, (ii) drive different learning algorithms (for both generative and discriminative models), and (iii) work across different programming languages. We evaluate our approach on the tasks of predicting variable names, method names, and full types. We use our representation to drive both {CRF}-based and word2vec-based learning, for programs of four languages: {JavaScript}, Java, Python and C\#. Our evaluation shows that our approach obtains better results than task-specific handcrafted representations across different tasks and programming languages.},
	pages = {404--419},
	number = {4},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-01},
	date = {2018-06-11},
	keywords = {read},
	file = {Full Text PDF:/data/zotero/storage/JSCL3N3W/Alon et al. - 2018 - A general path-based representation for predicting.pdf:application/pdf}
}

@inproceedings{yahav_programs_2018,
	location = {Cham},
	title = {From Programs to Interpretable Deep Models and Back},
	isbn = {978-3-319-96145-3},
	doi = {10/gg3j6m},
	series = {Lecture Notes in Computer Science},
	abstract = {We demonstrate how deep learning over programs is used to provide (preliminary) augmented programmer intelligence. In the first part, we show how to tackle tasks like code completion, code summarization, and captioning. We describe a general path-based representation of source code that can be used across programming languages and learning tasks, and discuss how this representation enables different learning algorithms. In the second part, we describe techniques for extracting interpretable representations from deep models, shedding light on what has actually been learned in various tasks.},
	pages = {27--37},
	booktitle = {Computer Aided Verification},
	publisher = {Springer International Publishing},
	author = {Yahav, Eran},
	editor = {Chockler, Hana and Weissenbacher, Georg},
	date = {2018},
	langid = {english},
	keywords = {to-read},
	file = {Yahav_2018_From Programs to Interpretable Deep Models and Back.pdf:/data/zotero/storage/KHN4TCVY/Yahav_2018_From Programs to Interpretable Deep Models and Back.pdf:application/pdf}
}

@article{alon_code2vec_2019,
	title = {code2vec: learning distributed representations of code},
	volume = {3},
	url = {https://doi.org/10.1145/3290353},
	doi = {10/ggssk3},
	shorttitle = {code2vec},
	abstract = {We present a neural model for representing snippets of code as continuous distributed vectors (``code embeddings''). The main idea is to represent a code snippet as a single fixed-length code vector, which can be used to predict semantic properties of the snippet. To this end, code is first decomposed to a collection of paths in its abstract syntax tree. Then, the network learns the atomic representation of each path while simultaneously learning how to aggregate a set of them. We demonstrate the effectiveness of our approach by using it to predict a method's name from the vector representation of its body. We evaluate our approach by training a model on a dataset of 12M methods. We show that code vectors trained on this dataset can predict method names from files that were unobserved during training. Furthermore, we show that our model learns useful method name vectors that capture semantic similarities, combinations, and analogies. A comparison of our approach to previous techniques over the same dataset shows an improvement of more than 75\%, making it the first to successfully predict method names based on a large, cross-project corpus. Our trained model, visualizations and vector similarities are available as an interactive online demo at http://code2vec.org. The code, data and trained models are available at https://github.com/tech-srl/code2vec.},
	pages = {40:1--40:29},
	issue = {{POPL}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Alon, Uri and Zilberstein, Meital and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-01},
	date = {2019-01-02},
	keywords = {read},
	file = {Alon et al_2019_code2vec.pdf:/data/zotero/storage/MAVMSVUJ/Alon et al_2019_code2vec.pdf:application/pdf}
}

@article{pradel_deepbugs_2018,
	title = {{DeepBugs}: a learning approach to name-based bug detection},
	volume = {2},
	url = {https://doi.org/10.1145/3276517},
	doi = {10/ggwxh2},
	shorttitle = {{DeepBugs}},
	abstract = {Natural language elements in source code, e.g., the names of variables and functions, convey useful information. However, most existing bug detection tools ignore this information and therefore miss some classes of bugs. The few existing name-based bug detection approaches reason about names on a syntactic level and rely on manually designed and tuned algorithms to detect bugs. This paper presents {DeepBugs}, a learning approach to name-based bug detection, which reasons about names based on a semantic representation and which automatically learns bug detectors instead of manually writing them. We formulate bug detection as a binary classification problem and train a classifier that distinguishes correct from incorrect code. To address the challenge that effectively learning a bug detector requires examples of both correct and incorrect code, we create likely incorrect code examples from an existing corpus of code through simple code transformations. A novel insight learned from our work is that learning from artificially seeded bugs yields bug detectors that are effective at finding bugs in real-world code. We implement our idea into a framework for learning-based and name-based bug detection. Three bug detectors built on top of the framework detect accidentally swapped function arguments, incorrect binary operators, and incorrect operands in binary operations. Applying the approach to a corpus of 150,000 {JavaScript} files yields bug detectors that have a high accuracy (between 89\% and 95\%), are very efficient (less than 20 milliseconds per analyzed file), and reveal 102 programming mistakes (with 68\% true positive rate) in real-world code.},
	pages = {147:1--147:25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Pradel, Michael and Sen, Koushik},
	urldate = {2020-06-01},
	date = {2018-10-24},
	keywords = {to-read},
	file = {Pradel_Sen_2018_DeepBugs.pdf:/data/zotero/storage/SRKBDBHS/Pradel_Sen_2018_DeepBugs.pdf:application/pdf}
}

@article{pandi_opttyper_2020,
	title = {{OptTyper}: Probabilistic Type Inference by Optimising Logical and Natural Constraints},
	url = {http://arxiv.org/abs/2004.00348},
	shorttitle = {{OptTyper}},
	abstract = {We present a new approach to the type inference problem for dynamic languages. Our goal is to combine logical constraints, that is, deterministic information from a type system, with natural constraints, that is, uncertain statistical information about types learnt from sources like identifier names. To this end, we introduce a framework for probabilistic type inference that combines logic and learning: logical constraints on the types are extracted from the program, and deep learning is applied to predict types from surface-level code properties that are statistically associated, such as variable names. The foremost insight of our method is to constrain the predictions from the learning procedure to respect the logical constraints, which we achieve by relaxing the logical inference problem of type prediction into a continuous optimisation problem. As proof of concept, we build a tool called {OptTyper} to predict missing types for {TypeScript} files. {OptTyper} combines a continuous interpretation of logical constraints derived by a simple program transformation and static analysis of {TypeScript} code, with natural constraints obtained from a deep learning model, which learns naming conventions for types from a large codebase. By evaluating {OptTyper}, we show that the combination of logical and natural constraints yields a large improvement in performance over either kind of information individually and achieves a 3\% improvement over the state-of-the-art.},
	journaltitle = {{arXiv}:2004.00348 [cs]},
	author = {Pandi, Irene Vlassi and Barr, Earl T. and Gordon, Andrew D. and Sutton, Charles},
	urldate = {2020-06-01},
	date = {2020-05-18},
	eprinttype = {arxiv},
	eprint = {2004.00348},
	keywords = {read, ⛔ No {DOI} found},
	file = {Pandi et al_2020_OptTyper.pdf:/data/zotero/storage/B8IUQGKV/Pandi et al_2020_OptTyper.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/RE2WZG54/2004.html:text/html}
}

@article{wei_lambdanet_2020,
	title = {{LambdaNet}: Probabilistic Type Inference using Graph Neural Networks},
	url = {http://arxiv.org/abs/2005.02161},
	shorttitle = {{LambdaNet}},
	abstract = {As gradual typing becomes increasingly popular in languages like Python and {TypeScript}, there is a growing need to infer type annotations automatically. While type annotations help with tasks like code completion and static error catching, these annotations cannot be fully determined by compilers and are tedious to annotate by hand. This paper proposes a probabilistic type inference scheme for {TypeScript} based on a graph neural network. Our approach first uses lightweight source code analysis to generate a program abstraction called a type dependency graph, which links type variables with logical constraints as well as name and usage information. Given this program abstraction, we then use a graph neural network to propagate information between related type variables and eventually make type predictions. Our neural architecture can predict both standard types, like number or string, as well as user-defined types that have not been encountered during training. Our experimental results show that our approach outperforms prior work in this space by \$14{\textbackslash}\%\$ (absolute) on library types, while having the ability to make type predictions that are out of scope for existing techniques.},
	journaltitle = {{arXiv}:2005.02161 [cs, stat]},
	author = {Wei, Jiayi and Goyal, Maruth and Durrett, Greg and Dillig, Isil},
	urldate = {2020-06-01},
	date = {2020-04-29},
	eprinttype = {arxiv},
	eprint = {2005.02161},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Wei et al_2020_LambdaNet.pdf:/data/zotero/storage/Y8DBPV9C/Wei et al_2020_LambdaNet.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/T687N76K/2005.html:text/html}
}

@article{pradel_typewriter_2020,
	title = {{TypeWriter}: Neural Type Prediction with Search-based Validation},
	url = {http://arxiv.org/abs/1912.03768},
	shorttitle = {{TypeWriter}},
	abstract = {Maintaining large code bases written in dynamically typed languages, such as {JavaScript} or Python, can be challenging due to the absence of type annotations: simple data compatibility errors proliferate, {IDE} support is limited, and {APIs} are hard to comprehend. Recent work attempts to address those issues through either static type inference or probabilistic type prediction. Unfortunately, static type inference for dynamic languages is inherently limited, while probabilistic approaches suffer from imprecision. This paper presents {TypeWriter}, the first combination of probabilistic type prediction with search-based refinement of predicted types. {TypeWriter}'s predictor learns to infer the return and argument types for functions from partially annotated code bases by combining the natural language properties of code with programming language-level information. To validate predicted types, {TypeWriter} invokes a gradual type checker with different combinations of the predicted types, while navigating the space of possible type combinations in a feedback-directed manner. We implement the {TypeWriter} approach for Python and evaluate it on two code corpora: a multi-million line code base at Facebook and a collection of 1,137 popular open-source projects. We show that {TypeWriter}'s type predictor achieves an F1 score of 0.64 (0.79) in the top-1 (top-5) predictions for return types, and 0.57 (0.80) for argument types, which clearly outperforms prior type prediction models. By combining predictions with search-based validation, {TypeWriter} can fully annotate between 14\% to 44\% of the files in a randomly selected corpus, while ensuring type correctness. A comparison with a static type inference tool shows that {TypeWriter} adds many more non-trivial types. {TypeWriter} currently suggests types to developers at Facebook and several thousands of types have already been accepted with minimal changes.},
	journaltitle = {{arXiv}:1912.03768 [cs]},
	author = {Pradel, Michael and Gousios, Georgios and Liu, Jason and Chandra, Satish},
	urldate = {2020-06-01},
	date = {2020-03-06},
	eprinttype = {arxiv},
	eprint = {1912.03768},
	keywords = {read, ⛔ No {DOI} found},
	file = {Pradel et al_2020_TypeWriter.pdf:/data/zotero/storage/4G6NT96X/Pradel et al_2020_TypeWriter.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/LEGIJE7F/1912.html:text/html}
}

@article{allamanis_learning_2018,
	title = {Learning to Represent Programs with Graphs},
	abstract = {Learning tasks on source code (i.e., formal languages) have been considered recently, but most work has tried to transfer natural language methods and does not capitalize on the unique opportunities offered by code's known syntax. For example, long-range dependencies induced by using the same variable or function in distant locations are often not considered. We propose to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. 
In this work, we present how to construct graphs from source code and how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: {VarNaming}, in which a network attempts to predict the name of a variable given its usage, and {VarMisuse}, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the {VarMisuse} task in many cases. Additionally, our testing showed that {VarMisuse} identifies a number of bugs in mature open-source projects.},
	journaltitle = {{ICLR}},
	author = {Allamanis, Miltiadis and Brockschmidt, Marc and Khademi, Mahmoud},
	date = {2018},
	keywords = {read, mpnn, ⛔ No {DOI} found, \_tablet},
	file = {Allamanis et al_2018_Learning to Represent Programs with Graphs.pdf:/data/zotero/storage/4CMFCKLE/Allamanis et al_2018_Learning to Represent Programs with Graphs.pdf:application/pdf}
}

@article{cummins_programl_2020,
	title = {{ProGraML}: Graph-based Deep Learning for Program Optimization and Analysis},
	shorttitle = {{ProGraML}},
	abstract = {The increasing complexity of computing systems places a tremendous burden on optimizing compilers, requiring ever more accurate and aggressive optimizations. Machine learning offers significant benefits for constructing optimization heuristics but there remains a gap between what state-of-the-art methods achieve and the performance of an optimal heuristic. Closing this gap requires improvements in two key areas: a representation that accurately captures the semantics of programs, and a model architecture with sufficient expressiveness to reason about this representation. 
We introduce {ProGraML} - Program Graphs for Machine Learning - a novel graph-based program representation using a low level, language agnostic, and portable format; and machine learning models capable of performing complex downstream tasks over these graphs. The {ProGraML} representation is a directed attributed multigraph that captures control, data, and call relations, and summarizes instruction and operand types and ordering. Message Passing Neural Networks propagate information through this structured representation, enabling whole-program or per-vertex classification tasks. 
{ProGraML} provides a general-purpose program representation that equips learnable models to perform the types of program analysis that are fundamental to optimization. To this end, we evaluate the performance of our approach first on a suite of traditional compiler analysis tasks: control flow reachability, dominator trees, data dependencies, variable liveness, and common subexpression detection. On a benchmark dataset of 250k {LLVM}-{IR} files covering six source programming languages, {ProGraML} achieves an average 94.0 F1 score, significantly outperforming the state-of-the-art approaches. We then apply our approach to two high-level tasks - heterogeneous device mapping and program classification - setting new state-of-the-art performance in both.},
	journaltitle = {{ArXiv}},
	author = {Cummins, Chris and Fisches, Zacharias V. and Ben-Nun, Tal and Hoefler, Torsten and Leather, Hugh},
	date = {2020},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Cummins et al_2020_ProGraML.pdf:/data/zotero/storage/M3PXDI69/Cummins et al_2020_ProGraML.pdf:application/pdf}
}

@article{brauckmann_compiler-based_2020,
	title = {Compiler-based graph representations for deep learning models of code},
	doi = {10/gg3j57},
	abstract = {In natural language processing, novel methods in deep learning, like recurrent neural networks ({RNNs}) on sequences of words, have been very successful. In contrast to natural languages, programming languages usually have a well-defined structure. With this structure compilers can reason about programs, using graphs such as abstract syntax trees ({ASTs}) or control-data flow graphs ({CDFGs}). In this paper, we argue that we should use these graph structures instead of sequences for learning compiler optimization tasks. To this end, we use graph neural networks ({GNNs}) for learning predictive compiler tasks on two representations based on {ASTs} and {CDFGs}. Experiments show that this improves upon the state-of-the-art in the task of heterogeneous {OpenCL} mapping, while providing orders of magnitude faster inference times, crucial for compiler optimizations. When testing on benchmark suites not included for training, our {AST}-based model significantly outperforms the state-of-the-art by over 12 percentage points in terms of accuracy. It is the only one to perform clearly better than a random mapping. On the task of predicting thread coarsening factors, we show that all of the methods fail to produce an overall speedup.},
	journaltitle = {{CC}},
	author = {Brauckmann, Alexander and Goens, Andrés and Ertel, Sebastian and Castrillón, Jerónimo},
	date = {2020},
	keywords = {read, mpnn},
	file = {Brauckmann et al_2020_Compiler-based graph representations for deep learning models of code.pdf:/data/zotero/storage/R4DGA26B/Brauckmann et al_2020_Compiler-based graph representations for deep learning models of code.pdf:application/pdf}
}

@article{wang_learning_2019,
	title = {Learning Scalable and Precise Representation of Program Semantics},
	abstract = {Neural program embedding has shown potential in aiding the analysis of large-scale, complicated software. Newly proposed deep neural architectures pride themselves on learning program semantics rather than superficial syntactic features. However, by considering the source code only, the vast majority of neural networks do not capture a deep, precise representation of program semantics. In this paper, we present {\textbackslash}dypro, a novel deep neural network that learns from program execution traces. Compared to the prior dynamic models, not only is {\textbackslash}dypro capable of generalizing across multiple executions for learning a program's dynamic semantics in its entirety, but {\textbackslash}dypro is also more efficient when dealing with programs yielding long execution traces. For evaluation, we task {\textbackslash}dypro with semantic classification (i.e. categorizing programs based on their semantics) and compared it against two prominent static models: Gated Graph Neural Network and {TreeLSTM}. We find that {\textbackslash}dypro achieves the highest prediction accuracy among all models. To further reveal the capacity of all aforementioned deep neural architectures, we examine if the models can learn to detect deeper semantic properties of a program. In particular given a task of recognizing loop invariants, we show {\textbackslash}dypro beats all static models by a wide margin.},
	journaltitle = {{ArXiv}},
	author = {Wang, Ke},
	date = {2019},
	keywords = {read, ⛔ No {DOI} found},
	file = {Wang_2019_Learning Scalable and Precise Representation of Program Semantics.pdf:/data/zotero/storage/JMJKHA7D/Wang_2019_Learning Scalable and Precise Representation of Program Semantics.pdf:application/pdf}
}

@article{hellendoorn_are_2019,
	title = {Are My Invariants Valid? A Learning Approach},
	shorttitle = {Are My Invariants Valid?},
	abstract = {Ensuring that a program operates correctly is a difficult task in large, complex systems. Enshrining invariants -- desired properties of correct execution -- in code or comments can support maintainability and help sustain correctness. Tools that can automatically infer and recommend invariants can thus be very beneficial. However, current invariant-suggesting tools, such as Daikon, suffer from high rates of false positives, in part because they only leverage traced program values from available test cases, rather than directly exploiting knowledge of the source code per se. We propose a machine-learning approach to judging the validity of invariants, specifically of method pre- and post-conditions, based directly on a method's source code. We introduce a new, scalable approach to creating labeled invariants: using programs with large test-suites, we generate Daikon invariants using traces from subsets of these test-suites, and then label these as valid/invalid by cross-validating them with held-out tests. This process induces a large set of labels that provide a form of noisy supervision, which is then used to train a deep neural model, based on gated graph neural networks. Our model learns to map the lexical, syntactic, and semantic structure of a given method's body into a probability that a candidate pre- or post-condition on that method's body is correct and is able to accurately label invariants based on the noisy signal, even in cross-project settings. Most importantly, it performs well on a hand-curated dataset of invariants.},
	journaltitle = {{ArXiv}},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar T. and Polozov, Oleksandr and Marron, Mark},
	date = {2019},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Hellendoorn et al_2019_Are My Invariants Valid.pdf:/data/zotero/storage/AMFS9RJI/Hellendoorn et al_2019_Are My Invariants Valid.pdf:application/pdf}
}

@article{ahmed_learning_2019,
	title = {Learning Lenient Parsing \& Typing via Indirect Supervision},
	abstract = {Both professional coders and teachers frequently deal with imperfect (fragmentary, incomplete, ill-formed) code. Such fragments are common in {StackOverflow}; students also frequently produce ill-formed code, for which instructors, {TAs} (or students themselves) must find repairs. In either case, the developer experience could be greatly improved if such code could somehow be parsed \& typed; this makes them more amenable to use within {IDEs} and allows early detection and repair of potential errors. We introduce a lenient parser, which can parse \& type fragments, even ones with simple errors. Training a machine learner to leniently parse \& type imperfect code requires a large training set of pairs of imperfect code and its repair (and/or type information); such training sets are limited by human effort and curation. In this paper, we present a novel indirectly supervised approach to train a lenient parser, without access to such human-curated training data. We leverage the huge corpus of mostly correct code available on Github, and the massive, efficient learning capacity of Transformer-based {NN} architectures. Using {GitHub} data, we first create a large dataset of fragments of code and corresponding tree fragments and type annotations; we then randomly corrupt the input fragments (while requiring correct output) by seeding errors that mimic corruptions found in {StackOverflow} and student data. Using this data, we train high-capacity transformer models to overcome both fragmentation and corruption. With this novel approach, we can achieve reasonable performance on parsing \& typing {StackOverflow} fragments; we also demonstrate that our approach achieves best-in-class performance on a large dataset of student errors.},
	journaltitle = {{ArXiv}},
	author = {Ahmed, Toufique and Hellendoorn, Vincent J. and Devanbu, Premkumar T.},
	date = {2019},
	keywords = {⛔ No {DOI} found},
	file = {Full Text PDF:/data/zotero/storage/FYQSWIE5/Ahmed et al. - 2019 - Learning Lenient Parsing & Typing via Indirect Sup.pdf:application/pdf}
}

@inproceedings{xu_learning_2017,
	title = {Learning Types for Binaries},
	doi = {10/gg3j62},
	abstract = {Type inference for Binary codes is a challenging problem due partly to the fact that much type-related information has been lost during the compilation from high-level source code. Most of the existing research on binary code type inference tend to resort to program analysis techniques, which can be too conservative to infer types with high accuracy or too heavy-weight to be viable in practice. In this paper, we propose a new approach to learning types for recovered variables from their related representative instructions. Our idea is motivated by “duck typing”, where the type of a variable is determined by its features and properties. Our approach first learns a classifier from existing binaries with debug information and then uses this classifier to predict types for new, unseen binaries. We have implemented our approach in a tool called {BITY} and used it to conduct some experiments on a well-known benchmark coreutils (v8.4). The results show that our tool is more precise than the commercial tool Hey-Rays, both in terms of correct types and compatible types.},
	booktitle = {{ICFEM}},
	author = {Xu, Zhiwu and Wen, Cheng and Qin, Shengchao},
	date = {2017},
	file = {Submitted Version:/data/zotero/storage/FM7URSXZ/Xu et al. - 2017 - Learning Types for Binaries.pdf:application/pdf}
}

@article{xu_type_2019,
	title = {Type Learning for Binaries and Its Applications},
	doi = {10/gg3j7h},
	abstract = {Binary type inference is a challenging problem due partly to the fact that during the compilation much type-related information has been lost. Most existing research work resorts to program analysis techniques, which can be either too heavyweight to be viable in practice or too conservative to be able to infer types with high accuracy. In this paper, we propose a new approach to learning types for binary code. Motivated by “duck typing,” our approach learn types for recovered variables from their features and properties (e.g., related representative instructions). We first use machine learning to train a classifier with basic types as its levels from binaries with debugging information. The classifier is then used to learn types for new and unseen binaries. While for composite types, such as {\textless}inline-formula{\textgreater}{\textless}tex-math notation="{LaTeX}"{\textgreater}\$\{pointer\}\${\textless}/tex-math{\textgreater}{\textless}/inline-formula{\textgreater} and {\textless}inline-formula{\textgreater}{\textless}tex-math notation="{LaTeX}"{\textgreater}\$\{struct\}\${\textless}/tex-math{\textgreater}{\textless}/inline-formula{\textgreater}, a points-to analysis is performed. Finally, several experiments are conducted to evaluate our approach. The results demonstrate that our approach is more precise, both in terms of correct types and compatible types, than the commercial tool Hex-Rays, the open source tool Snowman, and a recent tool {EKLAVYA} using machine learning. We also show that the type information our proposed system learns is capable of helping detect malware.},
	journaltitle = {{IEEE} Transactions on Reliability},
	author = {Xu, Zhiwu and Wen, Cheng and Qin, Shengchao},
	date = {2019},
	file = {Submitted Version:/data/zotero/storage/DGVUPWZ4/Xu et al. - 2019 - Type Learning for Binaries and Its Applications.pdf:application/pdf}
}

@article{venkatakeerthy_ir2vec_2019,
	title = {{IR}2Vec: A Flow Analysis based Scalable Infrastructure for Program Encodings},
	shorttitle = {{IR}2Vec},
	abstract = {We propose {IR}2Vec, a Concise and Scalable encoding infrastructure to represent programs as a distributed embedding in continuous space. This distributed embedding is obtained by combining representation learning methods with data and control flow information to capture the syntax as well as the semantics of the input programs. 
Our embeddings are obtained from the Intermediate Representation ({IR}) of the source code, and are both language as well as machine independent. The entities of the {IR} are modelled as relationships, and their representations are learned to form a seed embedding vocabulary. This vocabulary is used along with the flow analyses information to form a hierarchy of encodings based on various levels of program abstractions. 
We show the effectiveness of our methodology on a software engineering task (program classification) as well as optimization tasks (Heterogeneous device mapping and Thread coarsening). The embeddings generated by {IR}2Vec outperform the existing methods in all the three tasks even when using simple machine learning models. As we follow an agglomerative method of forming encodings at various levels using seed embedding vocabulary, our encoding is naturally more scalable and not data-hungry when compared to the other methods.},
	journaltitle = {{ArXiv}},
	author = {{VenkataKeerthy}, S. and Aggarwal, Rohit and Jain, Shalini and Desarkar, Maunendra Sankar and Upadrasta, Ramakrishna and Srikant, Y. N.},
	date = {2019},
	keywords = {to-read, ⛔ No {DOI} found},
	file = {VenkataKeerthy et al_2019_IR2Vec.pdf:/data/zotero/storage/HJPU75D6/VenkataKeerthy et al_2019_IR2Vec.pdf:application/pdf}
}

@inproceedings{kulkarni_beyond_2018,
	title = {Beyond Deductive Methods in Program Analysis},
	abstract = {Building effective program analysis tools is a challenging endeavor: analysis designers must balance multiple competing objectives, including scalability, fraction of false alarms, and the possibility of missed bugs. Not all of these design decisions are optimal when the analysis is applied to a new program with different coding idioms, environment assumptions, and quality requirements. Furthermore, the alarms produced are typically accompanied by limited information such as their location and abstract counter-examples. We present a framework {DIFFLOG} that fundamentally extends the deductive reasoning rules that underlie program analyses with numerical weights. Each alarm is now naturally accompanied by a score, indicating quantities such as the confidence that the alarm is a real bug, the anticipated severity, or expected relevance of the alarm to the programmer. To the analysis user, these techniques offer a lens by which to focus their attention on the most important alarms and a uniform method for the tool to interactively generalize from human feedback. To the analysis designer, these techniques offer novel ways to automatically synthesize analysis rules in a data-driven style. {DIFFLOG} shows large reductions in false alarm rates and missed bugs in large, complex programs, and it advances the state-of-the-art in synthesizing non-trivial analyses.},
	author = {Kulkarni, Sulekha and Zhang, Rita and Si, Ximing and Heo, Kihong and Lee, Woosuk and Naik, Manali},
	date = {2018},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Kulkarni et al_2018_Beyond Deductive Methods in Program Analysis.pdf:/data/zotero/storage/4DYSY9KE/Kulkarni et al_2018_Beyond Deductive Methods in Program Analysis.pdf:application/pdf}
}

@inproceedings{yahav_typestate_2019,
	location = {Beijing, China},
	title = {From typestate verification to interpretable deep models (invited talk abstract)},
	isbn = {978-1-4503-6224-5},
	url = {https://doi.org/10.1145/3293882.3338992},
	doi = {10/gg3j6k},
	series = {{ISSTA} 2019},
	abstract = {The paper ``Effective Typestate Verification in the Presence of Aliasing'' was published in the International Symposium on Software Testing and Analysis ({ISSTA}) 2006 Proceedings, and has now been selected to receive the {ISSTA} 2019 Retrospective Impact Paper Award. The paper described a scalable framework for verification of typestate properties in real-world Java programs. The paper introduced several techniques that have been used widely in the static analysis of real-world programs. Specifically, it introduced an abstract domain combining access-paths, aliasing information, and typestate that turned out to be simple, powerful, and useful. We review the original paper and show the evolution of the ideas over the years. We show how some of these ideas have evolved into work on machine learning for code completion, and discuss recent general results in machine learning for programming.},
	pages = {4--5},
	booktitle = {Proceedings of the 28th {ACM} {SIGSOFT} International Symposium on Software Testing and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Yahav, Eran and Fink, Stephen J. and Dor, Nurit and Ramalingam, G. and Geay, Emmanuel},
	urldate = {2020-06-01},
	date = {2019-07-10},
	keywords = {to-read},
	file = {Yahav et al_2019_From typestate verification to interpretable deep models (invited talk abstract).pdf:/data/zotero/storage/8JTZQEVG/Yahav et al_2019_From typestate verification to interpretable deep models (invited talk abstract).pdf:application/pdf}
}

@article{bugerya_recovery_2019,
	title = {Recovery of High-Level Intermediate Representations of Algorithms from Binary Code},
	doi = {10/gg3j69},
	abstract = {One of the tasks of binary code security analysis is detection of undocumented features in software. This task is hard to automate, and it requires participation of a cybersecurity expert. The way of representation of the algorithm under analysis strongly determines the analysis effort and quality of its results. Existing intermediate representations and languages are intended for use in software that either carries out optimizing transformations or analyzes binary code. Such representations and intermediate languages are unsuitable for manual data flow analysis. This paper proposes a high-level hierarchical flowchart-based representation of a program algorithm as well as an algorithm for its construction. The proposed representation is based on a hypergraph and it allows both automatic and manual data flow analysis on different detail levels. The hypergraph nodes represent functions. Every node contains a set of other nodes which are fragments. The fragment is a linear sequence of instructions that does not contain call and ret instructions. Edges represent data flows between nodes and correspond to memory buffers and registers. In the future this representation can be used to implement automatic analysis algorithms. An approach is proposed to increasing quality of the developed algorithm representation using grouping of single data flows into one flow connecting logical algorithm modules.},
	journaltitle = {2019 Ivannikov Memorial Workshop ({IVMEM})},
	author = {Bugerya, A. B. and Kulagin, Ivan and Padaryan, Vartan A. and Solov'ev, M. A. and Tikhonov, Andrei Yur'evich},
	date = {2019},
	keywords = {out-of-scope},
	file = {Bugerya et al_2019_Recovery of High-Level Intermediate Representations of Algorithms from Binary.pdf:/data/zotero/storage/T2IV4CVI/Bugerya et al_2019_Recovery of High-Level Intermediate Representations of Algorithms from Binary.pdf:application/pdf}
}

@article{zhao_neural-augmented_2018,
	title = {Neural-augmented static analysis of Android communication},
	doi = {10/gg3j6t},
	abstract = {We address the problem of discovering communication links between applications in the popular Android mobile operating system, an important problem for security and privacy in Android. Any scalable static analysis in this complex setting is bound to produce an excessive amount of false-positives, rendering it impractical. To improve precision, we propose to augment static analysis with a trained neural-network model that estimates the probability that a communication link truly exists. We describe a neural-network architecture that encodes abstractions of communicating objects in two applications and estimates the probability with which a link indeed exists. At the heart of our architecture are type-directed encoders ({TDE}), a general framework for elegantly constructing encoders of a compound data type by recursively composing encoders for its constituent types. We evaluate our approach on a large corpus of Android applications, and demonstrate that it achieves very high accuracy. Further, we conduct thorough interpretability studies to understand the internals of the learned neural networks.},
	journaltitle = {{ESEC}/{FSE} 2018},
	author = {Zhao, Jinman and Albarghouthi, Aws and Rastogi, Vaibhav and Jha, Somesh and Octeau, Damien},
	date = {2018},
	keywords = {\_tablet},
	file = {Zhao et al_2018_Neural-augmented static analysis of Android communication.pdf:/data/zotero/storage/B3CBHMPQ/Zhao et al_2018_Neural-augmented static analysis of Android communication.pdf:application/pdf}
}

@online{bielik_learning_2017,
	title = {Learning a Static Analyzer from Data},
	url = {/paper/Learning-a-Static-Analyzer-from-Data-Bielik-Raychev/18fc2aa116bf0d6a54eb658932146857ebe229cb},
	abstract = {To be practically useful, modern static analyzers must precisely model the effect of both, statements in the programming language as well as frameworks used by the program under analysis. While important, manually addressing these challenges is difficult for at least two reasons: (i) the effects on the overall analysis can be non-trivial, and (ii) as the size and complexity of modern libraries increase, so is the number of cases the analysis must handle.},
	titleaddon = {undefined},
	author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin T.},
	urldate = {2020-06-01},
	date = {2017},
	langid = {english},
	note = {Library Catalog: www.semanticscholar.org},
	file = {Bielik et al_2017_Learning a Static Analyzer from Data.pdf:/data/zotero/storage/ABMPI7VK/Bielik et al_2017_Learning a Static Analyzer from Data.pdf:application/pdf;Snapshot:/data/zotero/storage/FNAS2LIZ/18fc2aa116bf0d6a54eb658932146857ebe229cb.html:text/html}
}

@inproceedings{ho_data-driven_2017,
	title = {Data-Driven Abstraction},
	abstract = {Given a program analysis problem that consists of a program and a property of interest, we use a data-driven approach to automatically construct a sequence of abstractions that approach an ideal abstraction suitable for solving that problem. This process begins with an infinite concrete domain that maps to a finite abstract domain defined by statistical procedures resulting in a clustering mixture model. Given a set of properties expressed as formulas in a restricted and bounded variant of {CTL}, we can test the success of the abstraction with respect to a predefined performance level. In addition, we can perform iterative abstraction-refinement of the clustering by tuning hyperparameters that determine the accuracy of the cluster representations (abstract states) and determine the number of clusters. Our methodology yields an induced abstraction and refinement procedure for property verification.},
	author = {Ho, Vivian M.},
	date = {2017},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Ho_2017_Data-Driven Abstraction.pdf:/data/zotero/storage/38X7GQ8L/Ho_2017_Data-Driven Abstraction.pdf:application/pdf}
}

@article{jeong_data-driven_2017,
	title = {Data-driven context-sensitivity for points-to analysis},
	doi = {10/gg3j58},
	abstract = {We present a new data-driven approach to achieve highly cost-effective context-sensitive points-to analysis for Java. While context-sensitivity has greater impact on the analysis precision and performance than any other precision-improving techniques, it is difficult to accurately identify the methods that would benefit the most from context-sensitivity and decide how much context-sensitivity should be used for them. Manually designing such rules is a nontrivial and laborious task that often delivers suboptimal results in practice. To overcome these challenges, we propose an automated and data-driven approach that learns to effectively apply context-sensitivity from codebases. In our approach, points-to analysis is equipped with a parameterized and heuristic rules, in disjunctive form of properties on program elements, that decide when and how much to apply context-sensitivity. We present a greedy algorithm that efficiently learns the parameter of the heuristic rules. We implemented our approach in the Doop framework and evaluated using three types of context-sensitive analyses: conventional object-sensitivity, selective hybrid object-sensitivity, and type-sensitivity. In all cases, experimental results show that our approach significantly outperforms existing techniques.},
	journaltitle = {{PACMPL}},
	author = {Jeong, Sehun and Jeon, Minseok and Cha, Sung Deok and Oh, Hakjoo},
	date = {2017},
	file = {Jeong et al_2017_Data-driven context-sensitivity for points-to analysis.pdf:/data/zotero/storage/FMM2FHLK/Jeong et al_2017_Data-driven context-sensitivity for points-to analysis.pdf:application/pdf}
}

@article{jeon_machine-learning_2019,
	title = {A Machine-Learning Algorithm with Disjunctive Model for Data-Driven Program Analysis},
	doi = {10/gg3j5w},
	abstract = {We present a new machine-learning algorithm with disjunctive model for data-driven program analysis. One major challenge in static program analysis is a substantial amount of manual effort required for tuning the analysis performance. Recently, data-driven program analysis has emerged to address this challenge by automatically adjusting the analysis based on data through a learning algorithm. Although this new approach has proven promising for various program analysis tasks, its effectiveness has been limited due to simple-minded learning models and algorithms that are unable to capture sophisticated, in particular disjunctive, program properties. To overcome this shortcoming, this article presents a new disjunctive model for data-driven program analysis as well as a learning algorithm to find the model parameters. Our model uses Boolean formulas over atomic features and therefore is able to express nonlinear combinations of program properties. A key technical challenge is to efficiently determine a set of good Boolean formulas, as brute-force search would simply be impractical. We present a stepwise and greedy algorithm that efficiently learns Boolean formulas. We show the effectiveness and generality of our algorithm with two static analyzers: context-sensitive points-to analysis for Java and flow-sensitive interval analysis for C. Experimental results show that our automated technique significantly improves the performance of the state-of-the-art techniques including ones hand-crafted by human experts.},
	journaltitle = {{TOPL}},
	author = {Jeon, {MinSeok} and Jeong, Sehun and Cha, S. D. and Oh, Hakjoo},
	date = {2019},
	keywords = {\_tablet},
	file = {Jeon et al_2019_A Machine-Learning Algorithm with Disjunctive Model for Data-Driven Program.pdf:/data/zotero/storage/UNT7ZLYM/Jeon et al_2019_A Machine-Learning Algorithm with Disjunctive Model for Data-Driven Program.pdf:application/pdf}
}

@article{grech_ptaint_2017,
	title = {P/Taint: unified points-to and taint analysis},
	doi = {10/gg3j64},
	shorttitle = {P/Taint},
	abstract = {Static information-flow analysis (especially taint-analysis) is a key technique in software security, computing where sensitive or untrusted data can propagate in a program. Points-to analysis is a fundamental static program analysis, computing what abstract objects a program expression may point to. In this work, we propose a deep unification of information-flow and points-to analysis. We observe that information-flow analysis is not a mere high-level client of points-to information, but it is indeed identical to points-to analysis on artificial abstract objects that represent different information sources. The very same algorithm can compute, simultaneously, two interlinked but separate results (points-to and information-flow values) with changes only to its initial conditions. 
The benefits of such a unification are manifold. We can use existing points-to analysis implementations, with virtually no modification (only minor additions of extra logic for sanitization) to compute information flow concepts, such as value tainting. The algorithmic enhancements of points-to analysis (e.g., different flavors of context sensitivity) can be applied transparently to information-flow analysis. Heavy engineering work on points-to analysis (e.g., handling of the reflection {API} for Java) applies to information-flow analysis without extra effort. We demonstrate the benefits in a realistic implementation that leverages the Doop points-to analysis framework (including its context-sensitivity and reflection analysis features) to provide an information-flow analysis with excellent precision (over 91\%) and recall (over 99\%) for standard Java information-flow benchmarks. 
The analysis comfortably scales to large, real-world Android applications, analyzing the Facebook Messenger app with more than 55K classes in under 7 hours.},
	journaltitle = {{PACMPL}},
	author = {Grech, Neville and Smaragdakis, Yannis},
	date = {2017},
	keywords = {\_tablet},
	file = {Grech_Smaragdakis_2017_P-Taint.pdf:/data/zotero/storage/ESBXH6BC/Grech_Smaragdakis_2017_P-Taint.pdf:application/pdf}
}

@article{upadhyaya_collective_2018,
	title = {Collective Program Analysis},
	doi = {10/gg3j56},
	abstract = {Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis ({CPA}), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69\% reduction when compared to a baseline and on average a 36\% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.},
	journaltitle = {2018 {IEEE}/{ACM} 40th International Conference on Software Engineering ({ICSE})},
	author = {Upadhyaya, Ganesha and Rajan, Hridesh},
	date = {2018},
	file = {Upadhyaya_Rajan_2018_Collective Program Analysis.pdf:/data/zotero/storage/KE8ERF8J/Upadhyaya_Rajan_2018_Collective Program Analysis.pdf:application/pdf}
}

@inproceedings{grigore_abstraction_2016,
	title = {Abstraction refinement guided by a learnt probabilistic model},
	doi = {10/gg3j5z},
	abstract = {The core challenge in designing an effective static program analysis is to find a good program abstraction -- one that retains only details relevant to a given query. In this paper, we present a new approach for automatically finding such an abstraction. Our approach uses a pessimistic strategy, which can optionally use guidance from a probabilistic model. Our approach applies to parametric static analyses implemented in Datalog, and is based on counterexample-guided abstraction refinement. For each untried abstraction, our probabilistic model provides a probability of success, while the size of the abstraction provides an estimate of its cost in terms of analysis time. Combining these two metrics, probability and cost, our refinement algorithm picks an optimal abstraction. Our probabilistic model is a variant of the Erdos--Renyi random graph model, and it is tunable by what we call hyperparameters. We present a method to learn good values for these hyperparameters, by observing past runs of the analysis on an existing codebase. We evaluate our approach on an object sensitive pointer analysis for Java programs, with two client analyses ({PolySite} and Downcast).},
	booktitle = {{POPL} 2016},
	author = {Grigore, Radu and Yang, Hongseok},
	date = {2016},
	file = {Accepted Version:/data/zotero/storage/G5SYDIZU/Grigore and Yang - 2016 - Abstraction refinement guided by a learnt probabil.pdf:application/pdf}
}

@article{seidel_learning_2017,
	title = {Learning to blame: localizing novice type errors with data-driven diagnosis},
	doi = {10/gg3j6z},
	shorttitle = {Learning to blame},
	abstract = {Localizing type errors is challenging in languages with global type inference, as the type checker must make assumptions about what the programmer intended to do. We introduce Nate, a data-driven approach to error localization based on supervised learning. Nate analyzes a large corpus of training data -- pairs of ill-typed programs and their "fixed" versions -- to automatically learn a model of where the error is most likely to be found. Given a new ill-typed program, Nate executes the model to generate a list of potential blame assignments ranked by likelihood. We evaluate Nate by comparing its precision to the state of the art on a set of over 5,000 ill-typed {OCaml} programs drawn from two instances of an introductory programming course. We show that when the top-ranked blame assignment is considered, Nate's data-driven model is able to correctly predict the exact sub-expression that should be changed 72\% of the time, 28 points higher than {OCaml} and 16 points higher than the state-of-the-art {SHErrLoc} tool. Furthermore, Nate's accuracy surpasses 85\% when we consider the top two locations and reaches 91\% if we consider the top three.},
	journaltitle = {{PACMPL}},
	author = {Seidel, Eric L. and Sibghat, Huma and Chaudhuri, Kamalika and Weimer, Westley and Jhala, Ranjit},
	date = {2017},
	file = {Full Text:/data/zotero/storage/CP5JKD9S/Seidel et al. - 2017 - Learning to blame localizing novice type errors w.pdf:application/pdf}
}

@article{wang_learning_2019-1,
	title = {Learning Blended, Precise Semantic Program Embeddings},
	url = {http://arxiv.org/abs/1907.02136},
	abstract = {Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, {\textbackslash}liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated {\textbackslash}liger on {\textbackslash}coset, a recently proposed benchmark suite for evaluating neural program embeddings. Results show {\textbackslash}liger (1) is significantly more accurate than the state-of-the-art syntax-based models Gated Graph Neural Network and code2vec in classifying program semantics, and (2) requires on average 10x fewer executions covering 74{\textbackslash}\% fewer paths than the state-of-the-art dynamic model {\textbackslash}dypro. Furthermore, we extend {\textbackslash}liger to predict the name for a method from its body's vector representation. Learning on the same set of functions (more than 170K in total), {\textbackslash}liger significantly outperforms code2seq, the previous state-of-the-art for method name prediction.},
	journaltitle = {{arXiv}:1907.02136 [cs]},
	author = {Wang, Ke and Su, Zhendong},
	urldate = {2020-06-02},
	date = {2019-07-11},
	eprinttype = {arxiv},
	eprint = {1907.02136},
	keywords = {read, ⛔ No {DOI} found},
	file = {Wang_Su_2019_Learning Blended, Precise Semantic Program Embeddings.pdf:/data/zotero/storage/N9XLAV68/Wang_Su_2019_Learning Blended, Precise Semantic Program Embeddings.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/JUDFCZAW/1907.html:text/html}
}

@inproceedings{cummins_end--end_2017,
	title = {End-to-End Deep Learning of Optimization Heuristics},
	doi = {10/gf8rqb},
	abstract = {Accurate automatic optimization heuristics are necessary for dealing with thecomplexity and diversity of modern hardware and software. Machine learning is aproven technique for learning such heuristics, but its success is bound by thequality of the features used. These features must be hand crafted by developersthrough a combination of expert domain knowledge and trial and error. This makesthe quality of the final model directly dependent on the skill and availabletime of the system architect. Our work introduces a better way for building heuristics. We develop a deepneural network that learns heuristics over raw code, entirely without using codefeatures. The neural network simultaneously constructs appropriaterepresentations of the code and learns how best to optimize, removing the needfor manual feature creation. Further, we show that our neural nets can transferlearning from one optimization problem to another, improving the accuracy of newmodels, without the help of human experts. We compare the effectiveness of our automatically generated heuristics againstones with features hand-picked by experts. We examine two challenging tasks:predicting optimal mapping for heterogeneous parallelism and {GPU} threadcoarsening factors. In 89\% of the cases, the quality of our fully automaticheuristics matches or surpasses that of state-of-the-art predictive models usinghand-crafted features, providing on average 14\% and 12\% more performance withno human effort expended on designing features.},
	eventtitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	pages = {219--232},
	booktitle = {2017 26th International Conference on Parallel Architectures and Compilation Techniques ({PACT})},
	author = {Cummins, Chris and Petoumenos, Pavlos and Wang, Zheng and Leather, Hugh},
	date = {2017-09},
	file = {IEEE Xplore Abstract Record:/data/zotero/storage/G9QMWSJH/8091247.html:text/html;Cummins et al_2017_End-to-End Deep Learning of Optimization Heuristics.pdf:/data/zotero/storage/VX992JK5/Cummins et al_2017_End-to-End Deep Learning of Optimization Heuristics.pdf:application/pdf}
}

@inproceedings{cheung_using_2012,
	location = {Maui, Hawaii, {USA}},
	title = {Using program synthesis for social recommendations},
	isbn = {978-1-4503-1156-4},
	url = {https://doi.org/10.1145/2396761.2398507},
	doi = {10/gf8nnx},
	series = {{CIKM} '12},
	abstract = {This paper presents a new approach to select events of interest to users in a social media setting where events are generated from mobile devices. We argue that the problem is best solved by inductive learning, where the goal is to first generalize from the users' expressed "likes" and "dislikes" of specific events, then to produce a program that can be used to collect only data of interest. The key contribution of this paper is a new algorithm that combines machine learning techniques with program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application.1},
	pages = {1732--1736},
	booktitle = {Proceedings of the 21st {ACM} international conference on Information and knowledge management},
	publisher = {Association for Computing Machinery},
	author = {Cheung, Alvin and Solar-Lezama, Armando and Madden, Samuel},
	urldate = {2020-06-13},
	date = {2012-10-29},
	file = {Cheung et al_2012_Using program synthesis for social recommendations.pdf:/data/zotero/storage/GXELQRPU/Cheung et al_2012_Using program synthesis for social recommendations.pdf:application/pdf}
}

@inproceedings{sankaranarayanan_dynamic_2008,
	location = {Seattle, {WA}, {USA}},
	title = {Dynamic inference of likely data preconditions over predicates by tree learning},
	isbn = {978-1-60558-050-0},
	url = {https://doi.org/10.1145/1390630.1390666},
	doi = {10/fpfzhg},
	series = {{ISSTA} '08},
	abstract = {We present a technique to infer likely data preconditions forprocedures written in an imperative programming language. Given a procedure and a set of predicates over its inputs, our technique enumerates different truth assignments to the predicates, deriving test cases from each feasible truth assignment. The predicates themselves are derived automatically using simple heuristics. The enumeration of truth assignments is performed using a propositional {SAT} solver along with a theory satisfiability checker capable of generating unsatisfiable cores. For each assignment of truth values, a corresponding set of test cases are generated and executed. Based on the result of the execution, the truth assignment is classified as being safe or buggy. Finally, a decision tree classifier is used to generate a Boolean formula over the input predicates that explains the data obtained from the test cases. The resulting Boolean formula is, in effect, a likely data precondition for the procedure under consideration. We apply our techniques on a wide variety of functions from the standard C library. Our experiments show that the proposed technique is quite robust. For most cases, it successfully learns a precondition that captures a safe and permissive calling environment.},
	pages = {295--306},
	booktitle = {Proceedings of the 2008 international symposium on Software testing and analysis},
	publisher = {Association for Computing Machinery},
	author = {Sankaranarayanan, Sriram and Chaudhuri, Swarat and Ivančić, Franjo and Gupta, Aarti},
	urldate = {2020-06-13},
	date = {2008-07-20},
	file = {Sankaranarayanan et al_2008_Dynamic inference of likely data preconditions over predicates by tree learning.pdf:/data/zotero/storage/R4AS3JTG/Sankaranarayanan et al_2008_Dynamic inference of likely data preconditions over predicates by tree learning.pdf:application/pdf}
}

@article{zaremba_learning_2015,
	title = {Learning to Execute},
	url = {http://arxiv.org/abs/1410.4615},
	abstract = {Recurrent Neural Networks ({RNNs}) with Long Short-Term Memory units ({LSTM}) are widely used because they are expressive and are easy to train. Our interest lies in empirically evaluating the expressiveness and the learnability of {LSTMs} in the sequence-to-sequence regime by training them to evaluate short computer programs, a domain that has traditionally been seen as too complex for neural networks. We consider a simple class of programs that can be evaluated with a single left-to-right pass using constant memory. Our main result is that {LSTMs} can learn to map the character-level representations of such programs to their correct outputs. Notably, it was necessary to use curriculum learning, and while conventional curriculum learning proved ineffective, we developed a new variant of curriculum learning that improved our networks' performance in all experimental conditions. The improved curriculum had a dramatic impact on an addition problem, making it possible to train an {LSTM} to add two 9-digit numbers with 99\% accuracy.},
	journaltitle = {{arXiv}:1410.4615 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya},
	urldate = {2020-06-13},
	date = {2015-02-19},
	eprinttype = {arxiv},
	eprint = {1410.4615},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Zaremba_Sutskever_2015_Learning to Execute.pdf:/data/zotero/storage/A7M54DL5/Zaremba_Sutskever_2015_Learning to Execute.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ISI6SM53/1410.html:text/html}
}

@article{keller_what_2020,
	title = {What You See is What it Means! Semantic Representation Learning of Code based on Visualization and Transfer Learning},
	abstract = {Recent successes in training word embeddings for {NLP} tasks have encouraged a wave of research on representation learning for source code, which builds on similar {NLP} methods. The overall objective is then to produce code embeddings that capture the maximum of program semantics. State-of-the-art approaches invariably rely on a syntactic representation (i.e., raw lexical tokens, abstract syntax trees, or intermediate representation tokens) to generate embeddings, which are criticized in the literature as non-robust or non-generalizable. In this work, we investigate a novel embedding approach based on the intuition that source code has visual patterns of semantics. We further use these patterns to address the outstanding challenge of identifying semantic code clones. We propose the {WYSIWIM} ("What You See Is What It Means") approach where visual representations of source code are fed into powerful pre-trained image classification neural networks from the field of computer vision to benefit from the practical advantages of transfer learning. We evaluate the proposed embedding approach on two variations of the task of semantic code clone identification: code clone detection (a binary classification problem), and code classification (a multi-classification problem). We show with experiments on the {BigCloneBench} (Java) and Open Judge (C) datasets that although simple, our {WYSIWIM} approach performs as effectively as state of the art approaches such as {ASTNN} or {TBCNN}. We further explore the influence of different steps in our approach, such as the choice of visual representations or the classification algorithm, to eventually discuss the promises and limitations of this research direction.},
	journaltitle = {{ArXiv}},
	author = {Keller, Patrick and Plein, L. N. and Bissyand'e, Tegawend'e F. and Klein, Jacques and Traon, Yves Le},
	date = {2020},
	keywords = {to-read, ⛔ No {DOI} found},
	file = {Keller et al_2020_What You See is What it Means.pdf:/data/zotero/storage/PG3MKWGL/Keller et al_2020_What You See is What it Means.pdf:application/pdf}
}

@article{chae_automatically_2017,
	title = {Automatically generating features for learning program analysis heuristics for C-like languages},
	volume = {1},
	url = {https://doi.org/10.1145/3133925},
	doi = {10/gg3j55},
	abstract = {We present a technique for automatically generating features for data-driven program analyses. Recently data-driven approaches for building a program analysis have been developed, which mine existing codebases and automatically learn heuristics for finding a cost-effective abstraction for a given analysis task. Such approaches reduce the burden of the analysis designers, but they do not remove it completely; they still leave the nontrivial task of designing so called features to the hands of the designers. Our technique aims at automating this feature design process. The idea is to use programs as features after reducing and abstracting them. Our technique goes through selected program-query pairs in codebases, and it reduces and abstracts the program in each pair to a few lines of code, while ensuring that the analysis behaves similarly for the original and the new programs with respect to the query. Each reduced program serves as a boolean feature for program-query pairs. This feature evaluates to true for a given program-query pair when (as a program) it is included in the program part of the pair. We have implemented our approach for three real-world static analyses. The experimental results show that these analyses with automatically-generated features are cost-effective and consistently perform well on a wide range of programs.},
	pages = {101:1--101:25},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Chae, Kwonsoo and Oh, Hakjoo and Heo, Kihong and Yang, Hongseok},
	urldate = {2020-06-13},
	date = {2017-10-12},
	file = {Chae et al_2017_Automatically generating features for learning program analysis heuristics for.pdf:/data/zotero/storage/AITBKF9B/Chae et al_2017_Automatically generating features for learning program analysis heuristics for.pdf:application/pdf}
}

@article{wang_search_2018,
	title = {Search, align, and repair: data-driven feedback generation for introductory programming exercises},
	volume = {53},
	issn = {0362-1340},
	url = {https://doi.org/10.1145/3296979.3192384},
	doi = {10/gg3j7c},
	shorttitle = {Search, align, and repair},
	abstract = {This paper introduces the “Search, Align, and Repair” data-driven program repair framework to automate feedback generation for introductory programming exercises. Distinct from existing techniques, our goal is to develop an efficient, fully automated, and problem-agnostic technique for large or {MOOC}-scale introductory programming courses. We leverage the large amount of available student submissions in such settings and develop new algorithms for identifying similar programs, aligning correct and incorrect programs, and repairing incorrect programs by finding minimal fixes. We have implemented our technique in the Sarfgen system and evaluated it on thousands of real student attempts from the Microsoft-{DEV}204.1x {edX} course and the Microsoft {CodeHunt} platform. Our results show that Sarfgen can, within two seconds on average, generate concise, useful feedback for 89.7\% of the incorrect student submissions. It has been integrated with the Microsoft-{DEV}204.1X {edX} class and deployed for production use.},
	pages = {481--495},
	number = {4},
	journaltitle = {{ACM} {SIGPLAN} Notices},
	shortjournal = {{SIGPLAN} Not.},
	author = {Wang, Ke and Singh, Rishabh and Su, Zhendong},
	urldate = {2020-06-13},
	date = {2018-06-11},
	file = {Wang et al_2018_Search, align, and repair.pdf:/data/zotero/storage/XV57H4BF/Wang et al_2018_Search, align, and repair.pdf:application/pdf}
}

@article{wang_coset_2019,
	title = {{COSET}: A Benchmark for Evaluating Neural Program Embeddings},
	url = {http://arxiv.org/abs/1905.11445},
	shorttitle = {{COSET}},
	abstract = {Neural program embedding can be helpful in analyzing large software, a task that is challenging for traditional logic-based program analyses due to their limited scalability. A key focus of recent machine-learning advances in this area is on modeling program semantics instead of just syntax. Unfortunately evaluating such advances is not obvious, as program semantics does not lend itself to straightforward metrics. In this paper, we introduce a benchmarking framework called {COSET} for standardizing the evaluation of neural program embeddings. {COSET} consists of a diverse dataset of programs in source-code format, labeled by human experts according to a number of program properties of interest. A point of novelty is a suite of program transformations included in {COSET}. These transformations when applied to the base dataset can simulate natural changes to program code due to optimization and refactoring and can serve as a "debugging" tool for classification mistakes. We conducted a pilot study on four prominent models: {TreeLSTM}, gated graph neural network ({GGNN}), {AST}-Path neural network ({APNN}), and {DYPRO}. We found that {COSET} is useful in identifying the strengths and limitations of each model and in pinpointing specific syntactic and semantic characteristics of programs that pose challenges.},
	journaltitle = {{arXiv}:1905.11445 [cs, stat]},
	author = {Wang, Ke and Christodorescu, Mihai},
	urldate = {2020-06-13},
	date = {2019-05-27},
	eprinttype = {arxiv},
	eprint = {1905.11445},
	keywords = {read, ⛔ No {DOI} found},
	file = {Wang_Christodorescu_2019_COSET.pdf:/data/zotero/storage/YKCUVZ6L/Wang_Christodorescu_2019_COSET.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/6VVKFAKX/1905.html:text/html}
}

@inproceedings{henkel_code_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Code vectors: understanding programs through embedded abstracted symbolic traces},
	isbn = {978-1-4503-5573-5},
	url = {https://doi.org/10.1145/3236024.3236085},
	doi = {10/gf6gbq},
	series = {{ESEC}/{FSE} 2018},
	shorttitle = {Code vectors},
	abstract = {With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied. In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93\% top-1 accuracy on a benchmark consisting of over 19,000 {API}-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.},
	pages = {163--174},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas},
	urldate = {2020-06-13},
	date = {2018-10-26},
	keywords = {read},
	file = {Henkel et al_2018_Code vectors.pdf:/data/zotero/storage/G7EN5D36/Henkel et al_2018_Code vectors.pdf:application/pdf}
}

@article{proksch_intelligent_2015,
	title = {Intelligent Code Completion with Bayesian Networks},
	volume = {25},
	issn = {1049-331X},
	url = {https://doi.org/10.1145/2744200},
	doi = {10/f73z6p},
	abstract = {Code completion is an integral part of modern Integrated Development Environments ({IDEs}). Developers often use it to explore Application Programming Interfaces ({APIs}). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items. This work extends one of these existing approaches, the Best Matching Neighbor ({BMN}) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks ({PBN}), to the existing {BMN} algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that {PBN} can obtain comparable prediction quality to {BMN}, while model size and inference speed scale better with large input sizes.},
	pages = {3:1--3:31},
	number = {1},
	journaltitle = {{ACM} Transactions on Software Engineering and Methodology},
	shortjournal = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Proksch, Sebastian and Lerch, Johannes and Mezini, Mira},
	urldate = {2020-06-13},
	date = {2015-12-02},
	keywords = {to-read},
	file = {Proksch et al_2015_Intelligent Code Completion with Bayesian Networks.pdf:/data/zotero/storage/FMMJL9VF/Proksch et al_2015_Intelligent Code Completion with Bayesian Networks.pdf:application/pdf}
}

@inproceedings{piech_learning_2015,
	title = {Learning Program Embeddings to Propagate Feedback on Student Code},
	url = {http://proceedings.mlr.press/v37/piech15.html},
	abstract = {Providing feedback, both assessing final work
and giving hints to stuck students, is difficult
for open-ended assignments in massive online
classes which can range from thousands to mil-
lions of students. We introduce a neural network
method to encode programs as a linear mapping
from an embedded precondition space to an em-
bedded postcondition space and propose an al-
gorithm for feedback at scale using these lin-
ear maps as features. We apply our algorithm
to assessments from the Code.org Hour of Code
and Stanford University’s {CS}1 course, where we
propagate human comments on student assign-
ments to orders of magnitude more submissions.},
	eventtitle = {International Conference on Machine Learning},
	pages = {1093--1102},
	booktitle = {International Conference on Machine Learning},
	author = {Piech, Chris and Huang, Jonathan and Nguyen, Andy and Phulsuksombati, Mike and Sahami, Mehran and Guibas, Leonidas},
	urldate = {2020-06-15},
	date = {2015-06-01},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	keywords = {read, \_tablet},
	file = {Piech et al_2015_Learning Program Embeddings to Propagate Feedback on Student Code.pdf:/data/zotero/storage/VBHQUR9H/Piech et al_2015_Learning Program Embeddings to Propagate Feedback on Student Code.pdf:application/pdf;Snapshot:/data/zotero/storage/KVWFVIGK/piech15.html:text/html}
}

@article{reed_neural_2016,
	title = {Neural Programmer-Interpreters},
	url = {http://arxiv.org/abs/1511.06279},
	abstract = {We propose the neural programmer-interpreter ({NPI}): a recurrent and compositional neural network that learns to represent and execute programs. {NPI} has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single {NPI} to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, {NPI} reduces sample complexity and increases generalization ability compared to sequence-to-sequence {LSTMs}. The program memory allows efficient learning of additional tasks by building on existing programs. {NPI} can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the {NPI} with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, {NPI} learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single {NPI} learns to execute these programs and all 21 associated subprograms.},
	journaltitle = {{arXiv}:1511.06279 [cs]},
	author = {Reed, Scott and de Freitas, Nando},
	urldate = {2020-06-15},
	date = {2016-02-29},
	eprinttype = {arxiv},
	eprint = {1511.06279},
	keywords = {to-read, ⛔ No {DOI} found, \_tablet},
	file = {Reed_de Freitas_2016_Neural Programmer-Interpreters.pdf:/data/zotero/storage/ZLEZYAPP/Reed_de Freitas_2016_Neural Programmer-Interpreters.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/7HXHHA8H/1511.html:text/html}
}

@inproceedings{chibotaru_scalable_2019,
	location = {Phoenix, {AZ}, {USA}},
	title = {Scalable taint specification inference with big code},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314648},
	doi = {10/gg3j7b},
	series = {{PLDI} 2019},
	abstract = {We present a new scalable, semi-supervised method for inferring taint analysis specifications by learning from a large dataset of programs. Taint specifications capture the role of library {APIs} (source, sink, sanitizer) and are a critical ingredient of any taint analyzer that aims to detect security violations based on information flow. The core idea of our method is to formulate the taint specification learning problem as a linear optimization task over a large set of information flow constraints. The resulting constraint system can then be efficiently solved with state-of-the-art solvers. Thanks to its scalability, our method can infer many new and interesting taint specifications by simultaneously learning from a large dataset of programs (e.g., as found on {GitHub}), while requiring few manual annotations. We implemented our method in an end-to-end system, called Seldon, targeting Python, a language where static specification inference is particularly hard due to lack of typing information. We show that Seldon is practically effective: it learned almost 7,000 {API} roles from over 210,000 candidate {APIs} with very little supervision (less than 300 annotations) and with high estimated precision (67\%). Further, using the learned specifications, our taint analyzer flagged more than 20,000 violations in open source projects, 97\% of which were undetectable without the inferred specifications.},
	pages = {760--774},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Chibotaru, Victor and Bichsel, Benjamin and Raychev, Veselin and Vechev, Martin},
	urldate = {2020-06-15},
	date = {2019-06-08},
	keywords = {low-prio},
	file = {Chibotaru et al_2019_Scalable taint specification inference with big code.pdf:/data/zotero/storage/3KY9YA6B/Chibotaru et al_2019_Scalable taint specification inference with big code.pdf:application/pdf}
}

@inproceedings{padhi_data-driven_2016,
	location = {Santa Barbara, {CA}, {USA}},
	title = {Data-driven precondition inference with learned features},
	isbn = {978-1-4503-4261-2},
	url = {https://doi.org/10.1145/2908080.2908099},
	doi = {10/gg3j59},
	series = {{PLDI} '16},
	abstract = {We extend the data-driven approach to inferring preconditions for code from a set of test executions. Prior work requires a fixed set of features, atomic predicates that define the search space of possible preconditions, to be specified in advance. In contrast, we introduce a technique for on-demand feature learning, which automatically expands the search space of candidate preconditions in a targeted manner as necessary. We have instantiated our approach in a tool called {PIE}. In addition to making precondition inference more expressive, we show how to apply our feature-learning technique to the setting of data-driven loop invariant inference. We evaluate our approach by using {PIE} to infer rich preconditions for black-box {OCaml} library functions and using our loop-invariant inference algorithm as part of an automatic program verifier for C++ programs.},
	pages = {42--56},
	booktitle = {Proceedings of the 37th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd},
	urldate = {2020-06-15},
	date = {2016-06-02},
	keywords = {read},
	file = {Padhi et al_2016_Data-driven precondition inference with learned features.pdf:/data/zotero/storage/ZJUPJZ4X/Padhi et al_2016_Data-driven precondition inference with learned features.pdf:application/pdf}
}

@inproceedings{churchill_semantic_2019,
	location = {Phoenix, {AZ}, {USA}},
	title = {Semantic program alignment for equivalence checking},
	isbn = {978-1-4503-6712-7},
	url = {https://doi.org/10.1145/3314221.3314596},
	doi = {10/gg3j7d},
	series = {{PLDI} 2019},
	abstract = {We introduce a robust semantics-driven technique for program equivalence checking. Given two functions we find a trace alignment over a set of concrete executions of both programs and construct a product program particularly amenable to checking equivalence. We demonstrate that our algorithm is applicable to challenging equivalence problems beyond the scope of existing techniques. For example, we verify the correctness of the hand-optimized vector implementation of strlen that ships as part of the {GNU} C Library, as well as the correctness of vectorization optimizations for 56 benchmarks derived from the Test Suite for Vectorizing Compilers.},
	pages = {1027--1040},
	booktitle = {Proceedings of the 40th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Churchill, Berkeley and Padon, Oded and Sharma, Rahul and Aiken, Alex},
	urldate = {2020-06-15},
	date = {2019-06-08},
	keywords = {out-of-scope},
	file = {Churchill et al_2019_Semantic program alignment for equivalence checking.pdf:/data/zotero/storage/GKP8IQ7X/Churchill et al_2019_Semantic program alignment for equivalence checking.pdf:application/pdf}
}

@inproceedings{bastani_active_2018,
	location = {Philadelphia, {PA}, {USA}},
	title = {Active learning of points-to specifications},
	isbn = {978-1-4503-5698-5},
	url = {https://doi.org/10.1145/3192366.3192383},
	doi = {10/gg3j52},
	series = {{PLDI} 2018},
	abstract = {When analyzing programs, large libraries pose significant challenges to static points-to analysis. A popular solution is to have a human analyst provide points-to specifications that summarize relevant behaviors of library code, which can substantially improve precision and handle missing code such as native code. We propose Atlas, a tool that automatically infers points-to specifications. Atlas synthesizes unit tests that exercise the library code, and then infers points-to specifications based on observations from these executions. Atlas automatically infers specifications for the Java standard library, and produces better results for a client static information flow analysis on a benchmark of 46 Android apps compared to using existing handwritten specifications.},
	pages = {678--692},
	booktitle = {Proceedings of the 39th {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Bastani, Osbert and Sharma, Rahul and Aiken, Alex and Liang, Percy},
	urldate = {2020-06-15},
	date = {2018-06-11},
	keywords = {skimmed, \_tablet},
	file = {Bastani et al_2018_Active learning of points-to specifications.pdf:/data/zotero/storage/TZWYUBLY/Bastani et al_2018_Active learning of points-to specifications.pdf:application/pdf}
}

@inproceedings{sharma_data_2013,
	location = {Berlin, Heidelberg},
	title = {A Data Driven Approach for Algebraic Loop Invariants},
	isbn = {978-3-642-37036-6},
	doi = {10/gg3j5t},
	series = {Lecture Notes in Computer Science},
	abstract = {We describe a Guess-and-Check algorithm for computing algebraic equation invariants of the form ∧ i f i (x 1,…,x n ) = 0, where each f i is a polynomial over the variables x 1,…,x n of the program. The “guess” phase is data driven and derives a candidate invariant from data generated from concrete executions of the program. This candidate invariant is subsequently validated in a “check” phase by an off-the-shelf {SMT} solver. Iterating between the two phases leads to a sound algorithm. Moreover, we are able to prove a bound on the number of decision procedure queries which Guess-and-Check requires to obtain a sound invariant. We show how Guess-and-Check can be extended to generate arbitrary boolean combinations of linear equalities as invariants, which enables us to generate expressive invariants to be consumed by tools that cannot handle non-linear arithmetic. We have evaluated our technique on a number of benchmark programs from recent papers on invariant generation. Our results are encouraging – we are able to efficiently compute algebraic invariants in all cases, with only a few tests.},
	pages = {574--592},
	booktitle = {Programming Languages and Systems},
	publisher = {Springer},
	author = {Sharma, Rahul and Gupta, Saurabh and Hariharan, Bharath and Aiken, Alex and Liang, Percy and Nori, Aditya V.},
	editor = {Felleisen, Matthias and Gardner, Philippa},
	date = {2013},
	langid = {english},
	file = {Sharma et al_2013_A Data Driven Approach for Algebraic Loop Invariants.pdf:/data/zotero/storage/G3X9KVWZ/Sharma et al_2013_A Data Driven Approach for Algebraic Loop Invariants.pdf:application/pdf}
}

@article{wang_learning_2020,
	title = {Learning Semantic Program Embeddings with Graph Interval Neural Network},
	url = {http://arxiv.org/abs/2005.09997},
	abstract = {Learning distributed representations of source code has been a challenging task for machine learning models. Earlier works treated programs as text so that natural language methods can be readily applied. Unfortunately, such approaches do not capitalize on the rich structural information possessed by source code. Of late, Graph Neural Network ({GNN}) was proposed to learn embeddings of programs from their graph representations. Due to the homogeneous and expensive message-passing procedure, {GNN} can suffer from precision issues, especially when dealing with programs rendered into large graphs. In this paper, we present a new graph neural architecture, called Graph Interval Neural Network ({GINN}), to tackle the weaknesses of the existing {GNN}. Unlike the standard {GNN}, {GINN} generalizes from a curated graph representation obtained through an abstraction method designed to aid models to learn. In particular, {GINN} focuses exclusively on intervals for mining the feature representation of a program, furthermore, {GINN} operates on a hierarchy of intervals for scaling the learning to large graphs. We evaluate {GINN} for two popular downstream applications: variable misuse prediction and method name prediction. Results show in both cases {GINN} outperforms the state-of-the-art models by a comfortable margin. We have also created a neural bug detector based on {GINN} to catch null pointer deference bugs in Java code. While learning from the same 9,000 methods extracted from 64 projects, {GINN}-based bug detector significantly outperforms {GNN}-based bug detector on 13 unseen test projects. Next, we deploy our trained {GINN}-based bug detector and Facebook Infer to scan the codebase of 20 highly starred projects on {GitHub}. Through our manual inspection, we confirm 38 bugs out of 102 warnings raised by {GINN}-based bug detector compared to 34 bugs out of 129 warnings for Facebook Infer.},
	journaltitle = {{arXiv}:2005.09997 [cs]},
	author = {Wang, Yu and Gao, Fengjuan and Wang, Linzhang and Wang, Ke},
	urldate = {2020-06-15},
	date = {2020-05-26},
	eprinttype = {arxiv},
	eprint = {2005.09997},
	keywords = {read, ⛔ No {DOI} found},
	file = {Wang et al_2020_Learning Semantic Program Embeddings with Graph Interval Neural Network.pdf:/data/zotero/storage/IMPECCRJ/Wang et al_2020_Learning Semantic Program Embeddings with Graph Interval Neural Network.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3KRZ67XA/2005.html:text/html}
}

@inproceedings{clapp_modelgen_2015,
	location = {Baltimore, {MD}, {USA}},
	title = {Modelgen: mining explicit information flow specifications from concrete executions},
	isbn = {978-1-4503-3620-8},
	url = {https://doi.org/10.1145/2771783.2771810},
	doi = {10/gg3j63},
	series = {{ISSTA} 2015},
	shorttitle = {Modelgen},
	abstract = {We present a technique to mine explicit information flow specifications from concrete executions. These specifications can be consumed by a static taint analysis, enabling static analysis to work even when method definitions are missing or portions of the program are too difficult to analyze statically (e.g., due to dynamic features such as reflection). We present an implementation of our technique for the Android platform. When compared to a set of manually written specifications for 309 methods across 51 classes, our technique is able to recover 96.36\% of these manual specifications and produces many more correct annotations that our manual models missed. We incorporate the generated specifications into an existing static taint analysis system, and show that they enable it to find additional true flows. Although our implementation is Android-specific, our approach is applicable to other application frameworks.},
	pages = {129--140},
	booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
	publisher = {Association for Computing Machinery},
	author = {Clapp, Lazaro and Anand, Saswat and Aiken, Alex},
	urldate = {2020-06-15},
	date = {2015-07-13},
	keywords = {out-of-scope},
	file = {Clapp et al_2015_Modelgen.pdf:/data/zotero/storage/6J7IZCHH/Clapp et al_2015_Modelgen.pdf:application/pdf}
}

@article{bader_getafix_2019,
	title = {Getafix: Learning to Fix Bugs Automatically},
	volume = {1902},
	url = {http://adsabs.harvard.edu/abs/2019arXiv190206111B},
	shorttitle = {Getafix},
	abstract = {Static analyzers help find bugs early by warning about recurring bug categories. While fixing these bugs still remains a mostly manual task in practice, we observe that fixes for a specific bug category often are repetitive. This paper addresses the problem of automatically fixing instances of common bugs by learning from past fixes. We present Getafix, an approach that produces human-like fixes while being fast enough to suggest fixes in time proportional to the amount of time needed to obtain static analysis results in the first place. Getafix is based on a novel hierarchical clustering algorithm that summarizes fix patterns into a hierarchy ranging from general to specific patterns. Instead of a computationally expensive exploration of a potentially large space of candidate fixes, Getafix uses a simple yet effective ranking technique that uses the context of a code change to select the most appropriate fix for a given bug. Our evaluation applies Getafix to 1,268 bug fixes for six bug categories reported by popular static analyzers for Java, including null dereferences, incorrect {API} calls, and misuses of particular language constructs. The approach predicts exactly the human-written fix as the top-most suggestion between 12\% and 91\% of the time, depending on the bug category. The top-5 suggestions contain fixes for 526 of the 1,268 bugs. Moreover, we report on
deploying the approach within Facebook, where it contributes to the reliability of software used by billions of people. To the best of our knowledge, Getafix is the first industrially-deployed automated
bug-fixing tool that learns fix patterns from past, human-written fixes to produce human-like fixes.},
	pages = {arXiv:1902.06111},
	journaltitle = {{arXiv} e-prints},
	shortjournal = {{arXiv} e-prints},
	author = {Bader, Johannes and Scott, Andrew and Pradel, Michael and Chandra, Satish},
	urldate = {2020-06-18},
	date = {2019-02-01},
	keywords = {⛔ No {DOI} found},
	file = {Bader et al_2019_Getafix.pdf:/data/zotero/storage/GR59MM6T/Bader et al_2019_Getafix.pdf:application/pdf}
}

@article{ramakrishnan_semantic_2020,
	title = {Semantic Robustness of Models of Source Code},
	url = {http://arxiv.org/abs/2002.03043},
	abstract = {Deep neural networks are vulnerable to adversarial examples - small input perturbations that result in incorrect predictions. We study this problem for models of source code, where we want the network to be robust to source-code modifications that preserve code functionality. (1) We define a powerful adversary that can employ sequences of parametric, semantics-preserving program transformations; (2) we show how to perform adversarial training to learn models robust to such adversaries; (3) we conduct an evaluation on different languages and architectures, demonstrating significant quantitative gains in robustness.},
	journaltitle = {{arXiv}:2002.03043 [cs, stat]},
	author = {Ramakrishnan, Goutham and Henkel, Jordan and Wang, Zi and Albarghouthi, Aws and Jha, Somesh and Reps, Thomas},
	urldate = {2020-06-18},
	date = {2020-06-11},
	eprinttype = {arxiv},
	eprint = {2002.03043},
	keywords = {read, ⛔ No {DOI} found},
	file = {Ramakrishnan et al_2020_Semantic Robustness of Models of Source Code.pdf:/data/zotero/storage/XJQTVDH4/Ramakrishnan et al_2020_Semantic Robustness of Models of Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/GWWE3QTQ/2002.html:text/html}
}

@inproceedings{hindle_naturalness_2012,
	title = {On the naturalness of software},
	doi = {10/gg3j6w},
	abstract = {Natural languages like English are rich, complex, and powerful. The highly creative and graceful use of languages like English and Tamil, by masters like Shakespeare and Avvaiyar, can certainly delight and inspire. But in practice, given cognitive constraints and the exigencies of daily life, most human utterances are far simpler and much more repetitive and predictable. In fact, these utterances can be very usefully modeled using modern statistical methods. This fact has led to the phenomenal success of statistical approaches to speech recognition, natural language translation, question-answering, and text mining and comprehension. We begin with the conjecture that most software is also natural, in the sense that it is created by humans at work, with all the attendant constraints and limitations - and thus, like natural language, it is also likely to be repetitive and predictable. We then proceed to ask whether a) code can be usefully modeled by statistical language models and b) such models can be leveraged to support software engineers. Using the widely adopted n-gram model, we provide empirical evidence supportive of a positive answer to both these questions. We show that code is also very repetitive, and in fact even more so than natural languages. As an example use of the model, we have developed a simple code completion engine for Java that, despite its simplicity, already improves Eclipse's built-in completion capability. We conclude the paper by laying out a vision for future research in this area.},
	eventtitle = {2012 34th International Conference on Software Engineering ({ICSE})},
	pages = {837--847},
	booktitle = {2012 34th International Conference on Software Engineering ({ICSE})},
	author = {Hindle, Abram and Barr, Earl T. and Su, Zhendong and Gabel, Mark and Devanbu, Premkumar},
	date = {2012-06},
	note = {{ISSN}: 1558-1225},
	keywords = {skimmed},
	file = {Hindle et al_2012_On the naturalness of software.pdf:/data/zotero/storage/6NBC3TI8/Hindle et al_2012_On the naturalness of software.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/K47AQZL2/6227135.html:text/html}
}

@article{hu_codesum_2018,
	title = {{CodeSum}: Translate Program Language to Natural Language},
	url = {http://arxiv.org/abs/1708.01837},
	shorttitle = {{CodeSum}},
	abstract = {During software maintenance, programmers spend a lot of time on code comprehension. Reading comments is an effective way for programmers to reduce the reading and navigating time when comprehending source code. Therefore, as a critical task in software engineering, code summarization aims to generate brief natural language descriptions for source code. In this paper, we propose a new code summarization model named {CodeSum}. {CodeSum} exploits the attention-based sequence-to-sequence (Seq2Seq) neural network with Structure-based Traversal ({SBT}) of Abstract Syntax Trees ({AST}). The {AST} sequences generated by {SBT} can better present the structure of {ASTs} and keep unambiguous. We conduct experiments on three large-scale corpora in different program languages, i.e., Java, C\#, and {SQL}, in which Java corpus is our new proposed industry code extracted from Github. Experimental results show that our method {CodeSum} outperforms the state-of-the-art significantly.},
	journaltitle = {{arXiv}:1708.01837 [cs]},
	author = {Hu, Xing and Wei, Yuhan and Li, Ge and Jin, Zhi},
	urldate = {2020-06-18},
	date = {2018-01-31},
	eprinttype = {arxiv},
	eprint = {1708.01837},
	keywords = {to-read, ⛔ No {DOI} found},
	file = {arXiv.org Snapshot:/data/zotero/storage/UBNA8UZ4/1708.html:text/html}
}

@article{brockschmidt_gnn-film_2019,
	title = {{GNN}-{FiLM}: Graph Neural Networks with Feature-wise Linear Modulation},
	url = {http://arxiv.org/abs/1906.12192},
	shorttitle = {{GNN}-{FiLM}},
	abstract = {This paper presents a new Graph Neural Network ({GNN}) type using feature-wise linear modulation ({FiLM}). Many standard {GNN} variants propagate information along the edges of a graph by computing "messages" based only on the representation of the source of each edge. In {GNN}-{FiLM}, the representation of the target node of an edge is additionally used to compute a transformation that can be applied to all incoming messages, allowing feature-wise modulation of the passed information. Results of experiments comparing different {GNN} architectures on three tasks from the literature are presented, based on re-implementations of baseline methods. Hyperparameters for all methods were found using extensive search, yielding somewhat surprising results: differences between baseline models are smaller than reported in the literature. Nonetheless, {GNN}-{FiLM} outperforms baseline methods on a regression task on molecular graphs and performs competitively on other tasks.},
	journaltitle = {{arXiv}:1906.12192 [cs, stat]},
	author = {Brockschmidt, Marc},
	urldate = {2020-06-18},
	date = {2019-11-03},
	eprinttype = {arxiv},
	eprint = {1906.12192},
	keywords = {⛔ No {DOI} found},
	file = {Brockschmidt_2019_GNN-FiLM.pdf:/data/zotero/storage/8VX8AAR8/Brockschmidt_2019_GNN-FiLM.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DND3S9GN/1906.html:text/html}
}

@article{bielik_adversarial_2020,
	title = {Adversarial Robustness for Code},
	url = {http://arxiv.org/abs/2002.04694},
	abstract = {We propose a novel technique which addresses the challenge of learning accurate and robust models of code in a principled way. Our method consists of three key components: (i) learning to abstain from making a prediction if uncertain, (ii) adversarial training, and (iii) representation refinement which learns the program parts relevant for the prediction and abstracts the rest. These components are used to iteratively train multiple models, each of which learns a suitable program representation necessary to make robust predictions on a different subset of the dataset. We instantiated our approach to the task of type inference for dynamically typed languages and demonstrate its effectiveness by learning a model that achieves 88\% accuracy and 84\% robustness. Further, our evaluation shows that using the combination of all three components is key to obtaining accurate and robust models.},
	journaltitle = {{arXiv}:2002.04694 [cs, stat]},
	author = {Bielik, Pavol and Vechev, Martin},
	urldate = {2020-06-18},
	date = {2020-02-11},
	eprinttype = {arxiv},
	eprint = {2002.04694},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Bielik_Vechev_2020_Adversarial Robustness for Code.pdf:/data/zotero/storage/F6JXFY3U/Bielik_Vechev_2020_Adversarial Robustness for Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/J6U3W82U/2002.html:text/html}
}

@inproceedings{brockschmidt_generative_2018,
	title = {Generative Code Modeling with Graphs},
	url = {https://openreview.net/forum?id=Bke4KsA5FX},
	abstract = {Generative models forsource code are an interesting structured prediction problem, requiring to reason about both hard syntactic and semantic constraints as well as about natural, likely programs....},
	eventtitle = {International Conference on Learning Representations},
	author = {Brockschmidt, Marc and Allamanis, Miltiadis and Gaunt, Alexander L. and Polozov, Oleksandr},
	urldate = {2020-06-18},
	date = {2018-09-27},
	file = {Brockschmidt et al_2018_Generative Code Modeling with Graphs.pdf:/data/zotero/storage/LTBVTBKW/Brockschmidt et al_2018_Generative Code Modeling with Graphs.pdf:application/pdf;Snapshot:/data/zotero/storage/VZHNHCD2/forum.html:text/html}
}

@article{rabin_testing_2019,
	title = {Testing Neural Program Analyzers},
	url = {http://arxiv.org/abs/1908.10711},
	abstract = {Deep neural networks have been increasingly used in software engineering and program analysis tasks. They usually take a program and make some predictions about it, e.g., bug prediction. We call these models neural program analyzers. The reliability of neural programs can impact the reliability of the encompassing analyses. In this paper, we describe our ongoing efforts to develop effective techniques for testing neural programs. We discuss the challenges involved in developing such tools and our future plans. In our preliminary experiment on a neural model recently proposed in the literature, we found that the model is very brittle, and simple perturbations in the input can cause the model to make mistakes in its prediction.},
	journaltitle = {{arXiv}:1908.10711 [cs, stat]},
	author = {Rabin, Md Rafiqul Islam and Wang, Ke and Alipour, Mohammad Amin},
	urldate = {2020-06-18},
	date = {2019-09-25},
	eprinttype = {arxiv},
	eprint = {1908.10711},
	keywords = {read, ⛔ No {DOI} found},
	file = {Rabin et al_2019_Testing Neural Program Analyzers.pdf:/data/zotero/storage/HCEQLPSV/Rabin et al_2019_Testing Neural Program Analyzers.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/7AYW2MB7/1908.html:text/html}
}

@inproceedings{tai_improved_2015,
	location = {Beijing, China},
	title = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
	url = {https://www.aclweb.org/anthology/P15-1150},
	doi = {10/gfshwj},
	abstract = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory ({LSTM}) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying {LSTM} structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-{LSTM}, a generalization of {LSTMs} to tree-structured network topologies. Tree-{LSTMs} outperform all existing systems and strong {LSTM} baselines on two tasks: predicting the semantic relatedness of two sentences ({SemEval} 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
	eventtitle = {{ACL}-{IJCNLP} 2015},
	pages = {1556--1566},
	booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
	urldate = {2020-06-18},
	date = {2015-07},
	keywords = {\_tablet},
	file = {Tai et al_2015_Improved Semantic Representations From Tree-Structured Long Short-Term Memory.pdf:/data/zotero/storage/VGXNLNEU/Tai et al_2015_Improved Semantic Representations From Tree-Structured Long Short-Term Memory.pdf:application/pdf}
}

@article{shido_automatic_2019,
	title = {Automatic Source Code Summarization with Extended Tree-{LSTM}},
	url = {http://arxiv.org/abs/1906.08094},
	abstract = {Neural machine translation models are used to automatically generate a document from given source code since this can be regarded as a machine translation task. Source code summarization is one of the components for automatic document generation, which generates a summary in natural language from given source code. This suggests that techniques used in neural machine translation, such as Long Short-Term Memory ({LSTM}), can be used for source code summarization. However, there is a considerable difference between source code and natural language: Source code is essentially \{{\textbackslash}em structured\}, having loops and conditional branching, etc. Therefore, there is some obstacle to apply known machine translation models to source code. Abstract syntax trees ({ASTs}) capture these structural properties and play an important role in recent machine learning studies on source code. Tree-{LSTM} is proposed as a generalization of {LSTMs} for tree-structured data. However, there is a critical issue when applying it to {ASTs}: It cannot handle a tree that contains nodes having an arbitrary number of children and their order simultaneously, which {ASTs} generally have such nodes. To address this issue, we propose an extension of Tree-{LSTM}, which we call {\textbackslash}emph\{Multi-way Tree-{LSTM}\} and apply it for source code summarization. As a result of computational experiments, our proposal achieved better results when compared with several state-of-the-art techniques.},
	journaltitle = {{arXiv}:1906.08094 [cs, stat]},
	author = {Shido, Yusuke and Kobayashi, Yasuaki and Yamamoto, Akihiro and Miyamoto, Atsushi and Matsumura, Tadayuki},
	urldate = {2020-06-18},
	date = {2019-06-20},
	eprinttype = {arxiv},
	eprint = {1906.08094},
	keywords = {to-read, ⛔ No {DOI} found},
	file = {Shido et al_2019_Automatic Source Code Summarization with Extended Tree-LSTM.pdf:/data/zotero/storage/VSU7M6L2/Shido et al_2019_Automatic Source Code Summarization with Extended Tree-LSTM.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8DH6IZZN/1906.html:text/html}
}

@article{ahmad_transformer-based_2020,
	title = {A Transformer-based Approach for Source Code Summarization},
	url = {http://arxiv.org/abs/2005.00653},
	abstract = {Generating a readable summary that describes the functionality of a program is known as source code summarization. In this task, learning code representation by modeling the pairwise relationship between code tokens to capture their long-range dependencies is crucial. To learn code representation for summarization, we explore the Transformer model that uses a self-attention mechanism and has shown to be effective in capturing long-range dependencies. In this work, we show that despite the approach is simple, it outperforms the state-of-the-art techniques by a significant margin. We perform extensive analysis and ablation studies that reveal several important findings, e.g., the absolute encoding of source code tokens' position hinders, while relative encoding significantly improves the summarization performance. We have made our code publicly available to facilitate future research.},
	journaltitle = {{arXiv}:2005.00653 [cs, stat]},
	author = {Ahmad, Wasi Uddin and Chakraborty, Saikat and Ray, Baishakhi and Chang, Kai-Wei},
	urldate = {2020-06-18},
	date = {2020-05-01},
	eprinttype = {arxiv},
	eprint = {2005.00653},
	keywords = {⛔ No {DOI} found},
	file = {Ahmad et al_2020_A Transformer-based Approach for Source Code Summarization.pdf:/data/zotero/storage/TWA6SBGL/Ahmad et al_2020_A Transformer-based Approach for Source Code Summarization.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/NQD6SVWB/2005.html:text/html}
}

@incollection{ben-nun_neural_2018,
	title = {Neural Code Comprehension: A Learnable Representation of Code Semantics},
	url = {http://papers.nips.cc/paper/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics.pdf},
	shorttitle = {Neural Code Comprehension},
	abstract = {With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation ({IR}) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this {IR}, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single {RNN} architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.},
	pages = {3585--3597},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Ben-Nun, Tal and Jakobovits, Alice Shoshana and Hoefler, Torsten},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2020-06-19},
	date = {2018},
	keywords = {read, \_tablet},
	file = {Ben-Nun et al_2018_Neural Code Comprehension.pdf:/data/zotero/storage/LLSXY9JL/Ben-Nun et al_2018_Neural Code Comprehension.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/KK7AGQ6Y/7617-neural-code-comprehension-a-learnable-representation-of-code-semantics.html:text/html}
}

@inproceedings{mou_convolutional_2016,
	title = {Convolutional Neural Networks over Tree Structures for Programming Language Processing},
	rights = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence ({AAAI}), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify {AAAI}, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense {AAAI} may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to {AAAI} in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that {AAAI} copyright and the source are indicated, and that the copies are not used in a way that implies {AAAI} endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the {AAAI} electronic server, and shall not post other {AAAI} copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without {AAAI}’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, {AAAI} grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by {AAAI}, or is withdrawn by the author(s) before acceptance by {AAAI}, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775},
	abstract = {Programming language processing (similar to natural language processing) is a hot research topic in the field of software engineering; it has also aroused growing interest in the artificial intelligence community. However, different from a natural language sentence, a program contains rich, explicit, and complicated structural information. Hence, traditional {NLP} models may be inappropriate for programs. In this paper, we propose a novel tree-based convolutional neural network ({TBCNN}) for programming language processing, in which a convolution kernel is designed over programs' abstract syntax trees to capture structural information. {TBCNN} is a generic architecture for programming language processing; our experiments show its effectiveness in two different program analysis tasks: classifying programs according to functionality, and detecting code snippets of certain patterns. {TBCNN} outperforms baseline methods, including several neural models for {NLP}.},
	eventtitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	booktitle = {Thirtieth {AAAI} Conference on Artificial Intelligence},
	author = {Mou, Lili and Li, Ge and Zhang, Lu and Wang, Tao and Jin, Zhi},
	urldate = {2020-06-19},
	date = {2016-02-21},
	langid = {english},
	keywords = {dataset, skimmed, \_tablet},
	file = {Mou et al_2016_Convolutional Neural Networks over Tree Structures for Programming Language.pdf:/data/zotero/storage/EJ8HYDXP/Mou et al_2016_Convolutional Neural Networks over Tree Structures for Programming Language.pdf:application/pdf;Snapshot:/data/zotero/storage/9UQ37QMH/11775.html:text/html}
}

@inproceedings{xu_neural_2017,
	location = {Dallas, Texas, {USA}},
	title = {Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection},
	isbn = {978-1-4503-4946-8},
	url = {https://doi.org/10.1145/3133956.3134018},
	doi = {10/gg3j6s},
	series = {{CCS} '17},
	abstract = {The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph-matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.},
	pages = {363--376},
	booktitle = {Proceedings of the 2017 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {Xu, Xiaojun and Liu, Chang and Feng, Qian and Yin, Heng and Song, Le and Song, Dawn},
	urldate = {2020-06-19},
	date = {2017-10-30},
	keywords = {out-of-scope, \_tablet},
	file = {Xu et al_2017_Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity.pdf:/data/zotero/storage/LBX7M6Z2/Xu et al_2017_Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity.pdf:application/pdf}
}

@inproceedings{xu_python_2016,
	location = {Seattle, {WA}, {USA}},
	title = {Python probabilistic type inference with natural language support},
	isbn = {978-1-4503-4218-6},
	url = {https://doi.org/10.1145/2950290.2950343},
	doi = {10/gg3j68},
	series = {{FSE} 2016},
	abstract = {We propose a novel type inference technique for Python programs. Type inference is difficult for Python programs due to their heavy dependence on external {APIs} and the dynamic language features. We observe that Python source code often contains a lot of type hints such as attribute accesses and variable names. However, such type hints are not reliable. We hence propose to use probabilistic inference to allow the beliefs of individual type hints to be propagated, aggregated, and eventually converge on probabilities of variable types. Our results show that our technique substantially outperforms a state-of-the-art Python type inference engine based on abstract interpretation.},
	pages = {607--618},
	booktitle = {Proceedings of the 2016 24th {ACM} {SIGSOFT} International Symposium on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Xu, Zhaogui and Zhang, Xiangyu and Chen, Lin and Pei, Kexin and Xu, Baowen},
	urldate = {2020-06-23},
	date = {2016-11-01},
	keywords = {to-read},
	file = {Xu et al_2016_Python probabilistic type inference with natural language support.pdf:/data/zotero/storage/PRSWI5Y6/Xu et al_2016_Python probabilistic type inference with natural language support.pdf:application/pdf}
}

@inproceedings{defreez_path-based_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Path-based function embedding and its application to error-handling specification mining},
	isbn = {978-1-4503-5573-5},
	url = {https://doi.org/10.1145/3236024.3236059},
	doi = {10/gg3j65},
	series = {{ESEC}/{FSE} 2018},
	abstract = {Identifying relationships among program elements is useful for program understanding, debugging, and analysis. One such kind of relationship is synonymy. Function synonyms are functions that play a similar role in code; examples include functions that perform initialization for different device drivers, and functions that implement different symmetric-key encryption schemes. Function synonyms are not necessarily semantically equivalent and can be syntactically dissimilar; consequently, approaches for identifying code clones or functional equivalence cannot be used to identify them. This paper presents Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater}, a technique that learns an embedding mapping each function to a vector in a continuous vector space such that vectors for function synonyms are in close proximity. We compute the function embedding by training a neural network on sentences generated using random walks over the interprocedural control-flow graph. We show the effectiveness of Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} at identifying function synonyms in the Linux kernel. Finally, we apply Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} to the problem of mining error-handling specifications in Linux file systems and drivers. We show that the function synonyms identified by Func2{\textless}pre{\textgreater}vec{\textless}/pre{\textgreater} result in error-handling specifications with high support.},
	pages = {423--433},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {{DeFreez}, Daniel and Thakur, Aditya V. and Rubio-González, Cindy},
	urldate = {2020-06-23},
	date = {2018-10-26},
	keywords = {to-read},
	file = {DeFreez et al_2018_Path-based function embedding and its application to error-handling.pdf:/data/zotero/storage/C9RG9W4J/DeFreez et al_2018_Path-based function embedding and its application to error-handling.pdf:application/pdf}
}

@article{schrouff_inferring_2019,
	title = {Inferring Javascript types using Graph Neural Networks},
	url = {http://arxiv.org/abs/1905.06707},
	abstract = {The recent use of `Big Code' with state-of-the-art deep learning methods offers promising avenues to ease program source code writing and correction. As a first step towards automatic code repair, we implemented a graph neural network model that predicts token types for Javascript programs. The predictions achieve an accuracy above \$90{\textbackslash}\%\$, which improves on previous similar work.},
	journaltitle = {{arXiv}:1905.06707 [cs, stat]},
	author = {Schrouff, Jessica and Wohlfahrt, Kai and Marnette, Bruno and Atkinson, Liam},
	urldate = {2020-06-24},
	date = {2019-05-16},
	eprinttype = {arxiv},
	eprint = {1905.06707},
	keywords = {read, mpnn, ⛔ No {DOI} found},
	file = {Schrouff et al_2019_Inferring Javascript types using Graph Neural Networks.pdf:/data/zotero/storage/XUGRZKWV/Schrouff et al_2019_Inferring Javascript types using Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/TZMFR92B/1905.html:text/html}
}

@article{chen_literature_2019,
	title = {A Literature Study of Embeddings on Source Code},
	url = {http://arxiv.org/abs/1904.03061},
	abstract = {Natural language processing has improved tremendously after the success of word embedding techniques such as word2vec. Recently, the same idea has been applied on source code with encouraging results. In this survey, we aim to collect and discuss the usage of word embedding techniques on programs and source code. The articles in this survey have been collected by asking authors of related work and with an extensive search on Google Scholar. Each article is categorized into five categories: 1. embedding of tokens 2. embedding of functions or methods 3. embedding of sequences or sets of method calls 4. embedding of binary code 5. other embeddings. We also provide links to experimental data and show some remarkable visualization of code embeddings. In summary, word embedding has been successfully applied on different granularities of source code. With access to countless open-source repositories, we see a great potential of applying other data-driven natural language processing techniques on source code in the future.},
	journaltitle = {{arXiv}:1904.03061 [cs, stat]},
	author = {Chen, Zimin and Monperrus, Martin},
	urldate = {2020-06-24},
	date = {2019-04-05},
	eprinttype = {arxiv},
	eprint = {1904.03061},
	keywords = {read, ⛔ No {DOI} found},
	file = {Chen_Monperrus_2019_A Literature Study of Embeddings on Source Code.pdf:/data/zotero/storage/YB7DAWYF/Chen_Monperrus_2019_A Literature Study of Embeddings on Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/IM87K93M/1904.html:text/html}
}

@article{alon_code2seq_2019,
	title = {code2seq: Generating Sequences from Structured Representations of Code},
	url = {http://arxiv.org/abs/1808.01400},
	shorttitle = {code2seq},
	abstract = {The ability to generate natural language sequences from source code snippets has a variety of applications such as code summarization, documentation, and retrieval. Sequence-to-sequence (seq2seq) models, adopted from neural machine translation ({NMT}), have achieved state-of-the-art performance on these tasks by treating source code as a sequence of tokens. We present \$\{{\textbackslash}rm \{{\textbackslash}scriptsize {CODE}2SEQ\}\}\$: an alternative approach that leverages the syntactic structure of programming languages to better encode source code. Our model represents a code snippet as the set of compositional paths in its abstract syntax tree ({AST}) and uses attention to select the relevant paths while decoding. We demonstrate the effectiveness of our approach for two tasks, two programming languages, and four datasets of up to \$16\$M examples. Our model significantly outperforms previous models that were specifically designed for programming languages, as well as state-of-the-art {NMT} models. An interactive online demo of our model is available at http://code2seq.org. Our code, data and trained models are available at http://github.com/tech-srl/code2seq.},
	journaltitle = {{arXiv}:1808.01400 [cs, stat]},
	author = {Alon, Uri and Brody, Shaked and Levy, Omer and Yahav, Eran},
	urldate = {2020-06-24},
	date = {2019-02-21},
	eprinttype = {arxiv},
	eprint = {1808.01400},
	keywords = {⛔ No {DOI} found},
	file = {Alon et al_2019_code2seq.pdf:/data/zotero/storage/G8HURE7G/Alon et al_2019_code2seq.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/MLX724AD/1808.html:text/html}
}

@article{zhou_graph_2019,
	title = {Graph Neural Networks: A Review of Methods and Applications},
	url = {http://arxiv.org/abs/1812.08434},
	shorttitle = {Graph Neural Networks},
	abstract = {Lots of learning tasks require dealing with graph data which contains rich relation information among elements. Modeling physics system, learning molecular fingerprints, predicting protein interface, and classifying diseases require a model to learn from graph inputs. In other domains such as learning from non-structural data like texts and images, reasoning on extracted structures, like the dependency tree of sentences and the scene graph of images, is an important research topic which also needs graph reasoning models. Graph neural networks ({GNNs}) are connectionist models that capture the dependence of graphs via message passing between the nodes of graphs. Unlike standard neural networks, graph neural networks retain a state that can represent information from its neighborhood with arbitrary depth. Although the primitive {GNNs} have been found difficult to train for a fixed point, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful learning with them. In recent years, systems based on variants of graph neural networks such as graph convolutional network ({GCN}), graph attention network ({GAT}), gated graph neural network ({GGNN}) have demonstrated ground-breaking performance on many tasks mentioned above. In this survey, we provide a detailed review over existing graph neural network models, systematically categorize the applications, and propose four open problems for future research.},
	journaltitle = {{arXiv}:1812.08434 [cs, stat]},
	author = {Zhou, Jie and Cui, Ganqu and Zhang, Zhengyan and Yang, Cheng and Liu, Zhiyuan and Wang, Lifeng and Li, Changcheng and Sun, Maosong},
	urldate = {2020-06-24},
	date = {2019-07-10},
	eprinttype = {arxiv},
	eprint = {1812.08434},
	keywords = {⛔ No {DOI} found},
	file = {Zhou et al_2019_Graph Neural Networks.pdf:/data/zotero/storage/W7G7IGR7/Zhou et al_2019_Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ZLSV22XP/1812.html:text/html}
}

@article{le_deep_2020,
	title = {Deep Learning for Source Code Modeling and Generation: Models, Applications, and Challenges},
	volume = {53},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3383458},
	doi = {10/gg3j6b},
	shorttitle = {Deep Learning for Source Code Modeling and Generation},
	abstract = {Deep Learning ({DL}) techniques for Natural Language Processing have been evolving remarkably fast. Recently, the {DL} advances in language modeling, machine translation, and paragraph understanding are so prominent that the potential of {DL} in Software Engineering cannot be overlooked, especially in the field of program learning. To facilitate further research and applications of {DL} in this field, we provide a comprehensive review to categorize and investigate existing {DL} methods for source code modeling and generation. To address the limitations of the traditional source code models, we formulate common program learning tasks under an encoder-decoder framework. After that, we introduce recent {DL} mechanisms suitable to solve such problems. Then, we present the state-of-the-art practices and discuss their challenges with some recommendations for practitioners and researchers as well.},
	pages = {62:1--62:38},
	number = {3},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Le, Triet H. M. and Chen, Hao and Babar, Muhammad Ali},
	urldate = {2020-06-24},
	date = {2020-06-12},
	file = {Le et al_2020_Deep Learning for Source Code Modeling and Generation.pdf:/data/zotero/storage/ZAXLNWEW/Le et al_2020_Deep Learning for Source Code Modeling and Generation.pdf:application/pdf}
}

@inproceedings{kang_assessing_2019,
	title = {Assessing the Generalizability of Code2vec Token Embeddings},
	doi = {10/ggsskz},
	abstract = {Many Natural Language Processing ({NLP}) tasks, such as sentiment analysis or syntactic parsing, have benefited from the development of word embedding models. In particular, regardless of the training algorithms, the learned embeddings have often been shown to be generalizable to different {NLP} tasks. In contrast, despite recent momentum on word embeddings for source code, the literature lacks evidence of their generalizability beyond the example task they have been trained for. In this experience paper, we identify 3 potential downstream tasks, namely code comments generation, code authorship identification, and code clones detection, that source code token embedding models can be applied to. We empirically assess a recently proposed code token embedding model, namely code2vec's token embeddings. Code2vec was trained on the task of predicting method names, and while there is potential for using the vectors it learns on other tasks, it has not been explored in literature. Therefore, we fill this gap by focusing on its generalizability for the tasks we have identified. Eventually, we show that source code token embeddings cannot be readily leveraged for the downstream tasks. Our experiments even show that our attempts to use them do not result in any improvements over less sophisticated methods. We call for more research into effective and general use of code embeddings.},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {1--12},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Kang, Hong Jin and Bissyandé, Tegawendé F. and Lo, David},
	date = {2019-11},
	note = {{ISSN}: 2643-1572},
	file = {Kang et al_2019_Assessing the Generalizability of Code2vec Token Embeddings.pdf:/data/zotero/storage/H9NA7GKU/Kang et al_2019_Assessing the Generalizability of Code2vec Token Embeddings.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/HD77Q9XY/8952475.html:text/html}
}

@article{kanade_pre-trained_2019,
	title = {Pre-trained Contextual Embedding of Source Code},
	url = {http://arxiv.org/abs/2001.00059},
	abstract = {The source code of a program not only serves as a formal description of an executable task, but it also serves to communicate developer intent in a human-readable form. To facilitate this, developers use meaningful identifier names and natural-language documentation. This makes it possible to successfully apply sequence-modeling approaches, shown to be effective in natural-language processing, to source code. A major advancement in natural-language understanding has been the use of pre-trained token embeddings; {BERT} and other works have further shown that pre-trained contextual embeddings can be extremely powerful and can be fine-tuned effectively for a variety of downstream supervised tasks. Inspired by these developments, we present the first attempt to replicate this success on source code. We curate a massive corpus of Python programs from {GitHub} to pre-train a {BERT} model, which we call Code Understanding {BERT} ({CuBERT}). We also pre-train Word2Vec embeddings on the same dataset. We create a benchmark of five classification tasks and compare fine-tuned {CuBERT} against sequence models trained with and without the Word2Vec embeddings. Our results show that {CuBERT} outperforms the baseline methods by a margin of 2.9-22\%. We also show its superiority when fine-tuned with smaller datasets, and over fewer epochs. We further evaluate {CuBERT}'s effectiveness on a joint classification, localization and repair task involving prediction of two pointers.},
	journaltitle = {{arXiv}:2001.00059 [cs]},
	author = {Kanade, Aditya and Maniatis, Petros and Balakrishnan, Gogul and Shi, Kensen},
	urldate = {2020-06-24},
	date = {2019-12-21},
	eprinttype = {arxiv},
	eprint = {2001.00059},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Kanade et al_2019_Pre-trained Contextual Embedding of Source Code.pdf:/data/zotero/storage/ZI9MPRSM/Kanade et al_2019_Pre-trained Contextual Embedding of Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/56K4UP4G/2001.html:text/html}
}

@article{shi_learning_2020,
	title = {Learning Execution through Neural Code Fusion},
	url = {http://arxiv.org/abs/1906.07181},
	abstract = {As the performance of computer systems stagnates due to the end of Moore's Law, there is a need for new models that can understand and optimize the execution of general purpose code. While there is a growing body of work on using Graph Neural Networks ({GNNs}) to learn representations of source code, these representations do not understand how code dynamically executes. In this work, we propose a new approach to use {GNNs} to learn fused representations of general source code and its execution. Our approach defines a multi-task {GNN} over low-level representations of source code and program state (i.e., assembly code and dynamic memory states), converting complex source code constructs and complex data structures into a simpler, more uniform format. We show that this leads to improved performance over similar methods that do not use execution and it opens the door to applying {GNN} models to new tasks that would not be feasible from static code alone. As an illustration of this, we apply the new model to challenging dynamic tasks (branch prediction and prefetching) from the {SPEC} {CPU} benchmark suite, outperforming the state-of-the-art by 26\% and 45\% respectively. Moreover, we use the learned fused graph embeddings to demonstrate transfer learning with high performance on an indirectly related task (algorithm classification).},
	journaltitle = {{arXiv}:1906.07181 [cs, stat]},
	author = {Shi, Zhan and Swersky, Kevin and Tarlow, Daniel and Ranganathan, Parthasarathy and Hashemi, Milad},
	urldate = {2020-06-24},
	date = {2020-03-10},
	eprinttype = {arxiv},
	eprint = {1906.07181},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Shi et al_2020_Learning Execution through Neural Code Fusion.pdf:/data/zotero/storage/TF6M96DW/Shi et al_2020_Learning Execution through Neural Code Fusion.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DD2DEP45/1906.html:text/html}
}

@book{paasen_execution_2016,
	title = {Execution Traces as a Powerful Data Representation for Intelligent Tutoring Systems for Programming},
	url = {https://eric.ed.gov/?id=ED592662},
	abstract = {The first intelligent tutoring systems for computer programming have been proposed more than 30 years ago, mostly focusing on well defined programming tasks e.g. in the context of logic programming. Recent systems also teach complex programs, where explicit modelling of every possible program and mistake is no longer possible. Such systems are based on data-driven approaches, which focus on the syntax of a program or consider the output for example cases. However, the system's understanding of student programs could be enriched by a deeper focus on the actual execution of a program. This requires a suitable data representation which encodes information of programming style as well as its functionality in a suitable way, thus offering entry points for automated feedback generation. In this contribution we propose a representation of computer programs via execution traces for example input and demonstrate the power of this representation in three key challenges for intelligent tutoring systems: identifying the underlying solution strategy, identifying erroneous solutions and locating the errors in erroneous programs for feedback display. [For the full proceedings, see {ED}592609.]},
	publisher = {International Educational Data Mining Society},
	author = {Paaßen, Benjamin and Jensen, Joris and Hammer, Barbara},
	urldate = {2020-06-24},
	date = {2016},
	langid = {english},
	note = {Publication Title: International Educational Data Mining Society},
	file = {Paaßen et al_2016_Execution Traces as a Powerful Data Representation for Intelligent Tutoring.pdf:/data/zotero/storage/TYRCVD6R/Paaßen et al_2016_Execution Traces as a Powerful Data Representation for Intelligent Tutoring.pdf:application/pdf;Snapshot:/data/zotero/storage/E7H4DSZH/eric.ed.gov.html:text/html}
}

@article{shepperd_researcher_2014,
	title = {Researcher Bias: The Use of Machine Learning in Software Defect Prediction},
	volume = {40},
	issn = {1939-3520},
	doi = {10/gfsckz},
	shorttitle = {Researcher Bias},
	abstract = {Background. The ability to predict defect-prone software components would be valuable. Consequently, there have been many empirical studies to evaluate the performance of different techniques endeavouring to accomplish this effectively. However no one technique dominates and so designing a reliable defect prediction model remains problematic. Objective. We seek to make sense of the many conflicting experimental results and understand which factors have the largest effect on predictive performance. Method. We conduct a meta-analysis of all relevant, high quality primary studies of defect prediction to determine what factors influence predictive performance. This is based on 42 primary studies that satisfy our inclusion criteria that collectively report 600 sets of empirical prediction results. By reverse engineering a common response variable we build a random effects {ANOVA} model to examine the relative contribution of four model building factors (classifier, data set, input metrics and researcher group) to model prediction performance. Results. Surprisingly we find that the choice of classifier has little impact upon performance (1.3 percent) and in contrast the major (31 percent) explanatory factor is the researcher group. It matters more who does the work than what is done. Conclusion. To overcome this high level of researcher bias, defect prediction researchers should (i) conduct blind analysis, (ii) improve reporting protocols and (iii) conduct more intergroup studies in order to alleviate expertise issues. Lastly, research is required to determine whether this bias is prevalent in other applications domains.},
	pages = {603--616},
	number = {6},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Shepperd, Martin and Bowes, David and Hall, Tracy},
	date = {2014-06},
	note = {Conference Name: {IEEE} Transactions on Software Engineering},
	file = {IEEE Xplore Abstract Record:/data/zotero/storage/FU34QWEI/6824804.html:text/html;Shepperd et al_2014_Researcher Bias.pdf:/data/zotero/storage/UXASBBBZ/Shepperd et al_2014_Researcher Bias.pdf:application/pdf}
}

@article{rabin_evaluation_2020,
	title = {Evaluation of Generalizability of Neural Program Analyzers under Semantic-Preserving Transformations},
	url = {http://arxiv.org/abs/2004.07313},
	abstract = {The abundance of publicly available source code repositories, in conjunction with the advances in neural networks, has enabled data-driven approaches to program analysis. These approaches, called neural program analyzers, use neural networks to extract patterns in the programs for tasks ranging from development productivity to program reasoning. Despite the growing popularity of neural program analyzers, the extent to which their results are generalizable is unknown. In this paper, we perform a large-scale evaluation of the generalizability of two popular neural program analyzers using seven semantically-equivalent transformations of programs. Our results caution that in many cases the neural program analyzers fail to generalize well, sometimes to programs with negligible textual differences. The results provide the initial stepping stones for quantifying robustness in neural program analyzers.},
	journaltitle = {{arXiv}:2004.07313 [cs]},
	author = {Rabin, Md Rafiqul Islam and Alipour, Mohammad Amin},
	urldate = {2020-06-24},
	date = {2020-04-15},
	eprinttype = {arxiv},
	eprint = {2004.07313},
	keywords = {⛔ No {DOI} found},
	file = {Rabin_Alipour_2020_Evaluation of Generalizability of Neural Program Analyzers under.pdf:/data/zotero/storage/Y6JLVEJD/Rabin_Alipour_2020_Evaluation of Generalizability of Neural Program Analyzers under.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/YZY2EBGN/2004.html:text/html}
}

@inproceedings{khaled_saifullah_exploring_2020,
	title = {Exploring Type Inference Techniques of Dynamically Typed Languages},
	doi = {10/gg3j6h},
	abstract = {Developers often prefer dynamically typed programming languages, such as {JavaScript}, because such languages do not require explicit type declarations. However, such a feature hinders software engineering tasks, such as code completion, type related bug fixes and so on. Deep learning-based techniques are proposed in the literature to infer the types of code elements in {JavaScript} snippets. These techniques are computationally expensive. While several type inference techniques have been developed to detect types in code snippets written in statically typed languages, it is not clear how effective those techniques are for inferring types in dynamically typed languages, such as {JavaScript}. In this paper, we investigate the type inference techniques of {JavaScript} to understand the above two issues further. While doing that we propose a new technique that considers the locally specific code tokens as the context to infer the types of code elements. The evaluation result shows that the proposed technique is 20-47\% more accurate than the statically typed language-based techniques and 5-14 times faster than the deep learning techniques without sacrificing accuracy. Our analysis of sensitivity, overlapping of predicted types and the number of training examples justify the importance of our technique.},
	eventtitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	pages = {70--80},
	booktitle = {2020 {IEEE} 27th International Conference on Software Analysis, Evolution and Reengineering ({SANER})},
	author = {Khaled Saifullah, C. M. and Asaduzzaman, Muhammad and Roy, Chanchal K.},
	date = {2020-02},
	note = {{ISSN}: 1534-5351},
	file = {Khaled Saifullah et al_2020_Exploring Type Inference Techniques of Dynamically Typed Languages.pdf:/data/zotero/storage/4UET3L4Z/Khaled Saifullah et al_2020_Exploring Type Inference Techniques of Dynamically Typed Languages.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/IHJX7B23/9054814.html:text/html}
}

@article{wehr_learning_2019,
	title = {Learning Semantic Vector Representations of Source Code via a Siamese Neural Network},
	url = {http://arxiv.org/abs/1904.11968},
	abstract = {The abundance of open-source code, coupled with the success of recent advances in deep learning for natural language processing, has given rise to a promising new application of machine learning to source code. In this work, we explore the use of a Siamese recurrent neural network model on Python source code to create vectors which capture the semantics of code. We evaluate the quality of embeddings by identifying which problem from a programming competition the code solves. Our model significantly outperforms a bag-of-tokens embedding, providing promising results for improving code embeddings that can be used in future software engineering tasks.},
	journaltitle = {{arXiv}:1904.11968 [cs, stat]},
	author = {Wehr, David and Fede, Halley and Pence, Eleanor and Zhang, Bo and Ferreira, Guilherme and Walczyk, John and Hughes, Joseph},
	urldate = {2020-06-24},
	date = {2019-04-26},
	eprinttype = {arxiv},
	eprint = {1904.11968},
	keywords = {low-prio, ⛔ No {DOI} found},
	file = {Wehr et al_2019_Learning Semantic Vector Representations of Source Code via a Siamese Neural.pdf:/data/zotero/storage/DWU4E26P/Wehr et al_2019_Learning Semantic Vector Representations of Source Code via a Siamese Neural.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/84X6LJ3M/1904.html:text/html}
}

@inproceedings{compton_embedding_2020,
	title = {Embedding Java classes with code2vec: improvements from variable obfuscation [Accepted]},
	url = {https://researchcommons.waikato.ac.nz/handle/10289/13618},
	doi = {10.1145/3379597.3387445},
	shorttitle = {Embedding Java classes with code2vec},
	abstract = {Automatic source code analysis in key areas of software engineering, such as code security, can benefit from Machine Learning ({ML}). However, many standard {ML} approaches require a numeric representation of data and cannot be applied directly to source code. Thus, to enable {ML}, we need to embed source code into numeric feature vectors while maintaining the semantics of the code as much as possible. code2vec is a recently released embedding approach that uses the proxy task of method name prediction to map Java methods to feature vectors. However, experimentation with code2vec shows that it learns to rely on variable names for prediction, causing it to be easily fooled by typos or adversarial attacks. Moreover, it is only able to embed individual Java methods and cannot embed an entire collection of methods such as those present in a typical Java class, making it difficult to perform predictions at the class level (e.g., for the identification of malicious Java classes). Both shortcomings are addressed in the research presented in this paper. We investigate the effect of obfuscating variable names during training of a code2vec model to force it to rely on the structure of the code rather than specific names and consider a simple approach to creating class-level embeddings by aggregating sets of method embeddings. Our results, obtained on a challenging new collection of source-code classification problems, indicate that obfuscating variable names produces an embedding model that is both impervious to variable naming and more accurately reflects code semantics. The datasets, models, and code are shared1 for further {ML} research on source code.},
	eventtitle = {{MSR} 2020},
	booktitle = {{MSR} 2020},
	publisher = {{ACM}},
	author = {Compton, Rhys and Frank, Eibe and Patros, Panagiotis and Koay, Abigail},
	urldate = {2020-06-24},
	date = {2020},
	langid = {english},
	note = {Accepted: 2020-06-11T02:18:19Z},
	keywords = {⚠️ Invalid {DOI}},
	file = {Compton et al_2020_Embedding Java classes with code2vec.pdf:/data/zotero/storage/2M8AM37D/Compton et al_2020_Embedding Java classes with code2vec.pdf:application/pdf;Snapshot:/data/zotero/storage/P3K6QHAM/13618.html:text/html}
}

@incollection{si_learning_2018,
	title = {Learning Loop Invariants for Program Verification},
	url = {http://papers.nips.cc/paper/8001-learning-loop-invariants-for-program-verification.pdf},
	pages = {7751--7762},
	booktitle = {Advances in Neural Information Processing Systems 31},
	publisher = {Curran Associates, Inc.},
	author = {Si, Xujie and Dai, Hanjun and Raghothaman, Mukund and Naik, Mayur and Song, Le},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	urldate = {2020-06-25},
	date = {2018},
	keywords = {mpnn},
	file = {NIPS Snapshot:/data/zotero/storage/ABZQEEHM/8001-learning-loop-invariants-forprogram-verification.html:text/html;Si et al_2018_Learning Loop Invariants for Program Verification.pdf:/data/zotero/storage/8H9SZW4P/Si et al_2018_Learning Loop Invariants for Program Verification.pdf:application/pdf}
}

@inproceedings{lacomis_dire_2019,
	title = {{DIRE}: A Neural Approach to Decompiled Identifier Naming},
	doi = {10/gg3j6f},
	shorttitle = {{DIRE}},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine ({DIRE}), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from {GitHub}. Our results show that on this corpus {DIRE} can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	eventtitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	pages = {628--639},
	booktitle = {2019 34th {IEEE}/{ACM} International Conference on Automated Software Engineering ({ASE})},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	date = {2019-11},
	note = {{ISSN}: 2643-1572},
	keywords = {\_tablet},
	file = {Lacomis et al_2019_DIRE.pdf:/data/zotero/storage/II5VUXSN/Lacomis et al_2019_DIRE.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/7JE8R8G4/8952404.html:text/html}
}

@incollection{shin_program_2019,
	title = {Program Synthesis and Semantic Parsing with Learned Code Idioms},
	url = {http://papers.nips.cc/paper/9265-program-synthesis-and-semantic-parsing-with-learned-code-idioms.pdf},
	pages = {10825--10835},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Shin, Eui Chul and Allamanis, Miltiadis and Brockschmidt, Marc and Polozov, Alex},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-06-25},
	date = {2019},
	file = {Shin et al_2019_Program Synthesis and Semantic Parsing with Learned Code Idioms.pdf:/data/zotero/storage/GPCPYWY6/Shin et al_2019_Program Synthesis and Semantic Parsing with Learned Code Idioms.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/75228INK/9265-program-synthesis-and-semantic-parsing-with-learned-code-idioms.html:text/html}
}

@article{lipton_critical_2015,
	title = {A Critical Review of Recurrent Neural Networks for Sequence Learning},
	url = {http://arxiv.org/abs/1506.00019},
	abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks ({RNNs}) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory ({LSTM}) and bidirectional ({BRNN}) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
	journaltitle = {{arXiv}:1506.00019 [cs]},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	urldate = {2020-06-25},
	date = {2015-10-17},
	eprinttype = {arxiv},
	eprint = {1506.00019},
	keywords = {⛔ No {DOI} found},
	file = {Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:/data/zotero/storage/89YIZ97S/Lipton et al_2015_A Critical Review of Recurrent Neural Networks for Sequence Learning.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/SZDVJMWQ/1506.html:text/html}
}

@article{gilmer_neural_2017,
	title = {Neural Message Passing for Quantum Chemistry},
	url = {http://arxiv.org/abs/1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks ({MPNNs}) and explore additional novel variations within this framework. Using {MPNNs} we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	journaltitle = {{arXiv}:1704.01212 [cs]},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	urldate = {2020-06-27},
	date = {2017-06-12},
	eprinttype = {arxiv},
	eprint = {1704.01212},
	keywords = {read, ⛔ No {DOI} found},
	file = {Gilmer et al_2017_Neural Message Passing for Quantum Chemistry.pdf:/data/zotero/storage/I7RMPB2I/Gilmer et al_2017_Neural Message Passing for Quantum Chemistry.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/78R9WQEE/1704.html:text/html}
}

@article{wang_detecting_2020,
	title = {Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract Syntax Tree},
	url = {http://arxiv.org/abs/2002.08653},
	abstract = {Code clones are semantically similar code fragments pairs that are syntactically similar or different. Detection of code clones can help to reduce the cost of software maintenance and prevent bugs. Numerous approaches of detecting code clones have been proposed previously, but most of them focus on detecting syntactic clones and do not work well on semantic clones with different syntactic features. To detect semantic clones, researchers have tried to adopt deep learning for code clone detection to automatically learn latent semantic features from data. Especially, to leverage grammar information, several approaches used abstract syntax trees ({AST}) as input and achieved significant progress on code clone benchmarks in various programming languages. However, these {AST}-based approaches still can not fully leverage the structural information of code fragments, especially semantic information such as control flow and data flow. To leverage control and data flow information, in this paper, we build a graph representation of programs called flow-augmented abstract syntax tree ({FA}-{AST}). We construct {FA}-{AST} by augmenting original {ASTs} with explicit control and data flow edges. Then we apply two different types of graph neural networks ({GNN}) on {FA}-{AST} to measure the similarity of code pairs. As far as we have concerned, we are the first to apply graph neural networks on the domain of code clone detection. We apply our {FA}-{AST} and graph neural networks on two Java datasets: Google Code Jam and {BigCloneBench}. Our approach outperforms the state-of-the-art approaches on both Google Code Jam and {BigCloneBench} tasks.},
	journaltitle = {{arXiv}:2002.08653 [cs]},
	author = {Wang, Wenhan and Li, Ge and Ma, Bo and Xia, Xin and Jin, Zhi},
	urldate = {2020-06-27},
	date = {2020-02-20},
	eprinttype = {arxiv},
	eprint = {2002.08653},
	keywords = {⛔ No {DOI} found},
	file = {Wang et al_2020_Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract.pdf:/data/zotero/storage/ISPWS4AA/Wang et al_2020_Detecting Code Clones with Graph Neural Networkand Flow-Augmented Abstract.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/WWIH8WJG/2002.html:text/html}
}

@article{vasic_neural_2019,
	title = {Neural Program Repair by Jointly Learning to Localize and Repair},
	url = {http://arxiv.org/abs/1904.01720},
	abstract = {Due to its potential to improve programmer productivity and software quality, automated program repair has been an active topic of research. Newer techniques harness neural networks to learn directly from examples of buggy programs and their fixes. In this work, we consider a recently identified class of bugs called variable-misuse bugs. The state-of-the-art solution for variable misuse enumerates potential fixes for all possible bug locations in a program, before selecting the best prediction. We show that it is beneficial to train a model that jointly and directly localizes and repairs variable-misuse bugs. We present multi-headed pointer networks for this purpose, with one head each for localization and repair. The experimental results show that the joint model significantly outperforms an enumerative solution that uses a pointer based model for repair alone.},
	journaltitle = {{arXiv}:1904.01720 [cs, stat]},
	author = {Vasic, Marko and Kanade, Aditya and Maniatis, Petros and Bieber, David and Singh, Rishabh},
	urldate = {2020-06-27},
	date = {2019-04-02},
	eprinttype = {arxiv},
	eprint = {1904.01720},
	keywords = {⛔ No {DOI} found},
	file = {Vasic et al_2019_Neural Program Repair by Jointly Learning to Localize and Repair.pdf:/data/zotero/storage/V9I3CVIG/Vasic et al_2019_Neural Program Repair by Jointly Learning to Localize and Repair.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/X36VZ3I8/1904.html:text/html}
}

@article{hamilton_representation_2018,
	title = {Representation Learning on Graphs: Methods and Applications},
	url = {http://arxiv.org/abs/1709.05584},
	shorttitle = {Representation Learning on Graphs},
	abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
	journaltitle = {{arXiv}:1709.05584 [cs]},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	urldate = {2020-06-27},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1709.05584},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Hamilton et al_2018_Representation Learning on Graphs.pdf:/data/zotero/storage/2B6RR9LL/Hamilton et al_2018_Representation Learning on Graphs.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8M54ITCS/1709.html:text/html}
}

@inproceedings{devlin_robustfill_2017,
	location = {Sydney, {NSW}, Australia},
	title = {{RobustFill}: neural program learning under noisy I/O},
	series = {{ICML}'17},
	shorttitle = {{RobustFill}},
	abstract = {The problem of automatically generating a computer program from some specification has been studied since the early days of {AI}. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention {RNN} to allow encoding of variable-sized sets of I/O pairs, which achieve 92\% accuracy on a real-world test set, compared to the 34\% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely.},
	pages = {990--998},
	booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
	publisher = {{JMLR}.org},
	author = {Devlin, Jacob and Uesato, Jonathan and Bhupatiraju, Surya and Singh, Rishabh and Mohamed, Abdel-rahman and Kohli, Pushmeet},
	urldate = {2020-06-27},
	date = {2017-08-06},
	keywords = {to-read},
	file = {Devlin et al_2017_RobustFill.pdf:/data/zotero/storage/IHA7L8EE/Devlin et al_2017_RobustFill.pdf:application/pdf}
}

@inproceedings{kalyan_neural-guided_2018,
	title = {Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples},
	url = {https://openreview.net/forum?id=rywDjg-RW},
	abstract = {Synthesizing user-intended programs from a small number of input-output exam-
  ples is a challenging problem with several important applications like spreadsheet
  manipulation, data wrangling and...},
	eventtitle = {International Conference on Learning Representations},
	author = {Kalyan, Ashwin and Mohta, Abhishek and Polozov, Oleksandr and Batra, Dhruv and Jain, Prateek and Gulwani, Sumit},
	urldate = {2020-06-27},
	date = {2018-02-15},
	file = {Kalyan et al_2018_Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples.pdf:/data/zotero/storage/C6PKJE8F/Kalyan et al_2018_Neural-Guided Deductive Search for Real-Time Program Synthesis from Examples.pdf:application/pdf;Snapshot:/data/zotero/storage/HIBB8E4A/forum.html:text/html}
}

@article{velickovic_neural_2020,
	title = {Neural Execution of Graph Algorithms},
	url = {http://arxiv.org/abs/1910.10593},
	abstract = {Graph Neural Networks ({GNNs}) are a powerful representational tool for solving problems on graph-structured inputs. In almost all cases so far, however, they have been applied to directly recovering a final solution from raw inputs, without explicit guidance on how to structure their problem-solving. Here, instead, we focus on learning in the space of algorithms: we train several state-of-the-art {GNN} architectures to imitate individual steps of classical graph algorithms, parallel (breadth-first search, Bellman-Ford) as well as sequential (Prim's algorithm). As graph algorithms usually rely on making discrete decisions within neighbourhoods, we hypothesise that maximisation-based message passing neural networks are best-suited for such objectives, and validate this claim empirically. We also demonstrate how learning in the space of algorithms can yield new opportunities for positive transfer between tasks---showing how learning a shortest-path algorithm can be substantially improved when simultaneously learning a reachability algorithm.},
	journaltitle = {{arXiv}:1910.10593 [cs, stat]},
	author = {Veličković, Petar and Ying, Rex and Padovano, Matilde and Hadsell, Raia and Blundell, Charles},
	urldate = {2020-06-27},
	date = {2020-01-15},
	eprinttype = {arxiv},
	eprint = {1910.10593},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Veličković et al_2020_Neural Execution of Graph Algorithms.pdf:/data/zotero/storage/IVD5DIBJ/Veličković et al_2020_Neural Execution of Graph Algorithms.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/ZWIZPWB8/1910.html:text/html}
}

@article{yan_neural_2020,
	title = {Neural Execution Engines: Learning to Execute Subroutines},
	url = {http://arxiv.org/abs/2006.08084},
	shorttitle = {Neural Execution Engines},
	abstract = {A significant effort has been made to train neural networks that replicate algorithmic reasoning, but they often fail to learn the abstract concepts underlying these algorithms. This is evidenced by their inability to generalize to data distributions that are outside of their restricted training sets, namely larger inputs and unseen data. We study these generalization issues at the level of numerical subroutines that comprise common algorithms like sorting, shortest paths, and minimum spanning trees. First, we observe that transformer-based sequence-to-sequence models can learn subroutines like sorting a list of numbers, but their performance rapidly degrades as the length of lists grows beyond those found in the training set. We demonstrate that this is due to attention weights that lose fidelity with longer sequences, particularly when the input numbers are numerically similar. To address the issue, we propose a learned conditional masking mechanism, which enables the model to strongly generalize far outside of its training range with near-perfect accuracy on a variety of algorithms. Second, to generalize to unseen data, we show that encoding numbers with a binary representation leads to embeddings with rich structure once trained on downstream tasks like addition or multiplication. This allows the embedding to handle missing data by faithfully interpolating numbers not seen during training.},
	journaltitle = {{arXiv}:2006.08084 [cs, stat]},
	author = {Yan, Yujun and Swersky, Kevin and Koutra, Danai and Ranganathan, Parthasarathy and Hashemi, Milad},
	urldate = {2020-06-27},
	date = {2020-06-22},
	eprinttype = {arxiv},
	eprint = {2006.08084},
	keywords = {⛔ No {DOI} found},
	file = {Yan et al_2020_Neural Execution Engines.pdf:/data/zotero/storage/R5Z9WGZL/Yan et al_2020_Neural Execution Engines.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/X5CRABTM/2006.html:text/html}
}

@article{parisotto_neuro-symbolic_2016,
	title = {Neuro-Symbolic Program Synthesis},
	url = {http://arxiv.org/abs/1611.01855},
	abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
	journaltitle = {{arXiv}:1611.01855 [cs]},
	author = {Parisotto, Emilio and Mohamed, Abdel-rahman and Singh, Rishabh and Li, Lihong and Zhou, Dengyong and Kohli, Pushmeet},
	urldate = {2020-06-28},
	date = {2016-11-06},
	eprinttype = {arxiv},
	eprint = {1611.01855},
	keywords = {⛔ No {DOI} found},
	file = {Parisotto et al_2016_Neuro-Symbolic Program Synthesis.pdf:/data/zotero/storage/Y6AZQGZA/Parisotto et al_2016_Neuro-Symbolic Program Synthesis.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/PBTVXW7A/1611.html:text/html}
}

@article{nguyen_suggesting_nodate,
	title = {Suggesting Natural Method Names to Check Name Consistencies},
	author = {Nguyen, Son and Phan, Hung and Le, Trinh and Nguyen, Tien N.},
	keywords = {⛔ No {DOI} found},
	file = {Nguyen et al_Suggesting Natural Method Names to Check Name Consistencies.pdf:/data/zotero/storage/47VQSDNE/Nguyen et al_Suggesting Natural Method Names to Check Name Consistencies.pdf:application/pdf}
}

@article{leclair_improved_2020,
	title = {Improved Code Summarization via a Graph Neural Network},
	url = {http://arxiv.org/abs/2004.02843},
	abstract = {Automatic source code summarization is the task of generating natural language descriptions for source code. Automatic code summarization is a rapidly expanding research area, especially as the community has taken greater advantage of advances in neural network and {AI} technologies. In general, source code summarization techniques use the source code as input and outputs a natural language description. Yet a strong consensus is developing that using structural information as input leads to improved performance. The first approaches to use structural information flattened the {AST} into a sequence. Recently, more complex approaches based on random {AST} paths or graph neural networks have improved on the models using flattened {ASTs}. However, the literature still does not describe the using a graph neural network together with source code sequence as separate inputs to a model. Therefore, in this paper, we present an approach that uses a graph-based neural architecture that better matches the default structure of the {AST} to generate these summaries. We evaluate our technique using a data set of 2.1 million Java method-comment pairs and show improvement over four baseline techniques, two from the software engineering literature, and two from machine learning literature.},
	journaltitle = {{arXiv}:2004.02843 [cs]},
	author = {{LeClair}, Alexander and Haque, Sakib and Wu, Lingfei and {McMillan}, Collin},
	urldate = {2020-06-28},
	date = {2020-04-07},
	eprinttype = {arxiv},
	eprint = {2004.02843},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {LeClair et al_2020_Improved Code Summarization via a Graph Neural Network.pdf:/data/zotero/storage/VFX8K9G9/LeClair et al_2020_Improved Code Summarization via a Graph Neural Network.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/2MHRCUU3/2004.html:text/html}
}

@article{yefet_adversarial_2020,
	title = {Adversarial Examples for Models of Code},
	url = {http://arxiv.org/abs/1910.07517},
	abstract = {Neural models of code have shown impressive performance for tasks such as predicting method names and identifying certain kinds of bugs. In this paper, we show that these models are vulnerable to adversarial examples, and introduce a novel approach for attacking trained models of code with adversarial examples. The main idea is to force a given trained model to make an incorrect prediction as specified by the adversary by introducing small perturbations that do not change the program's semantics. To find such perturbations, we present a new technique for Discrete Adversarial Manipulation of Programs ({DAMP}). {DAMP} works by deriving the desired prediction with respect to the model's inputs while holding the model weights constant and following the gradients to slightly modify the input code. We show that our {DAMP} attack is effective across three neural architectures: code2vec, {GGNN}, and {GNN}-{FiLM}, in both Java and C\#. We show that {DAMP} has up to 89\% success rate in changing a prediction to the adversary's choice ("targeted attack"), and a success rate of up to 94\% in changing a given prediction to any incorrect prediction ("non-targeted attack"). To defend a model against such attacks, we examine a variety of possible defenses empirically and discuss their trade-offs. We show that some of these defenses drop the success rate of the attacker drastically, with a minor penalty of 2\% relative degradation in accuracy while not performing under attack.},
	journaltitle = {{arXiv}:1910.07517 [cs]},
	author = {Yefet, Noam and Alon, Uri and Yahav, Eran},
	urldate = {2020-06-28},
	date = {2020-05-27},
	eprinttype = {arxiv},
	eprint = {1910.07517},
	keywords = {⛔ No {DOI} found},
	file = {Yefet et al_2020_Adversarial Examples for Models of Code.pdf:/data/zotero/storage/NAW8Y4SP/Yefet et al_2020_Adversarial Examples for Models of Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/FFFHWVRB/1910.html:text/html}
}

@article{zhang_generating_2020,
	title = {Generating Adversarial Examples for Holding Robustness of Source Code Processing Models},
	volume = {34},
	rights = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5469},
	doi = {10/gg3j6p},
	abstract = {Automated processing, analysis, and generation of source code are among the key activities in software and system lifecycle. To this end, while deep learning ({DL}) exhibits a certain level of capability in handling these tasks, the current state-of-the-art {DL} models still suffer from non-robust issues and can be easily fooled by adversarial attacks.Different from adversarial attacks for image, audio, and natural languages, the structured nature of programming languages brings new challenges. In this paper, we propose a Metropolis-Hastings sampling-based identifier renaming technique, named {\textbackslash}fullmethod ({\textbackslash}method), which generates adversarial examples for {DL} models specialized for source code processing. Our in-depth evaluation on a functionality classification benchmark demonstrates the effectiveness of {\textbackslash}method in generating adversarial examples of source code. The higher robustness and performance enhanced through our adversarial training with {\textbackslash}method further confirms the usefulness of {DL} models-based method for future fully automated source code processing.},
	pages = {1169--1176},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Zhang, Huangzhao and Li, Zhuo and Li, Ge and Ma, Lei and Liu, Yang and Jin, Zhi},
	urldate = {2020-06-28},
	date = {2020-04-03},
	langid = {english},
	note = {Number: 01},
	file = {Zhang et al_2020_Generating Adversarial Examples for Holding Robustness of Source Code.pdf:/data/zotero/storage/V76LMS33/Zhang et al_2020_Generating Adversarial Examples for Holding Robustness of Source Code.pdf:application/pdf;Snapshot:/data/zotero/storage/V5HMPJH7/5469.html:text/html}
}

@article{fernandes_structured_2019,
	title = {Structured Neural Summarization},
	url = {http://arxiv.org/abs/1811.01824},
	abstract = {Summarization of long sequences into a concise statement is a core problem in natural language processing, requiring non-trivial understanding of the input. Based on the promising results of graph neural networks on highly structured data, we develop a framework to extend existing sequence encoders with a graph component that can reason about long-distance relationships in weakly structured data such as text. In an extensive evaluation, we show that the resulting hybrid sequence-graph models outperform both pure sequence models as well as pure graph models on a range of summarization tasks.},
	journaltitle = {{arXiv}:1811.01824 [cs, stat]},
	author = {Fernandes, Patrick and Allamanis, Miltiadis and Brockschmidt, Marc},
	urldate = {2020-06-28},
	date = {2019},
	eprinttype = {arxiv},
	eprint = {1811.01824},
	keywords = {⛔ No {DOI} found},
	file = {Fernandes et al_2020_Structured Neural Summarization.pdf:/data/zotero/storage/6G8WT7JE/Fernandes et al_2020_Structured Neural Summarization.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/678H7KKD/1811.html:text/html}
}

@article{yasunaga_graph-based_2020,
	title = {Graph-based, Self-Supervised Program Repair from Diagnostic Feedback},
	url = {http://arxiv.org/abs/2005.10636},
	abstract = {We consider the problem of learning to repair programs from diagnostic feedback (e.g., compiler error messages). Program repair is challenging for two reasons: First, it requires reasoning and tracking symbols across source code and diagnostic feedback. Second, labeled datasets available for program repair are relatively small. In this work, we propose novel solutions to these two challenges. First, we introduce a program-feedback graph, which connects symbols relevant to program repair in source code and diagnostic feedback, and then apply a graph neural network on top to model the reasoning process. Second, we present a self-supervised learning paradigm for program repair that leverages unlabeled programs available online to create a large amount of extra program repair examples, which we use to pre-train our models. We evaluate our proposed approach on two applications: correcting introductory programming assignments ({DeepFix} dataset) and correcting the outputs of program synthesis ({SPoC} dataset). Our final system, {DrRepair}, significantly outperforms prior work, achieving 66.1\% full repair rate on {DeepFix} (+20.8\% over the prior best), and 48.0\% synthesis success rate on {SPoC} (+3.3\% over the prior best).},
	journaltitle = {{arXiv}:2005.10636 [cs, stat]},
	author = {Yasunaga, Michihiro and Liang, Percy},
	urldate = {2020-06-28},
	date = {2020-05-20},
	eprinttype = {arxiv},
	eprint = {2005.10636},
	keywords = {⛔ No {DOI} found},
	file = {Yasunaga_Liang_2020_Graph-based, Self-Supervised Program Repair from Diagnostic Feedback.pdf:/data/zotero/storage/52KW3PYF/Yasunaga_Liang_2020_Graph-based, Self-Supervised Program Repair from Diagnostic Feedback.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/96YZY5KD/2005.html:text/html}
}

@article{li_using_2019,
	title = {Using {GGNN} to recommend log statement level},
	url = {http://arxiv.org/abs/1912.05097},
	abstract = {In software engineering, log statement is an important part because programmers can't access to users' program and they can only rely on log message to find the root of bugs. The mechanism of "log level" allows developers and users to specify the appropriate amount of logs to print during the execution of the software. And 26{\textbackslash}\% of the log statement modification is to modify the level. We tried to use {ML} method to predict the suitable level of log statement. The specific model is {GGNN}(gated graph neural network) and we have drawn lessons from Microsoft's research. In this work, we apply Graph Neural Networks to predict the usage of log statement level of some open source java projects from github. Given the good performance of {GGNN} in this task, we are confident that {GGNN} is an excellent choice for processing source code. We envision this model can play an important role in applying {AI}/{ML} technique for Software Development Life Cycle more broadly.},
	journaltitle = {{arXiv}:1912.05097 [cs]},
	author = {Li, Mingzhe and Pei, Jianrui and He, Jin and Song, Kevin and Che, Frank and Huang, Yongfeng and Wang, Chitai},
	urldate = {2020-06-28},
	date = {2019-12-10},
	eprinttype = {arxiv},
	eprint = {1912.05097},
	keywords = {mpnn, ⛔ No {DOI} found},
	file = {Li et al_2019_Using GGNN to recommend log statement level.pdf:/data/zotero/storage/FKDDA4D5/Li et al_2019_Using GGNN to recommend log statement level.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/IW7Q8GDP/1912.html:text/html}
}

@inproceedings{tufano_deep_2018,
	title = {Deep Learning Similarities from Different Representations of Source Code},
	abstract = {Assessing the similarity between code components plays a pivotal role in a number of Software Engineering ({SE}) tasks, such as clone detection, impact analysis, refactoring, etc. Code similarity is generally measured by relying on manually defined or hand-crafted features, e.g., by analyzing the overlap among identifiers or comparing the Abstract Syntax Trees of two code components. These features represent a best guess at what {SE} researchers can utilize to exploit and reliably assess code similarity for a given task. Recent work has shown, when using a stream of identifiers to represent the code, that Deep Learning ({DL}) can effectively replace manual feature engineering for the task of clone detection. However, source code can be represented at different levels of abstraction: identifiers, Abstract Syntax Trees, Control Flow Graphs, and Bytecode. We conjecture that each code representation can provide a different, yet orthogonal view of the same code fragment, thus, enabling a more reliable detection of similarities in code. In this paper, we demonstrate how {SE} tasks can benefit from a {DL}-based approach, which can automatically learn code similarities from different representations.},
	eventtitle = {2018 {IEEE}/{ACM} 15th International Conference on Mining Software Repositories ({MSR})},
	pages = {542--553},
	booktitle = {2018 {IEEE}/{ACM} 15th International Conference on Mining Software Repositories ({MSR})},
	author = {Tufano, Michele and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and White, Martin and Poshyvanyk, Denys},
	date = {2018-05},
	note = {{ISSN}: 2574-3864},
	keywords = {to-read},
	file = {Tufano et al_2018_Deep Learning Similarities from Different Representations of Source Code.pdf:/data/zotero/storage/2SWKYEPB/Tufano et al_2018_Deep Learning Similarities from Different Representations of Source Code.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/DXPNHVSS/8595238.html:text/html}
}

@inproceedings{hellendoorn_global_2019,
	title = {Global Relational Models of Source Code},
	url = {https://openreview.net/forum?id=B1lnbRNtwr&noteId=B1lnbRNtwr},
	abstract = {Models of code can learn distributed representations of a program's syntax and semantics to predict many non-trivial properties of a program. Recent state-of-the-art models leverage highly...},
	eventtitle = {International Conference on Learning Representations},
	author = {Hellendoorn, Vincent J. and Sutton, Charles and Singh, Rishabh and Maniatis, Petros and Bieber, David},
	urldate = {2020-06-28},
	date = {2019-09-25},
	keywords = {read, mpnn, uni, \_tablet},
	file = {Hellendoorn et al_2019_Global Relational Models of Source Code.pdf:/data/zotero/storage/SGSSMPK5/Hellendoorn et al_2019_Global Relational Models of Source Code.pdf:application/pdf;Snapshot:/data/zotero/storage/KDBYDTCY/forum.html:text/html}
}

@inproceedings{hellendoorn_deep_2018,
	location = {Lake Buena Vista, {FL}, {USA}},
	title = {Deep learning type inference},
	isbn = {978-1-4503-5573-5},
	url = {http://dl.acm.org/citation.cfm?doid=3236024.3236051},
	doi = {10/gf8npn},
	abstract = {Dynamically typed languages such as {JavaScript} and Python are increasingly popular, yet static typing has not been totally eclipsed: Python now supports type annotations and languages like {TypeScript} offer a middle-ground for {JavaScript}: a strict superset of {JavaScript}, to which it transpiles, coupled with a type system that permits partially typed programs. However, static typing has a cost: adding annotations, reading the added syntax, and wrestling with the type system to fix type errors. Type inference can ease the transition to more statically typed code and unlock the benefits of richer compile-time information, but is limited in languages like {JavaScript} as it cannot soundly handle duck-typing or runtime evaluation via eval. We propose {DeepTyper}, a deep learning model that understands which types naturally occur in certain contexts and relations and can provide type suggestions, which can often be verified by the type checker, even if it could not infer the type initially. {DeepTyper}, leverages an automatically aligned corpus of tokens and types to accurately predict thousands of variable and function type annotations. Furthermore, we demonstrate that context is key in accurately assigning these types and introduce a technique to reduce overfitting on local cues while highlighting the need for further improvements. Finally, we show that our model can interact with a compiler to provide more than 4,000 additional type annotations with over 95\% precision that could not be inferred without the aid of {DeepTyper}.},
	eventtitle = {the 2018 26th {ACM} Joint Meeting},
	pages = {152--162},
	booktitle = {Proceedings of the 2018 26th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering - {ESEC}/{FSE} 2018},
	publisher = {{ACM} Press},
	author = {Hellendoorn, Vincent J. and Bird, Christian and Barr, Earl T. and Allamanis, Miltiadis},
	urldate = {2020-06-28},
	date = {2018},
	langid = {english},
	keywords = {to-read},
	file = {Hellendoorn et al_2018_Deep learning type inference.pdf:/data/zotero/storage/XGRWB53Y/Hellendoorn et al_2018_Deep learning type inference.pdf:application/pdf}
}

@article{arakelyan_towards_2020,
	title = {Towards Learning Representations of Binary Executable Files for Security Tasks},
	url = {http://arxiv.org/abs/2002.03388},
	abstract = {Tackling binary analysis problems has traditionally implied manually defining rules and heuristics. As an alternative, we are suggesting using machine learning models for learning distributed representations of binaries that can be applicable for a number of downstream tasks. We construct a computational graph from the binary executable and use it with a graph convolutional neural network to learn a high dimensional representation of the program. We show the versatility of this approach by using our representations to solve two semantically different binary analysis tasks -- algorithm classification and vulnerability discovery. We compare the proposed approach to our own strong baseline as well as published results and demonstrate improvement on the state of the art methods for both tasks.},
	journaltitle = {{arXiv}:2002.03388 [cs, stat]},
	author = {Arakelyan, Shushan and Hauser, Christophe and Kline, Erik and Galstyan, Aram},
	urldate = {2020-06-28},
	date = {2020-02-09},
	eprinttype = {arxiv},
	eprint = {2002.03388},
	keywords = {⛔ No {DOI} found},
	file = {Arakelyan et al_2020_Towards Learning Representations of Binary Executable Files for Security Tasks.pdf:/data/zotero/storage/5PMH833B/Arakelyan et al_2020_Towards Learning Representations of Binary Executable Files for Security Tasks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/WESJKMNE/2002.html:text/html}
}

@article{wang_cocogum_2020,
	title = {{CoCoGUM}: Contextual Code Summarization with Multi-Relational {GNN} on {UMLs}},
	url = {https://www.microsoft.com/en-us/research/publication/cocogum-contextual-code-summarization-with-multi-relational-gnn-on-umls/},
	shorttitle = {{CoCoGUM}},
	abstract = {Code summaries are short natural language ({NL}) descriptions of code snippets that help developers better understand and maintain source code. Due to the pivotal role of code summaries in software development and maintenance, there is a surge of works on automatic code summarization to reduce the heavy burdens of developers. However, contemporary approaches only leverage […]},
	author = {Wang, Yanlin and Du, Lun and Shi, Ensheng and Hu, Yuxuan and Han, Shi and Zhang, Dongmei},
	urldate = {2020-06-28},
	date = {2020-05-20},
	langid = {american},
	keywords = {⛔ No {DOI} found},
	file = {Wang et al_2020_CoCoGUM.pdf:/data/zotero/storage/U3D25DPJ/Wang et al_2020_CoCoGUM.pdf:application/pdf;Snapshot:/data/zotero/storage/JGX2M2M9/cocogum-contextual-code-summarization-with-multi-relational-gnn-on-umls.html:text/html}
}

@inproceedings{dinella_hoppity_2019,
	title = {{HOPPITY}: {LEARNING} {GRAPH} {TRANSFORMATIONS} {TO} {DETECT} {AND} {FIX} {BUGS} {IN} {PROGRAMS}},
	url = {https://openreview.net/forum?id=SJeqs6EFvB&noteId=SJeqs6EFvB},
	shorttitle = {{HOPPITY}},
	abstract = {We present a learning-based approach to detect and fix a broad range of bugs in Javascript programs. We frame the problem in terms of learning a sequence of graph transformations: given a buggy...},
	eventtitle = {International Conference on Learning Representations},
	author = {Dinella, Elizabeth and Dai, Hanjun and Li, Ziyang and Naik, Mayur and Song, Le and Wang, Ke},
	urldate = {2020-06-28},
	date = {2019-09-25},
	file = {Dinella et al_2019_HOPPITY.pdf:/data/zotero/storage/UM2L3P5T/Dinella et al_2019_HOPPITY.pdf:application/pdf;Snapshot:/data/zotero/storage/HCR34JYR/forum.html:text/html}
}

@inproceedings{oono_graph_2020,
	title = {Graph Neural Networks Exponentially Lose Expressive Power for Node Classification},
	url = {https://iclr.cc/virtual_2020/poster_S1ldO2EFPr.html#details},
	abstract = {Graph Neural Networks (graph {NNs}) are a promising deep learning approach for analyzing graph-structured data. However, it is known that they do not improve (or sometimes worsen) their predictive performance as we pile up many layers and add non-lineality. To tackle this problem, we investigate the expressive power of graph {NNs} via their asymptotic behaviors as the layer size tends to infinity. Our strategy is to generalize the forward propagation of a Graph Convolutional Network ({GCN}), which is a popular graph {NN} variant, as a specific dynamical system. In the case of a {GCN}, we show that when its weights satisfy the conditions determined by the spectra of the (augmented) normalized Laplacian, its output exponentially approaches the set of signals that carry information of the connected components and node degrees only for distinguishing nodes. Our theory enables us to relate the expressive power of {GCNs} with the topological information of the underlying graphs inherent in the graph spectra. To demonstrate this, we characterize the asymptotic behavior of {GCNs} on the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph. We show that when the Erd{\textbackslash}H\{o\}s -- R{\textbackslash}'\{e\}nyi graph is sufficiently dense and large, a broad range of {GCNs} on it suffers from the ``information loss" in the limit of infinite layers with high probability. Based on the theory, we provide a principled guideline for weight normalization of graph {NNs}. We experimentally confirm that the proposed weight scaling enhances the predictive performance of {GCNs} in real data. Code is available at https://github.com/delta2323/gnn-asymptotics.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Oono, Kenta and Suzuki, Taiji},
	urldate = {2020-06-28},
	date = {2020-04},
	langid = {english},
	file = {Oono_Suzuki_2020_Graph Neural Networks Exponentially Lose Expressive Power for Node.pdf:/data/zotero/storage/A428VZVG/Oono_Suzuki_2020_Graph Neural Networks Exponentially Lose Expressive Power for Node.pdf:application/pdf;Snapshot:/data/zotero/storage/2MCA2D72/poster_S1ldO2EFPr.html:text/html}
}

@inproceedings{deng_graphzoom_2020,
	title = {{GraphZoom}: A Multi-level Spectral Approach for Accurate and Scalable Graph Embedding},
	url = {https://iclr.cc/virtual_2020/poster_r1lGO0EKDH.html},
	shorttitle = {{GraphZoom}},
	abstract = {Graph embedding techniques have been increasingly deployed in a multitude of different applications that involve learning on non-Euclidean data. However, existing graph embedding models either fail to incorporate node attribute information during training or suffer from node attribute noise, which compromises the accuracy. Moreover, very few of them scale to large graphs due to their high computational complexity and memory usage. In this paper we propose {GraphZoom}, a multi-level framework for improving both accuracy and scalability of unsupervised graph embedding algorithms. {GraphZoom} first performs graph fusion to generate a new graph that effectively encodes the topology of the original graph and the node attribute information. This fused graph is then repeatedly coarsened into much smaller graphs by merging nodes with high spectral similarities. {GraphZoom} allows any existing embedding methods to be applied to the coarsened graph, before it progressively refine the embeddings obtained at the coarsest level to increasingly finer graphs. We have evaluated our approach on a number of popular graph datasets for both transductive and inductive tasks. Our experiments show that {GraphZoom} can substantially increase the classification accuracy and significantly accelerate the entire graph embedding process by up to \$40.8 {\textbackslash}times\$, when compared to the state-of-the-art unsupervised embedding methods.},
	eventtitle = {Eighth International Conference on Learning Representations},
	author = {Deng, Chenhui and Zhao, Zhiqiang and Wang, Yongyu and Zhang, Zhiru and Feng, Zhuo},
	urldate = {2020-06-28},
	date = {2020-04},
	langid = {english},
	file = {Deng et al_2020_GraphZoom.pdf:/data/zotero/storage/U3HYIYCK/Deng et al_2020_GraphZoom.pdf:application/pdf;Snapshot:/data/zotero/storage/39IM3AJI/poster_r1lGO0EKDH.html:text/html}
}

@article{li_improving_2019,
	title = {Improving bug detection via context-based code representation learning and attention-based neural networks},
	volume = {3},
	url = {https://doi.org/10.1145/3360588},
	doi = {10/gg3j6n},
	abstract = {Bug detection has been shown to be an effective way to help developers in detecting bugs early, thus, saving much effort and time in software development process. Recently, deep learning-based bug detection approaches have gained successes over the traditional machine learning-based approaches, the rule-based program analysis approaches, and mining-based approaches. However, they are still limited in detecting bugs that involve multiple methods and suffer high rate of false positives. In this paper, we propose a combination approach with the use of contexts and attention neural network to overcome those limitations. We propose to use as the global context the Program Dependence Graph ({PDG}) and Data Flow Graph ({DFG}) to connect the method under investigation with the other relevant methods that might contribute to the buggy code. The global context is complemented by the local context extracted from the path on the {AST} built from the method’s body. The use of {PDG} and {DFG} enables our model to reduce the false positive rate, while to complement for the potential reduction in recall, we make use of the attention neural network mechanism to put more weights on the buggy paths in the source code. That is, the paths that are similar to the buggy paths will be ranked higher, thus, improving the recall of our model. We have conducted several experiments to evaluate our approach on a very large dataset with +4.973M methods in 92 different project versions. The results show that our tool can have a relative improvement up to 160\% on F-score when comparing with the state-of-the-art bug detection approaches. Our tool can detect 48 true bugs in the list of top 100 reported bugs, which is 24 more true bugs when comparing with the baseline approaches. We also reported that our representation is better suitable for bug detection and relatively improves over the other representations up to 206\% in accuracy.},
	pages = {162:1--162:30},
	issue = {{OOPSLA}},
	journaltitle = {Proceedings of the {ACM} on Programming Languages},
	shortjournal = {Proc. {ACM} Program. Lang.},
	author = {Li, Yi and Wang, Shaohua and Nguyen, Tien N. and Van Nguyen, Son},
	urldate = {2020-06-28},
	date = {2019-10-10},
	file = {Li et al_2019_Improving bug detection via context-based code representation learning and.pdf:/data/zotero/storage/QYGVK3TG/Li et al_2019_Improving bug detection via context-based code representation learning and.pdf:application/pdf}
}

@article{tarlow_learning_2019,
	title = {Learning to Fix Build Errors with Graph2Diff Neural Networks},
	url = {http://arxiv.org/abs/1911.01205},
	abstract = {Professional software developers spend a significant amount of time fixing builds, but this has received little attention as a problem in automatic program repair. We present a new deep learning architecture, called Graph2Diff, for automatically localizing and fixing build errors. We represent source code, build configuration files, and compiler diagnostic messages as a graph, and then use a Graph Neural Network model to predict a diff. A diff specifies how to modify the code's abstract syntax tree, represented in the neural network as a sequence of tokens and of pointers to code locations. Our network is an instance of a more general abstraction that we call Graph2Tocopo, which is potentially useful in any development tool for predicting source code changes. We evaluate the model on a dataset of over 500k real build errors and their resolutions from professional developers. Compared to the approach of {DeepDelta} (Mesbah et al., 2019), our approach tackles the harder task of predicting a more precise diff but still achieves over double the accuracy.},
	journaltitle = {{arXiv}:1911.01205 [cs, stat]},
	author = {Tarlow, Daniel and Moitra, Subhodeep and Rice, Andrew and Chen, Zimin and Manzagol, Pierre-Antoine and Sutton, Charles and Aftandilian, Edward},
	urldate = {2020-06-28},
	date = {2019-11-04},
	eprinttype = {arxiv},
	eprint = {1911.01205},
	keywords = {⛔ No {DOI} found},
	file = {Tarlow et al_2019_Learning to Fix Build Errors with Graph2Diff Neural Networks.pdf:/data/zotero/storage/ET9WHQFK/Tarlow et al_2019_Learning to Fix Build Errors with Graph2Diff Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/SSCLASPE/1911.html:text/html}
}

@article{tomczak_simulating_2019,
	title = {Simulating Execution Time of Tensor Programs using Graph Neural Networks},
	url = {http://arxiv.org/abs/1904.11876},
	abstract = {Optimizing the execution time of tensor program, e.g., a convolution, involves finding its optimal configuration. Searching the configuration space exhaustively is typically infeasible in practice. In line with recent research using {TVM}, we propose to learn a surrogate model to overcome this issue. The model is trained on an acyclic graph called an abstract syntax tree, and utilizes a graph convolutional network to exploit structure in the graph. We claim that a learnable graph-based data processing is a strong competitor to heuristic-based feature extraction. We present a new dataset of graphs corresponding to configurations and their execution time for various tensor programs. We provide baselines for a runtime prediction task.},
	journaltitle = {{arXiv}:1904.11876 [cs, stat]},
	author = {Tomczak, Jakub M. and Lepert, Romain and Wiggers, Auke},
	urldate = {2020-06-28},
	date = {2019-11-27},
	eprinttype = {arxiv},
	eprint = {1904.11876},
	keywords = {⛔ No {DOI} found},
	file = {Tomczak et al_2019_Simulating Execution Time of Tensor Programs using Graph Neural Networks.pdf:/data/zotero/storage/2CJPK6XZ/Tomczak et al_2019_Simulating Execution Time of Tensor Programs using Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/YZ5VBGKK/1904.html:text/html}
}

@article{allamanis_survey_2018,
	title = {A Survey of Machine Learning for Big Code and Naturalness},
	url = {http://arxiv.org/abs/1709.06182},
	abstract = {Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit code's abundance of patterns. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.},
	journaltitle = {{arXiv}:1709.06182 [cs]},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Devanbu, Premkumar and Sutton, Charles},
	urldate = {2020-06-28},
	date = {2018-05-04},
	eprinttype = {arxiv},
	eprint = {1709.06182},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Allamanis et al_2018_A Survey of Machine Learning for Big Code and Naturalness.pdf:/data/zotero/storage/YCJQEGXB/Allamanis et al_2018_A Survey of Machine Learning for Big Code and Naturalness.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3QBG83QT/1709.html:text/html}
}

@article{cho_properties_2014,
	title = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
	url = {http://arxiv.org/abs/1409.1259},
	shorttitle = {On the Properties of Neural Machine Translation},
	abstract = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; {RNN} Encoder--Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
	journaltitle = {{arXiv}:1409.1259 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	urldate = {2020-06-28},
	date = {2014-10-07},
	eprinttype = {arxiv},
	eprint = {1409.1259},
	keywords = {⛔ No {DOI} found},
	file = {Cho et al_2014_On the Properties of Neural Machine Translation.pdf:/data/zotero/storage/HKBG6TN2/Cho et al_2014_On the Properties of Neural Machine Translation.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/BQJFKP7H/1409.html:text/html}
}

@article{cho_learning_2014,
	title = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical Machine Translation},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called {RNN} Encoder-Decoder that consists of two recurrent neural networks ({RNN}). One {RNN} encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the {RNN} Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	journaltitle = {{arXiv}:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	urldate = {2020-06-28},
	date = {2014-09-02},
	eprinttype = {arxiv},
	eprint = {1406.1078},
	keywords = {⛔ No {DOI} found},
	file = {Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:/data/zotero/storage/R45VZFRT/Cho et al_2014_Learning Phrase Representations using RNN Encoder-Decoder for Statistical.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/I9U7GK6P/1406.html:text/html}
}

@inproceedings{cvitkovic_open_2019,
	title = {Open Vocabulary Learning on Source Code with a Graph-Structured Cache},
	url = {http://proceedings.mlr.press/v97/cvitkovic19b.html},
	abstract = {Machine learning models that take computer program source code as input typically use Natural Language Processing ({NLP}) techniques. However, a major challenge is that code is written using an open,...},
	eventtitle = {International Conference on Machine Learning},
	pages = {1475--1485},
	booktitle = {International Conference on Machine Learning},
	author = {Cvitkovic, Milan and Singh, Badal and Anandkumar, Animashree},
	urldate = {2020-06-28},
	date = {2019-05-24},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	file = {Cvitkovic et al_2019_Open Vocabulary Learning on Source Code with a Graph-Structured Cache.pdf:/data/zotero/storage/9CP4EVJF/Cvitkovic et al_2019_Open Vocabulary Learning on Source Code with a Graph-Structured Cache.pdf:application/pdf;Snapshot:/data/zotero/storage/52MIZLE4/cvitkovic19b.html:text/html}
}

@article{li_gated_2017,
	title = {Gated Graph Sequence Neural Networks},
	url = {http://arxiv.org/abs/1511.05493},
	abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., {LSTMs}) when the problem is graph-structured. We demonstrate the capabilities on some simple {AI} ({bAbI}) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
	journaltitle = {{arXiv}:1511.05493 [cs, stat]},
	author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
	urldate = {2020-06-29},
	date = {2017-09-22},
	eprinttype = {arxiv},
	eprint = {1511.05493},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Li et al_2017_Gated Graph Sequence Neural Networks.pdf:/data/zotero/storage/9VGW4RVZ/Li et al_2017_Gated Graph Sequence Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/CSZ8QBR2/1511.html:text/html}
}

@article{velickovic_graph_2018,
	title = {Graph Attention Networks},
	url = {http://arxiv.org/abs/1710.10903},
	abstract = {We present graph attention networks ({GATs}), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our {GAT} models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	journaltitle = {{arXiv}:1710.10903 [cs, stat]},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	urldate = {2020-06-29},
	date = {2018-02-04},
	eprinttype = {arxiv},
	eprint = {1710.10903},
	keywords = {⛔ No {DOI} found},
	file = {Veličković et al_2018_Graph Attention Networks.pdf:/data/zotero/storage/J2EGEYQ3/Veličković et al_2018_Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/8WYQG8PV/1710.html:text/html}
}

@inproceedings{malik_nl2type_2019,
	title = {{NL}2Type: Inferring {JavaScript} Function Types from Natural Language Information},
	doi = {10/gg3j6v},
	shorttitle = {{NL}2Type},
	abstract = {{JavaScript} is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal {IDE} support, difficult to understand {APIs}, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and {TypeScript}, but they rely on developers to annotate code with types. This paper presents {NL}2Type, a learning-based approach for predicting likely type signatures of {JavaScript} functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, {LSTM}-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 {JavaScript} files from real-world projects. {NL}2Type predicts types with a precision of 84.1\% and a recall of 78.9\% when considering only the top-most suggestion, and with a precision of 95.5\% and a recall of 89.6\% when considering the top-5 suggestions. The approach outperforms both {JSNice}, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and {DeepTyper}, a recent type prediction approach that is also based on deep learning. Beyond predicting types, {NL}2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.},
	eventtitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	pages = {304--315},
	booktitle = {2019 {IEEE}/{ACM} 41st International Conference on Software Engineering ({ICSE})},
	author = {Malik, Rabee Sohail and Patra, Jibesh and Pradel, Michael},
	date = {2019-05},
	note = {{ISSN}: 1558-1225},
	keywords = {skimmed},
	file = {Malik et al_2019_NL2Type.pdf:/data/zotero/storage/SJ96GUCG/Malik et al_2019_NL2Type.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/6AJAV9TV/8811893.html:text/html}
}

@inproceedings{raychev_learning_2016,
	location = {St. Petersburg, {FL}, {USA}},
	title = {Learning programs from noisy data},
	isbn = {978-1-4503-3549-2},
	url = {https://doi.org/10.1145/2837614.2837671},
	doi = {10/gg3j6x},
	series = {{POPL} '16},
	abstract = {We present a new approach for learning programs from noisy datasets. Our approach is based on two new concepts: a regularized program generator which produces a candidate program based on a small sample of the entire dataset while avoiding overfitting, and a dataset sampler which carefully samples the dataset by leveraging the candidate program's score on that dataset. The two components are connected in a continuous feedback-directed loop. We show how to apply this approach to two settings: one where the dataset has a bound on the noise, and another without a noise bound. The second setting leads to a new way of performing approximate empirical risk minimization on hypotheses classes formed by a discrete search space. We then present two new kinds of program synthesizers which target the two noise settings. First, we introduce a novel regularized bitstream synthesizer that successfully generates programs even in the presence of incorrect examples. We show that the synthesizer can detect errors in the examples while combating overfitting -- a major problem in existing synthesis techniques. We also show how the approach can be used in a setting where the dataset grows dynamically via new examples (e.g., provided by a human). Second, we present a novel technique for constructing statistical code completion systems. These are systems trained on massive datasets of open source programs, also known as ``Big Code''. The key idea is to introduce a domain specific language ({DSL}) over trees and to learn functions in that {DSL} directly from the dataset. These learned functions then condition the predictions made by the system. This is a flexible and powerful technique which generalizes several existing works as we no longer need to decide a priori on what the prediction should be conditioned (another benefit is that the learned functions are a natural mechanism for explaining the prediction). As a result, our code completion system surpasses the prediction capabilities of existing, hard-wired systems.},
	pages = {761--774},
	booktitle = {Proceedings of the 43rd Annual {ACM} {SIGPLAN}-{SIGACT} Symposium on Principles of Programming Languages},
	publisher = {Association for Computing Machinery},
	author = {Raychev, Veselin and Bielik, Pavol and Vechev, Martin and Krause, Andreas},
	urldate = {2020-06-29},
	date = {2016-01-11},
	keywords = {read},
	file = {Raychev et al_2016_Learning programs from noisy data.pdf:/data/zotero/storage/2YTEDNMS/Raychev et al_2016_Learning programs from noisy data.pdf:application/pdf}
}

@inproceedings{hellendoorn_are_2017,
	location = {Paderborn, Germany},
	title = {Are deep neural networks the best choice for modeling source code?},
	isbn = {978-1-4503-5105-8},
	url = {https://doi.org/10.1145/3106237.3106290},
	doi = {10/gg3j53},
	series = {{ESEC}/{FSE} 2017},
	abstract = {Current statistical language modeling techniques, including deep-learning based models, have proven to be quite effective for source code. We argue here that the special properties of source code can be exploited for further improvements. In this work, we enhance established language modeling approaches to handle the special challenges of modeling source code, such as: frequent changes, larger, changing vocabularies, deeply nested scopes, etc. We present a fast, nested language modeling toolkit specifically designed for software, with the ability to add \& remove text, and mix \& swap out many models. Specifically, we improve upon prior cache-modeling work and present a model with a much more expansive, multi-level notion of locality that we show to be well-suited for modeling software. We present results on varying corpora in comparison with traditional N-gram, as well as {RNN}, and {LSTM} deep-learning language models, and release all our source code for public use. Our evaluations suggest that carefully adapting N-gram models for source code can yield performance that surpasses even {RNN} and {LSTM} based deep-learning models.},
	pages = {763--773},
	booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Hellendoorn, Vincent J. and Devanbu, Premkumar},
	urldate = {2020-06-29},
	date = {2017-08-21},
	keywords = {read},
	file = {Hellendoorn_Devanbu_2017_Are deep neural networks the best choice for modeling source code.pdf:/data/zotero/storage/W9PCB95A/Hellendoorn_Devanbu_2017_Are deep neural networks the best choice for modeling source code.pdf:application/pdf}
}

@inproceedings{sen_jalangi_2013,
	location = {Saint Petersburg, Russia},
	title = {Jalangi: a selective record-replay and dynamic analysis framework for {JavaScript}},
	isbn = {978-1-4503-2237-9},
	url = {https://doi.org/10.1145/2491411.2491447},
	doi = {10/gg3j6q},
	series = {{ESEC}/{FSE} 2013},
	shorttitle = {Jalangi},
	abstract = {{JavaScript} is widely used for writing client-side web applications and is getting increasingly popular for writing mobile applications. However, unlike C, C++, and Java, there are not that many tools available for analysis and testing of {JavaScript} applications. In this paper, we present a simple yet powerful framework, called Jalangi, for writing heavy-weight dynamic analyses. Our framework incorporates two key techniques: 1) selective record-replay, a technique which enables to record and to faithfully replay a user-selected part of the program, and 2) shadow values and shadow execution, which enables easy implementation of heavy-weight dynamic analyses. Our implementation makes no special assumption about {JavaScript}, which makes it applicable to real-world {JavaScript} programs running on multiple platforms. We have implemented concolic testing, an analysis to track origins of nulls and undefined, a simple form of taint analysis, an analysis to detect likely type inconsistencies, and an object allocation profiler in Jalangi. Our evaluation of Jalangi on the {SunSpider} benchmark suite and on five web applications shows that Jalangi has an average slowdown of 26X during recording and 30X slowdown during replay and analysis. The slowdowns are comparable with slowdowns reported for similar tools, such as {PIN} and Valgrind for x86 binaries. We believe that the techniques proposed in this paper are applicable to other dynamic languages.},
	pages = {488--498},
	booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Sen, Koushik and Kalasapur, Swaroop and Brutch, Tasneem and Gibbs, Simon},
	urldate = {2020-06-29},
	date = {2013-08-18},
	file = {Sen et al_2013_Jalangi.pdf:/data/zotero/storage/2NUS865H/Sen et al_2013_Jalangi.pdf:application/pdf}
}

@article{andreasen_survey_2017,
	title = {A Survey of Dynamic Analysis and Test Generation for {JavaScript}},
	volume = {50},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3106739},
	doi = {10/gc5phq},
	abstract = {{JavaScript} has become one of the most prevalent programming languages. Unfortunately, some of the unique properties that contribute to this popularity also make {JavaScript} programs prone to errors and difficult for program analyses to reason about. These properties include the highly dynamic nature of the language, a set of unusual language features, a lack of encapsulation mechanisms, and the “no crash” philosophy. This article surveys dynamic program analysis and test generation techniques for {JavaScript} targeted at improving the correctness, reliability, performance, security, and privacy of {JavaScript}-based software.},
	pages = {66:1--66:36},
	number = {5},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Andreasen, Esben and Gong, Liang and Møller, Anders and Pradel, Michael and Selakovic, Marija and Sen, Koushik and Staicu, Cristian-Alexandru},
	urldate = {2020-06-29},
	date = {2017-09-26},
	keywords = {\_tablet},
	file = {Andreasen et al_2017_A Survey of Dynamic Analysis and Test Generation for JavaScript.pdf:/data/zotero/storage/EZPNN4CE/Andreasen et al_2017_A Survey of Dynamic Analysis and Test Generation for JavaScript.pdf:application/pdf}
}

@inproceedings{pradel_typedevil_2015,
	title = {{TypeDevil}: Dynamic Type Inconsistency Analysis for {JavaScript}},
	volume = {1},
	doi = {10/gg3j7j},
	shorttitle = {{TypeDevil}},
	abstract = {Dynamic languages, such as {JavaScript}, give programmers the freedom to ignore types, and enable them to write concise code in short time. Despite this freedom, many programs follow implicit type rules, for example, that a function has a particular signature or that a property has a particular type. Violations of such implicit type rules often correlate with problems in the program. This paper presents Type Devil, a mostly dynamic analysis that warns developers about inconsistent types. The key idea is to assign a set of observed types to each variable, property, and function, to merge types based in their structure, and to warn developers about variables, properties, and functions that have inconsistent types. To deal with the pervasiveness of polymorphic behavior in real-world {JavaScript} programs, we present a set of techniques to remove spurious warnings and to merge related warnings. Applying Type Devil to widely used benchmark suites and real-world web applications reveals 15 problematic type inconsistencies, including correctness problems, performance problems, and dangerous coding practices.},
	eventtitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	pages = {314--324},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Pradel, Michael and Schuh, Parker and Sen, Koushik},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	file = {Pradel et al_2015_TypeDevil.pdf:/data/zotero/storage/CEBQEVJL/Pradel et al_2015_TypeDevil.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/KWK7IJGJ/7194584.html:text/html}
}

@inproceedings{andreasen_trace_2016,
	location = {Dagstuhl, Germany},
	title = {Trace Typing: An Approach for Evaluating Retrofitted Type Systems},
	volume = {56},
	isbn = {978-3-95977-014-9},
	url = {http://drops.dagstuhl.de/opus/volltexte/2016/6095},
	doi = {10/gg3j7g},
	series = {Leibniz International Proceedings in Informatics ({LIPIcs})},
	shorttitle = {Trace Typing},
	pages = {1:1--1:26},
	booktitle = {30th European Conference on Object-Oriented Programming ({ECOOP} 2016)},
	publisher = {Schloss Dagstuhl–Leibniz-Zentrum fuer Informatik},
	author = {Andreasen, Esben and Gordon, Colin S. and Chandra, Satish and Sridharan, Manu and Tip, Frank and Sen, Koushik},
	editor = {Krishnamurthi, Shriram and Lerner, Benjamin S.},
	urldate = {2020-06-29},
	date = {2016},
	note = {{ISSN}: 1868-8969},
	file = {Snapshot:/data/zotero/storage/J9I33BF8/6095.html:text/html;Andreasen et al_2016_Trace Typing.pdf:/data/zotero/storage/D59NKHFM/Andreasen et al_2016_Trace Typing.pdf:application/pdf}
}

@inproceedings{brockschmidt_learning_2017,
	location = {Cham},
	title = {Learning Shape Analysis},
	isbn = {978-3-319-66706-5},
	doi = {10/gg3j6r},
	series = {Lecture Notes in Computer Science},
	abstract = {We present a data-driven verification framework to automatically prove memory safety of heap-manipulating programs. Our core contribution is a novel statistical machine learning technique that maps observed program states to (possibly disjunctive) separation logic formulas describing the invariant shape of (possibly nested) data structures at relevant program locations. We then attempt to verify these predictions using a program verifier, where counterexamples to a predicted invariant are used as additional input to the shape predictor in a refinement loop. We have implemented our techniques in Locust, an extension of the {GRASShopper} verification tool. Locust is able to automatically prove memory safety of implementations of classical heap-manipulating programs such as insertionsort, quicksort and traversals of nested data structures.},
	pages = {66--87},
	booktitle = {Static Analysis},
	publisher = {Springer International Publishing},
	author = {Brockschmidt, Marc and Chen, Yuxin and Kohli, Pushmeet and Krishna, Siddharth and Tarlow, Daniel},
	editor = {Ranzato, Francesco},
	date = {2017},
	langid = {english},
	keywords = {skimmed},
	file = {Brockschmidt et al_2017_Learning Shape Analysis.pdf:/data/zotero/storage/2QZC3A6B/Brockschmidt et al_2017_Learning Shape Analysis.pdf:application/pdf}
}

@article{chistyakov_semantic_2018,
	title = {Semantic embeddings for program behavior patterns},
	url = {http://arxiv.org/abs/1804.03635},
	abstract = {In this paper, we propose a new feature extraction technique for program execution logs. First, we automatically extract complex patterns from a program's behavior graph. Then, we embed these patterns into a continuous space by training an autoencoder. We evaluate the proposed features on a real-world malicious software detection task. We also find that the embedding space captures interpretable structures in the space of pattern parts.},
	journaltitle = {{arXiv}:1804.03635 [cs, stat]},
	author = {Chistyakov, Alexander and Lobacheva, Ekaterina and Kuznetsov, Arseny and Romanenko, Alexey},
	urldate = {2020-06-29},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1804.03635},
	keywords = {⛔ No {DOI} found},
	file = {Chistyakov et al_2018_Semantic embeddings for program behavior patterns.pdf:/data/zotero/storage/3WHTR8SU/Chistyakov et al_2018_Semantic embeddings for program behavior patterns.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/BMPQ9T59/1804.html:text/html}
}

@inproceedings{allamanis_suggesting_2015,
	location = {Bergamo, Italy},
	title = {Suggesting accurate method and class names},
	isbn = {978-1-4503-3675-8},
	url = {https://doi.org/10.1145/2786805.2786849},
	doi = {10/gf8np5},
	series = {{ESEC}/{FSE} 2015},
	abstract = {Descriptive names are a vital part of readable, and hence maintainable, code. Recent progress on automatically suggesting names for local variables tantalizes with the prospect of replicating that success with method and class names. However, suggesting names for methods and classes is much more difficult. This is because good method and class names need to be functionally descriptive, but suggesting such names requires that the model goes beyond local context. We introduce a neural probabilistic language model for source code that is specifically designed for the method naming problem. Our model learns which names are semantically similar by assigning them to locations, called embeddings, in a high-dimensional continuous space, in such a way that names with similar embeddings tend to be used in similar contexts. These embeddings seem to contain semantic information about tokens, even though they are learned only from statistical co-occurrences of tokens. Furthermore, we introduce a variant of our model that is, to our knowledge, the first that can propose neologisms, names that have not appeared in the training corpus. We obtain state of the art results on the method, class, and even the simpler variable naming tasks. More broadly, the continuous embeddings that are learned by our model have the potential for wide application within software engineering.},
	pages = {38--49},
	booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Allamanis, Miltiadis and Barr, Earl T. and Bird, Christian and Sutton, Charles},
	urldate = {2020-06-29},
	date = {2015-08-30},
	file = {Allamanis et al_2015_Suggesting accurate method and class names.pdf:/data/zotero/storage/JHV9RRUX/Allamanis et al_2015_Suggesting accurate method and class names.pdf:application/pdf}
}

@inproceedings{habib_finding_2016,
	location = {Amsterdam, Netherlands},
	title = {Finding concurrency bugs using graph-based anomaly detection in big code},
	isbn = {978-1-4503-4437-1},
	url = {https://doi.org/10.1145/2984043.2998542},
	doi = {10/gg3j6j},
	series = {{SPLASH} Companion 2016},
	abstract = {Concurrency bugs are very difficult and subtle to discover, reproduce, and fix. Many techniques have been devised by the academic as well as the industry communities to find these bugs. However, most of the effective techniques tend to focus on a subset of the various concurrency bugs types. We propose a new generic concurrency bug detection technique that leverages "Big Code": millions of lines of code freely available on code repositories. Our approach tries to learn the properties of what constitutes a good and a bad synchronization pattern from hundreds of concurrent software using graph-based anomaly detection.},
	pages = {55--56},
	booktitle = {Companion Proceedings of the 2016 {ACM} {SIGPLAN} International Conference on Systems, Programming, Languages and Applications: Software for Humanity},
	publisher = {Association for Computing Machinery},
	author = {Habib, Andrew},
	urldate = {2020-06-29},
	date = {2016-10-20},
	keywords = {\_tablet},
	file = {Habib_2016_Finding concurrency bugs using graph-based anomaly detection in big code.pdf:/data/zotero/storage/BHFWSR3H/Habib_2016_Finding concurrency bugs using graph-based anomaly detection in big code.pdf:application/pdf}
}

@inproceedings{he_debin_2018,
	location = {Toronto, Canada},
	title = {Debin: Predicting Debug Information in Stripped Binaries},
	isbn = {978-1-4503-5693-0},
	url = {https://doi.org/10.1145/3243734.3243866},
	doi = {10/gg3j6c},
	series = {{CCS} '18},
	shorttitle = {Debin},
	abstract = {We present a novel approach for predicting debug information in stripped binaries. Using machine learning, we first train probabilistic models on thousands of non-stripped binaries and then use these models to predict properties of meaningful elements in unseen stripped binaries. Our focus is on recovering symbol names, types and locations, which are critical source-level information wiped off during compilation and stripping. Our learning approach is able to distinguish and extract key elements such as register-allocated and memory-allocated variables usually not evident in the stripped binary. To predict names and types of extracted elements, we use scalable structured prediction algorithms in probabilistic graphical models with an extensive set of features which capture key characteristics of binary code. Based on this approach, we implemented an automated tool, called Debin, which handles {ELF} binaries on three of the most popular architectures: x86, x64 and {ARM}. Given a stripped binary, Debin outputs a binary augmented with the predicted debug information. Our experimental results indicate that Debin is practically useful: for x64, it predicts symbol names and types with 68.8\% precision and 68.3\% recall. We also show that Debin is helpful for the task of inspecting real-world malware -- it revealed suspicious library usage and behaviors such as {DNS} resolver reader.},
	pages = {1667--1680},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {He, Jingxuan and Ivanov, Pesho and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
	urldate = {2020-06-29},
	date = {2018-01-15},
	keywords = {\_tablet},
	file = {He et al_2018_Debin.pdf:/data/zotero/storage/SEX6U6DY/He et al_2018_Debin.pdf:application/pdf}
}

@article{heo_adaptive_2018,
	title = {Adaptive Static Analysis via Learning with Bayesian Optimization},
	volume = {40},
	issn = {0164-0925},
	url = {https://doi.org/10.1145/3121135},
	doi = {10/gg3j5x},
	abstract = {Building a cost-effective static analyzer for real-world programs is still regarded an art. One key contributor to this grim reputation is the difficulty in balancing the cost and the precision of an analyzer. An ideal analyzer should be adaptive to a given analysis task and avoid using techniques that unnecessarily improve precision and increase analysis cost. However, achieving this ideal is highly nontrivial, and it requires a large amount of engineering efforts. In this article, we present a new learning-based approach for adaptive static analysis. In our approach, the analysis includes a sophisticated parameterized strategy that decides, for each part of a given program, whether to apply a precision-improving technique to that part or not. We present a method for learning a good parameter for such a strategy from an existing codebase via Bayesian optimization. The learnt strategy is then used for new, unseen programs. Using our approach, we developed partially flow- and context-sensitive variants of a realistic C static analyzer. The experimental results demonstrate that using Bayesian optimization is crucial for learning from an existing codebase. Also, they show that among all program queries that require flow- or context-sensitivity, our partially flow- and context-sensitive analysis answers 75\% of them, while increasing the analysis cost only by 3.3× of the baseline flow- and context-insensitive analysis, rather than 40× or more of the fully sensitive version.},
	pages = {14:1--14:37},
	number = {4},
	journaltitle = {{ACM} Transactions on Programming Languages and Systems},
	shortjournal = {{ACM} Trans. Program. Lang. Syst.},
	author = {Heo, Kihong and Oh, Hakjoo and Yang, Hongseok and Yi, Kwangkeun},
	urldate = {2020-06-29},
	date = {2018-11-16},
	file = {Heo et al_2018_Adaptive Static Analysis via Learning with Bayesian Optimization.pdf:/data/zotero/storage/RY5DW2UG/Heo et al_2018_Adaptive Static Analysis via Learning with Bayesian Optimization.pdf:application/pdf}
}

@inproceedings{katz_estimating_2016,
	location = {St. Petersburg, {FL}, {USA}},
	title = {Estimating types in binaries using predictive modeling},
	isbn = {978-1-4503-3549-2},
	url = {https://doi.org/10.1145/2837614.2837674},
	doi = {10/gg3j6g},
	series = {{POPL} '16},
	abstract = {Reverse engineering is an important tool in mitigating vulnerabilities in binaries. As a lot of software is developed in object-oriented languages, reverse engineering of object-oriented code is of critical importance. One of the major hurdles in reverse engineering binaries compiled from object-oriented code is the use of dynamic dispatch. In the absence of debug information, any dynamic dispatch may seem to jump to many possible targets, posing a significant challenge to a reverse engineer trying to track the program flow. We present a novel technique that allows us to statically determine the likely targets of virtual function calls. Our technique uses object tracelets – statically constructed sequences of operations performed on an object – to capture potential runtime behaviors of the object. Our analysis automatically pre-labels some of the object tracelets by relying on instances where the type of an object is known. The resulting type-labeled tracelets are then used to train a statistical language model ({SLM}) for each type.We then use the resulting ensemble of {SLMs} over unlabeled tracelets to generate a ranking of their most likely types, from which we deduce the likely targets of dynamic dispatches.We have implemented our technique and evaluated it over real-world C++ binaries. Our evaluation shows that when there are multiple alternative targets, our approach can drastically reduce the number of targets that have to be considered by a reverse engineer.},
	pages = {313--326},
	booktitle = {Proceedings of the 43rd Annual {ACM} {SIGPLAN}-{SIGACT} Symposium on Principles of Programming Languages},
	publisher = {Association for Computing Machinery},
	author = {Katz, Omer and El-Yaniv, Ran and Yahav, Eran},
	urldate = {2020-06-29},
	date = {2016-01-11},
	file = {Katz et al_2016_Estimating types in binaries using predictive modeling.pdf:/data/zotero/storage/6BDEFPNB/Katz et al_2016_Estimating types in binaries using predictive modeling.pdf:application/pdf}
}

@inproceedings{katz_statistical_2018,
	location = {Williamsburg, {VA}, {USA}},
	title = {Statistical Reconstruction of Class Hierarchies in Binaries},
	isbn = {978-1-4503-4911-6},
	url = {https://doi.org/10.1145/3173162.3173202},
	doi = {10/gg3j7f},
	series = {{ASPLOS} '18},
	abstract = {We address a fundamental problem in reverse engineering of object-oriented code: the reconstruction of a program's class hierarchy from its stripped binary. Existing approaches rely heavily on structural information that is not always available, e.g., calls to parent constructors. As a result, these approaches often leave gaps in the hierarchies they construct, or fail to construct them altogether. Our main insight is that behavioral information can be used to infer subclass/superclass relations, supplementing any missing structural information. Thus, we propose the first statistical approach for static reconstruction of class hierarchies based on behavioral similarity. We capture the behavior of each type using a statistical language model ({SLM}), define a metric for pairwise similarity between types based on the Kullback-Leibler divergence between their {SLMs}, and lift it to determine the most likely class hierarchy. We implemented our approach in a tool called {ROCK} and used it to automatically reconstruct the class hierarchies of several real-world stripped C++ binaries. Our results demonstrate that {ROCK} obtained significantly more accurate class hierarchies than those obtained using structural analysis alone.},
	pages = {363--376},
	booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
	publisher = {Association for Computing Machinery},
	author = {Katz, Omer and Rinetzky, Noam and Yahav, Eran},
	urldate = {2020-06-29},
	date = {2018-03-19},
	file = {Katz et al_2018_Statistical Reconstruction of Class Hierarchies in Binaries.pdf:/data/zotero/storage/AXP6TBEI/Katz et al_2018_Statistical Reconstruction of Class Hierarchies in Binaries.pdf:application/pdf}
}

@inproceedings{li_detecting_2016,
	location = {Cham},
	title = {Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel},
	isbn = {978-3-319-35122-3},
	doi = {10/gg3j6d},
	series = {Lecture Notes in Computer Science},
	abstract = {With the increasing availability of source code on the Internet, many new approaches to retrieve, repair, and reuse code have emerged that rely on the ability to efficiently compute the similarity of two pieces of code. The meaning of similarity, however, heavily depends on the application domain. For predicting {API} calls, for example, programs can be considered similar if they call a specific set of functions in a similar way, while for automated bug fixing, it is important that similar programs share a similar data-flow.In this paper, we propose an algorithm to compute program similarity based on the Weisfeiler-Leman graph kernel. Our algorithm is able to operate on different graph-based representations of programs and thus can be applied in different domains. We show the usefulness of our approach in two experiments using data-flow similarity and {API}-call similarity.},
	pages = {315--330},
	booktitle = {Software Reuse: Bridging with Social-Awareness},
	publisher = {Springer International Publishing},
	author = {Li, Wenchao and Saidi, Hassen and Sanchez, Huascar and Schäf, Martin and Schweitzer, Pascal},
	editor = {Kapitsaki, Georgia M. and Santana de Almeida, Eduardo},
	date = {2016},
	langid = {english},
	keywords = {out-of-scope},
	file = {Li et al_2016_Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel.pdf:/data/zotero/storage/CSMC4ZDP/Li et al_2016_Detecting Similar Programs via The Weisfeiler-Leman Graph Kernel.pdf:application/pdf}
}

@online{bourgeois_learning_2019,
	title = {Learning Representations of Source Code from Structure and Context},
	url = {https://infoscience.epfl.ch/record/277163},
	abstract = {Large codebases are routinely indexed by standard Information Retrieval systems, starting from the assumption that code written by humans shows similar statistical properties to written text [Hindle et al., 2012]. While those {IR} systems are still relatively successful inside companies to help developers search on their proprietary codebase, the same cannot be said about most of public platforms: throughout the years many notable names (Google Code Search, Koders, Ohloh, etc.) have been shut down. The limited functionalities offered, combined with the low quality of the results, did not attract a critical mass of users to justify running those services. To this date, even {GitHub} (arguably the largest code repository in the world) offers search functionalities that are not more innovative than those present in platforms from the past decade. We argue that the reason why this happens has happened can be imputed to the fundamental limitation of mining information exclusively from the textual representation of the code. Developing a more powerful representation of code will not only enable a new generation of search systems, but will also allow us to explore code by functional similarity, i.e., searching for blocks of code which accomplish similar (and not strictly equivalent) tasks. In this thesis, we want to explore the opportunities provided by a multimodal representation of code: (1) hierarchical (both in terms of object and package hierarchy), (2) syntactical (leveraging the Abstract Syntax Tree representation of code), (3) distributional (embedding by means of co-occurrences), and (4) textual (mining the code documentation). Our goal is to distill as much information as possible from the complex nature of code. Recent advances in deep learning are providing a new set of techniques that we plan to employ for the different modes, for instance Poincaré Embeddings [Nickel and Kiela, 2017] for (1) hierarchical, and Gated Graph {NNs} [Li et al., 2016] for (2) syntactical. Last but not the least, learning multimodal similarity [{McFee} and Lanckriet, 2011] is an ulterior research challenge, especially at the scale of large codebases – we will explore the opportunities offered by a framework like {GraphSAGE} [Hamilton et al., 2017] to harmonize a large graph with rich feature information.},
	titleaddon = {Infoscience},
	author = {Bourgeois, Dylan},
	urldate = {2020-06-29},
	date = {2019-03-15},
	langid = {english},
	note = {Library Catalog: infoscience.epfl.ch
Number: {STUDENT}},
	keywords = {\_tablet},
	file = {Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:/data/zotero/storage/AEVV85EZ/Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:application/pdf;Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:/data/zotero/storage/M6NZWADR/Bourgeois_2019_Learning Representations of Source Code from Structure and Context.pdf:application/pdf}
}

@incollection{gupta_neural_2019,
	title = {Neural Attribution for Semantic Bug-Localization in Student Programs},
	url = {http://papers.nips.cc/paper/9358-neural-attribution-for-semantic-bug-localization-in-student-programs.pdf},
	pages = {11884--11894},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	author = {Gupta, Rahul and Kanade, Aditya and Shevade, Shirish},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d\{{\textbackslash}textbackslash\}textquotesingle and Fox, E. and Garnett, R.},
	urldate = {2020-06-29},
	date = {2019},
	file = {Gupta et al_2019_Neural Attribution for Semantic Bug-Localization in Student Programs.pdf:/data/zotero/storage/QS65Q2NC/Gupta et al_2019_Neural Attribution for Semantic Bug-Localization in Student Programs.pdf:application/pdf;NIPS Snapshot:/data/zotero/storage/KXBDBBMF/9358-neural-attribution-for-semantic-bug-localization-in-student-programs.html:text/html}
}

@inproceedings{azcona_user2code2vec_2019,
	location = {Tempe, {AZ}, {USA}},
	title = {user2code2vec: Embeddings for Profiling Students Based on Distributional Representations of Source Code},
	isbn = {978-1-4503-6256-6},
	url = {https://doi.org/10.1145/3303772.3303813},
	doi = {10/gg3j7k},
	series = {{LAK}19},
	shorttitle = {user2code2vec},
	abstract = {In this work, we propose a new methodology to profile individual students of computer science based on their programming design using a technique called embeddings. We investigate different approaches to analyze user source code submissions in the Python language. We compare the performances of different source code vectorization techniques to predict the correctness of a code submission. In addition, we propose a new mechanism to represent students based on their code submissions for a given set of laboratory tasks on a particular course. This way, we can make deeper recommendations for programming solutions and pathways to support student learning and progression in computer programming modules effectively at a Higher Education Institution. Recent work using Deep Learning tends to work better when more and more data is provided. However, in Learning Analytics, the number of students in a course is an unavoidable limit. Thus we cannot simply generate more data as is done in other domains such as {FinTech} or Social Network Analysis. Our findings indicate there is a need to learn and develop better mechanisms to extract and learn effective data features from students so as to analyze the students' progression and performance effectively.},
	pages = {86--95},
	booktitle = {Proceedings of the 9th International Conference on Learning Analytics \& Knowledge},
	publisher = {Association for Computing Machinery},
	author = {Azcona, David and Arora, Piyush and Hsiao, I-Han and Smeaton, Alan},
	urldate = {2020-06-29},
	date = {2019-03-04},
	file = {Azcona et al_2019_user2code2vec.pdf:/data/zotero/storage/VW653GRD/Azcona et al_2019_user2code2vec.pdf:application/pdf}
}

@article{arabshahi_combining_2018,
	title = {Combining Symbolic Expressions and Black-box Function Evaluations in Neural Programs},
	url = {https://arxiv.org/abs/1801.04342v3},
	abstract = {Neural programming involves training neural networks to learn programs,
mathematics, or logic from data. Previous works have failed to achieve good
generalization performance, especially on problems and programs with high
complexity or on large domains. This is because they mostly rely either on
black-box function evaluations that do not capture the structure of the
program, or on detailed execution traces that are expensive to obtain, and
hence the training data has poor coverage of the domain under consideration. We
present a novel framework that utilizes black-box function evaluations, in
conjunction with symbolic expressions that define relationships between the
given functions. We employ tree {LSTMs} to incorporate the structure of the
symbolic expression trees. We use tree encoding for numbers present in function
evaluation data, based on their decimal representation. We present an
evaluation benchmark for this task to demonstrate our proposed model combines
symbolic reasoning and function evaluation in a fruitful manner, obtaining high
accuracies in our experiments. Our framework generalizes significantly better
to expressions of higher depth and is able to fill partial equations with valid
completions.},
	author = {Arabshahi, Forough and Singh, Sameer and Anandkumar, Animashree},
	urldate = {2020-06-29},
	date = {2018-01-12},
	langid = {english},
	keywords = {theory, ⛔ No {DOI} found},
	file = {Arabshahi et al_2018_Combining Symbolic Expressions and Black-box Function Evaluations in Neural.pdf:/data/zotero/storage/C3WANNVS/Arabshahi et al_2018_Combining Symbolic Expressions and Black-box Function Evaluations in Neural.pdf:application/pdf;Snapshot:/data/zotero/storage/7ZWVGPZJ/1801.html:text/html}
}

@inproceedings{allamanis_learning_2017,
	title = {Learning Continuous Semantic Representations of Symbolic Expressions},
	url = {http://proceedings.mlr.press/v70/allamanis17a.html},
	abstract = {Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning. As a step in this direction, we propose a new architecture, called neural eq...},
	eventtitle = {International Conference on Machine Learning},
	pages = {80--88},
	booktitle = {International Conference on Machine Learning},
	author = {Allamanis, Miltiadis and Chanthirasegaran, Pankajan and Kohli, Pushmeet and Sutton, Charles},
	urldate = {2020-06-29},
	date = {2017-07-17},
	langid = {english},
	note = {{ISSN}: 1938-7228
Section: Machine Learning},
	file = {Allamanis et al_2017_Learning Continuous Semantic Representations of Symbolic Expressions.pdf:/data/zotero/storage/AATHK5H2/Allamanis et al_2017_Learning Continuous Semantic Representations of Symbolic Expressions.pdf:application/pdf;Snapshot:/data/zotero/storage/TVU7WSRE/allamanis17a.html:text/html}
}

@article{balog_deepcoder_2017,
	title = {{DeepCoder}: Learning to Write Programs},
	url = {http://arxiv.org/abs/1611.01989},
	shorttitle = {{DeepCoder}},
	abstract = {We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network's predictions to augment search techniques from the programming languages community, including enumerative search and an {SMT}-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites.},
	journaltitle = {{arXiv}:1611.01989 [cs]},
	author = {Balog, Matej and Gaunt, Alexander L. and Brockschmidt, Marc and Nowozin, Sebastian and Tarlow, Daniel},
	urldate = {2020-06-29},
	date = {2017-03-08},
	eprinttype = {arxiv},
	eprint = {1611.01989},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Balog et al_2017_DeepCoder.pdf:/data/zotero/storage/JZBYXY4D/Balog et al_2017_DeepCoder.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/MTIWTEB5/1611.html:text/html}
}

@article{solar-lezama_combinatorial_2006,
	title = {Combinatorial sketching for finite programs},
	volume = {34},
	issn = {0163-5964},
	url = {https://doi.org/10.1145/1168919.1168907},
	doi = {10/dz9ppd},
	abstract = {Sketching is a software synthesis approach where the programmer develops a partial implementation - a sketch - and a separate specification of the desired functionality. The synthesizer then completes the sketch to behave like the specification. The correctness of the synthesized implementation is guaranteed by the compiler, which allows, among other benefits, rapid development of highly tuned implementations without the fear of introducing bugs.We develop {SKETCH}, a language for finite programs with linguistic support for sketching. Finite programs include many highperformance kernels, including cryptocodes. In contrast to prior synthesizers, which had to be equipped with domain-specific rules, {SKETCH} completes sketches by means of a combinatorial search based on generalized boolean satisfiability. Consequently, our combinatorial synthesizer is complete for the class of finite programs: it is guaranteed to complete any sketch in theory, and in practice has scaled to realistic programming problems.Freed from domain rules, we can now write sketches as simpleto-understand partial programs, which are regular programs in which difficult code fragments are replaced with holes to be filled by the synthesizer. Holes may stand for index expressions, lookup tables, or bitmasks, but the programmer can easily define new kinds of holes using a single versatile synthesis operator.We have used {SKETCH} to synthesize an efficient implementation of the {AES} cipher standard. The synthesizer produces the most complex part of the implementation and runs in about an hour.},
	pages = {404--415},
	number = {5},
	journaltitle = {{ACM} {SIGARCH} Computer Architecture News},
	shortjournal = {{SIGARCH} Comput. Archit. News},
	author = {Solar-Lezama, Armando and Tancau, Liviu and Bodik, Rastislav and Seshia, Sanjit and Saraswat, Vijay},
	urldate = {2020-06-30},
	date = {2006-10-20},
	file = {Solar-Lezama et al_2006_Combinatorial sketching for finite programs.pdf:/data/zotero/storage/CYDMNI5N/Solar-Lezama et al_2006_Combinatorial sketching for finite programs.pdf:application/pdf}
}

@article{padhi_loopinvgen_2019,
	title = {{LoopInvGen}: A Loop Invariant Generator based on Precondition Inference},
	url = {http://arxiv.org/abs/1707.02029},
	shorttitle = {{LoopInvGen}},
	abstract = {We describe the {LoopInvGen} tool for generating loop invariants that can provably guarantee correctness of a program with respect to a given specification. {LoopInvGen} is an efficient implementation of the inference technique originally proposed in our earlier work on {PIE} (https://doi.org/10.1145/2908080.2908099). In contrast to existing techniques, {LoopInvGen} is not restricted to a fixed set of features -- atomic predicates that are composed together to build complex loop invariants. Instead, we start with no initial features, and use program synthesis techniques to grow the set on demand. This not only enables a less onerous and more expressive approach, but also appears to be significantly faster than the existing tools over the {SyGuS}-{COMP} 2018 benchmarks from the {INV} track.},
	journaltitle = {{arXiv}:1707.02029 [cs]},
	author = {Padhi, Saswat and Sharma, Rahul and Millstein, Todd},
	urldate = {2020-06-30},
	date = {2019-10-31},
	eprinttype = {arxiv},
	eprint = {1707.02029},
	keywords = {⛔ No {DOI} found},
	file = {Padhi et al_2019_LoopInvGen.pdf:/data/zotero/storage/N9WPAYND/Padhi et al_2019_LoopInvGen.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/3868M8R2/1707.html:text/html}
}

@inproceedings{allamanis_convolutional_2016,
	title = {A convolutional attention network for extreme summarization of source code},
	pages = {2091--2100},
	booktitle = {International conference on machine learning},
	author = {Allamanis, Miltiadis and Peng, Hao and Sutton, Charles},
	date = {2016},
	keywords = {⛔ No {DOI} found},
	file = {Allamanis et al_2016_A convolutional attention network for extreme summarization of source code.pdf:/data/zotero/storage/RD4PS2JY/Allamanis et al_2016_A convolutional attention network for extreme summarization of source code.pdf:application/pdf}
}

@inproceedings{wang_blended_2020,
	location = {London, {UK}},
	title = {Blended, precise semantic program embeddings},
	isbn = {978-1-4503-7613-6},
	url = {10/gg4mtc},
	doi = {10/gg4mtc},
	series = {{PLDI} 2020},
	abstract = {Learning neural program embeddings is key to utilizing deep neural networks in program languages research --- precise and efficient program representations enable the application of deep models to a wide range of program analysis tasks. Existing approaches predominately learn to embed programs from their source code, and, as a result, they do not capture deep, precise program semantics. On the other hand, models learned from runtime information critically depend on the quality of program executions, thus leading to trained models with highly variant quality. This paper tackles these inherent weaknesses of prior approaches by introducing a new deep neural network, Liger, which learns program representations from a mixture of symbolic and concrete execution traces. We have evaluated Liger on two tasks: method name prediction and semantics classification. Results show that Liger is significantly more accurate than the state-of-the-art static model code2seq in predicting method names, and requires on average around 10x fewer executions covering nearly 4x fewer paths than the state-of-the-art dynamic model {DYPRO} in both tasks. Liger offers a new, interesting design point in the space of neural program embeddings and opens up this new direction for exploration.},
	pages = {121--134},
	booktitle = {Proceedings of the 41st {ACM} {SIGPLAN} Conference on Programming Language Design and Implementation},
	publisher = {Association for Computing Machinery},
	author = {Wang, Ke and Su, Zhendong},
	urldate = {2020-07-12},
	date = {2020-06-11},
	file = {Wang_Su_2020_Blended, precise semantic program embeddings.pdf:/data/zotero/storage/9AT7KZMX/Wang_Su_2020_Blended, precise semantic program embeddings.pdf:application/pdf}
}

@incollection{backhouse_galois_2002,
	title = {Galois connections and fixed point calculus},
	pages = {89--150},
	booktitle = {Algebraic and coalgebraic methods in the mathematics of program construction},
	publisher = {Springer},
	author = {Backhouse, Roland},
	date = {2002},
	file = {Backhouse_2002_Galois connections and fixed point calculus.pdf:/data/zotero/storage/LU6V2JW8/Backhouse_2002_Galois connections and fixed point calculus.pdf:application/pdf;Snapshot:/data/zotero/storage/7VSTD5MJ/3-540-47797-7_4.html:text/html}
}

@inproceedings{ye_deep_2020,
	location = {Virtual Event {GA} {USA}},
	title = {Deep Program Structure Modeling Through Multi-Relational Graph-based Learning},
	isbn = {978-1-4503-8075-1},
	url = {https://dl.acm.org/doi/10.1145/3410463.3414670},
	doi = {10/ghgb4p},
	eventtitle = {{PACT} '20: International Conference on Parallel Architectures and Compilation Techniques},
	pages = {111--123},
	booktitle = {Proceedings of the {ACM} International Conference on Parallel Architectures and Compilation Techniques},
	publisher = {{ACM}},
	author = {Ye, Guixin and Tang, Zhanyong and Wang, Huanting and Fang, Dingyi and Fang, Jianbin and Huang, Songfang and Wang, Zheng},
	urldate = {2020-10-22},
	date = {2020-09-30},
	langid = {english},
	keywords = {read, uni, \_tablet},
	file = {Ye et al_2020_Deep Program Structure Modeling Through Multi-Relational Graph-based Learning.pdf:/data/zotero/storage/JGYWVTTJ/Ye et al_2020_Deep Program Structure Modeling Through Multi-Relational Graph-based Learning.pdf:application/pdf}
}

@article{kruse_autotuning_2020,
	title = {Autotuning Search Space for Loop Transformations},
	url = {http://arxiv.org/abs/2010.06521},
	abstract = {One of the challenges for optimizing compilers is to predict whether applying an optimization will improve its execution speed. Programmers may override the compiler's profitability heuristic using optimization directives such as pragmas in the source code. Machine learning in the form of autotuning can assist users in finding the best optimizations for each platform. In this paper we propose a loop transformation search space that takes the form of a tree, in contrast to previous approaches that usually use vector spaces to represent loop optimization configurations. We implemented a simple autotuner exploring the search space and applied it to a selected set of {PolyBench} kernels. While the autotuner is capable of representing every possible sequence of loop transformations and their relations, the results motivate the use of better search strategies such as Monte Carlo tree search to find sophisticated loop transformations such as multilevel tiling.},
	journaltitle = {{arXiv}:2010.06521 [cs]},
	author = {Kruse, Michael and Finkel, Hal and Wu, Xingfu},
	urldate = {2020-10-22},
	date = {2020-10-13},
	eprinttype = {arxiv},
	eprint = {2010.06521},
	keywords = {⛔ No {DOI} found, \_tablet},
	file = {Kruse et al_2020_Autotuning Search Space for Loop Transformations.pdf:/data/zotero/storage/KHBM8EA6/Kruse et al_2020_Autotuning Search Space for Loop Transformations.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/I244D94T/2010.html:text/html}
}

@inproceedings{yamaguchi_modeling_2014,
	title = {Modeling and Discovering Vulnerabilities with Code Property Graphs},
	url = {10/gf5b7d},
	doi = {10/gf5b7d},
	abstract = {The vast majority of security breaches encountered today are a direct result of insecure code. Consequently, the protection of computer systems critically depends on the rigorous identification of vulnerabilities in software, a tedious and error-prone process requiring significant expertise. Unfortunately, a single flaw suffices to undermine the security of a system and thus the sheer amount of code to audit plays into the attacker's cards. In this paper, we present a method to effectively mine large amounts of source code for vulnerabilities. To this end, we introduce a novel representation of source code called a code property graph that merges concepts of classic program analysis, namely abstract syntax trees, control flow graphs and program dependence graphs, into a joint data structure. This comprehensive representation enables us to elegantly model templates for common vulnerabilities with graph traversals that, for instance, can identify buffer overflows, integer overflows, format string vulnerabilities, or memory disclosures. We implement our approach using a popular graph database and demonstrate its efficacy by identifying 18 previously unknown vulnerabilities in the source code of the Linux kernel.},
	eventtitle = {2014 {IEEE} Symposium on Security and Privacy},
	pages = {590--604},
	booktitle = {2014 {IEEE} Symposium on Security and Privacy},
	author = {Yamaguchi, F. and Golde, N. and Arp, D. and Rieck, K.},
	date = {2014-05},
	note = {{ISSN}: 2375-1207},
	keywords = {read, uni, \_tablet},
	file = {Yamaguchi et al_2014_Modeling and Discovering Vulnerabilities with Code Property Graphs.pdf:/data/zotero/storage/DNQEJ6VW/Yamaguchi et al_2014_Modeling and Discovering Vulnerabilities with Code Property Graphs.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/4I7SCESR/6956589.html:text/html}
}

@inproceedings{brauckmann_compy-learn_2020,
	title = {{ComPy}-Learn: A Toolbox for Exploring Machine Learning Representations for Compilers},
	booktitle = {2020 Forum for Specification and Design Languages ({FDL})},
	author = {Brauckmann, Alexander and Goens, Andrés and Castrillon, Jeronimo},
	date = {2020-09},
	note = {event-place: Kiel, Germany},
	keywords = {read, uni, \_tablet},
	file = {Brauckmann et al_2020_ComPy-Learn.pdf:/data/zotero/storage/NFF7A9BC/Brauckmann et al_2020_ComPy-Learn.pdf:application/pdf}
}

@inproceedings{jin_current_2018,
	title = {Current and Future Research of Machine Learning Based Vulnerability Detection},
	url = {10/ghgn9t},
	doi = {10/ghgn9t},
	abstract = {The quantity and volume of software increase rapidly, more vulnerabilities are hidden there, and thus vulnerability detection becomes more important. This paper reviews works about machine learning based vulnerability detection methods for source code, especially on program representation and vectorization, and Machine Learning based Methods. However, current machine learning methods detect vulnerabilities in coarse-grained level, and locating the vulnerabilities still needs much additional work. This problem can be solved by vulnerability detection in fine-grained level. This paper assumes that vulnerability detection in fine-grained level will be a future research trend, and proposes several possible solutions.},
	eventtitle = {2018 Eighth International Conference on Instrumentation Measurement, Computer, Communication and Control ({IMCCC})},
	pages = {1562--1566},
	booktitle = {2018 Eighth International Conference on Instrumentation Measurement, Computer, Communication and Control ({IMCCC})},
	author = {Jin, Z. and Yu, Y.},
	date = {2018-07},
	note = {{ISSN}: 2373-6844},
	keywords = {uni, \_tablet, boring},
	file = {Jin_Yu_2018_Current and Future Research of Machine Learning Based Vulnerability Detection.pdf:/data/zotero/storage/AS9PXKWS/Jin_Yu_2018_Current and Future Research of Machine Learning Based Vulnerability Detection.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/IBK2GPM4/9045496.html:text/html}
}

@article{chakraborty_deep_2020,
	title = {Deep Learning based Vulnerability Detection: Are We There Yet?},
	url = {http://arxiv.org/abs/2009.07235},
	shorttitle = {Deep Learning based Vulnerability Detection},
	abstract = {Automated detection of software vulnerabilities is a fundamental problem in software security. Existing program analysis techniques either suffer from high false positives or false negatives. Recent progress in Deep Learning ({DL}) has resulted in a surge of interest in applying {DL} for automated vulnerability detection. Several recent studies have demonstrated promising results achieving an accuracy of up to 95\% at detecting vulnerabilities. In this paper, we ask, "how well do the state-of-the-art {DL}-based techniques perform in a real-world vulnerability prediction scenario?". To our surprise, we find that their performance drops by more than 50\%. A systematic investigation of what causes such precipitous performance drop reveals that existing {DL}-based vulnerability prediction approaches suffer from challenges with the training data (e.g., data duplication, unrealistic distribution of vulnerable classes, etc.) and with the model choices (e.g., simple token-based models). As a result, these approaches often do not learn features related to the actual cause of the vulnerabilities. Instead, they learn unrelated artifacts from the dataset (e.g., specific variable/function names, etc.). Leveraging these empirical findings, we demonstrate how a more principled approach to data collection and model design, based on realistic settings of vulnerability prediction, can lead to better solutions. The resulting tools perform significantly better than the studied baseline: up to 33.57\% boost in precision and 128.38\% boost in recall compared to the best performing model in the literature. Overall, this paper elucidates existing {DL}-based vulnerability prediction systems' potential issues and draws a roadmap for future {DL}-based vulnerability prediction research. In that spirit, we make available all the artifacts supporting our results: https://git.io/Jf6IA.},
	journaltitle = {{arXiv}:2009.07235 [cs]},
	author = {Chakraborty, Saikat and Krishna, Rahul and Ding, Yangruibo and Ray, Baishakhi},
	urldate = {2020-10-26},
	date = {2020-09-03},
	eprinttype = {arxiv},
	eprint = {2009.07235},
	keywords = {dataset, uni, \_tablet},
	file = {Chakraborty et al_2020_Deep Learning based Vulnerability Detection.pdf:/data/zotero/storage/PIT8EZEI/Chakraborty et al_2020_Deep Learning based Vulnerability Detection.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/K727LDDL/2009.html:text/html}
}

@article{suneja_learning_2020,
	title = {Learning to map source code to software vulnerability using code-as-a-graph},
	url = {http://arxiv.org/abs/2006.08614},
	abstract = {We explore the applicability of Graph Neural Networks in learning the nuances of source code from a security perspective. Specifically, whether signatures of vulnerabilities in source code can be learned from its graph representation, in terms of relationships between nodes and edges. We create a pipeline we call {AI}4VA, which first encodes a sample source code into a Code Property Graph. The extracted graph is then vectorized in a manner which preserves its semantic information. A Gated Graph Neural Network is then trained using several such graphs to automatically extract templates differentiating the graph of a vulnerable sample from a healthy one. Our model outperforms static analyzers, classic machine learning, as well as {CNN} and {RNN}-based deep learning models on two of the three datasets we experiment with. We thus show that a code-as-graph encoding is more meaningful for vulnerability detection than existing code-as-photo and linear sequence encoding approaches. (Submitted Oct 2019, Paper \#28, {ICST})},
	journaltitle = {{arXiv}:2006.08614 [cs]},
	author = {Suneja, Sahil and Zheng, Yunhui and Zhuang, Yufan and Laredo, Jim and Morari, Alessandro},
	urldate = {2020-10-26},
	date = {2020-06-15},
	eprinttype = {arxiv},
	eprint = {2006.08614},
	keywords = {read, uni, \_tablet},
	file = {Suneja et al_2020_Learning to map source code to software vulnerability using code-as-a-graph.pdf:/data/zotero/storage/WB46J7EJ/Suneja et al_2020_Learning to map source code to software vulnerability using code-as-a-graph.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/UP86HWAP/2006.html:text/html}
}

@inproceedings{klees_evaluating_2018,
	location = {Toronto, Canada},
	title = {Evaluating Fuzz Testing},
	isbn = {978-1-4503-5693-0},
	url = {10/ggcc69},
	doi = {10/ggcc69},
	series = {{CCS} '18},
	abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
	pages = {2123--2138},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
	urldate = {2020-11-05},
	date = {2018-01-15},
	keywords = {uni, \_tablet, fuzz},
	file = {Klees et al_2018_Evaluating Fuzz Testing.pdf:/data/zotero/storage/2T4JSEDS/Klees et al_2018_Evaluating Fuzz Testing.pdf:application/pdf}
}

@article{lin_software_2019,
	title = {Software Vulnerability Discovery via Learning Multi-domain Knowledge Bases},
	issn = {1941-0018},
	url = {10/ghk5jn},
	doi = {10/ghk5jn},
	abstract = {Machine learning ({ML}) has great potential in automated code vulnerability discovery. However, automated discovery application driven by off-the-shelf machine learning tools often performs poorly due to the shortage of high-quality training data. The scarceness of vulnerability data is almost always a problem for any developing software project during its early stages, which is referred to as the cold-start problem. This paper proposes a framework that utilizes transferable knowledge from pre-existing data sources. In order to improve the detection performance, multiple vulnerability-relevant data sources were selected to form a broader base for learning transferable knowledge. The selected vulnerability-relevant data sources are cross-domain, including historical vulnerability data from different software projects and data from the Software Assurance Reference Database ({SARD}) consisting of synthetic vulnerability examples and proof-of-concept test cases. To extract the information applicable in vulnerability detection from the cross-domain data sets, we designed a deep-learning-based framework with Long-short Term Memory ({LSTM}) cells. Our framework combines the heterogeneous data sources to learn unified representations of the patterns of the vulnerable source codes. Empirical studies showed that the unified representations generated by the proposed deep learning networks are feasible and effective, and are transferable for real-world vulnerability detection. Our experiments demonstrated that by leveraging two heterogeneous data sources, the performance of our vulnerability detection outperformed the static vulnerability discovery tool Flawfinder. The findings of this paper may stimulate further research in {ML}-based vulnerability detection using heterogeneous data sources.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Dependable and Secure Computing},
	author = {Lin, G. and Zhang, J. and Luo, W. and Pan, L. and Vel, O. De and Montague, P. and Xiang, Y.},
	date = {2019},
	note = {Conference Name: {IEEE} Transactions on Dependable and Secure Computing},
	keywords = {uni, \_tablet},
	file = {Lin et al_2019_Software Vulnerability Discovery via Learning Multi-domain Knowledge Bases.pdf:/data/zotero/storage/LPVRVY6C/Lin et al_2019_Software Vulnerability Discovery via Learning Multi-domain Knowledge Bases.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/3FUGXRNK/8906156.html:text/html}
}

@article{li_comparative_2019,
	title = {A Comparative Study of Deep Learning-Based Vulnerability Detection System},
	volume = {7},
	issn = {2169-3536},
	url = {10/ggsskk},
	doi = {10/ggsskk},
	abstract = {Source code static analysis has been widely used to detect vulnerabilities in the development of software products. The vulnerability patterns purely based on human experts are laborious and error prone, which has motivated the use of machine learning for vulnerability detection. In order to relieve human experts of defining vulnerability rules or features, a recent study shows the feasibility of leveraging deep learning to detect vulnerabilities automatically. However, the impact of different factors on the effectiveness of vulnerability detection is unknown. In this paper, we collect two datasets from the programs involving 126 types of vulnerabilities, on which we conduct the first comparative study to quantitatively evaluate the impact of different factors on the effectiveness of vulnerability detection. The experimental results show that accommodating control dependency can increase the overall effectiveness of vulnerability detection F1-measure by 20.3\%; the imbalanced data processing methods are not effective for the dataset we create; bidirectional recurrent neural networks ({RNNs}) are more effective than unidirectional {RNNs} and convolutional neural network, which in turn are more effective than multi-layer perception; using the last output corresponding to the time step for the bidirectional long short-term memory ({BLSTM}) can reduce the false negative rate by 2.0\% at the price of increasing the false positive rate by 0.5\%.},
	pages = {103184--103197},
	journaltitle = {{IEEE} Access},
	author = {Li, Z. and Zou, D. and Tang, J. and Zhang, Z. and Sun, M. and Jin, H.},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	keywords = {uni, \_tablet},
	file = {Li et al_2019_A Comparative Study of Deep Learning-Based Vulnerability Detection System.pdf:/data/zotero/storage/FRKNXQ78/Li et al_2019_A Comparative Study of Deep Learning-Based Vulnerability Detection System.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/IN68MKSW/8769937.html:text/html}
}

@article{li_vuldeepecker_nodate,
	title = {{VulDeePecker}: A Deep Learning-Based System for Vulnerability Detection},
	url = {10/gf96vp},
	doi = {10/gf96vp},
	shorttitle = {{VulDeePecker}},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Ou, Xinyu and Jin, Hai and Wang, Sujuan and Deng, Zhijun and Zhong, Yuyi},
	keywords = {skimmed, uni, \_tablet},
	file = {Li et al_VulDeePecker.pdf:/data/zotero/storage/TVJLML7S/Li et al_VulDeePecker.pdf:application/pdf}
}

@article{zhou_devign_2019,
	title = {Devign: Effective Vulnerability Identification by Learning Comprehensive Program Semantics via Graph Neural Networks},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html},
	shorttitle = {Devign},
	pages = {10197--10207},
	journaltitle = {Advances in Neural Information Processing Systems},
	author = {Zhou, Yaqin and Liu, Shangqing and Siow, Jingkai and Du, Xiaoning and Liu, Yang},
	urldate = {2020-12-09},
	date = {2019},
	langid = {english},
	keywords = {read, uni, \_tablet},
	file = {Zhou et al_2019_Devign.pdf:/data/zotero/storage/CNMQ8JFE/Zhou et al_2019_Devign.pdf:application/pdf;Snapshot:/data/zotero/storage/QGGJKVL8/49265d2447bc3bbfe9e76306ce40a31f-Abstract.html:text/html}
}

@inproceedings{guan_survey_2020,
	title = {A Survey on Deep Learning-Based Source Code Defect Analysis},
	url = {10/ghn24b},
	doi = {10/ghn24b},
	abstract = {With the rapid development of information technology, various software applications are flooding our daily lives. The development of these application software inevitably generates a lot of source code. How to detect and analyze various defects in the source code, such as {API}/Function call errors, array misuse, and expression syntax error, etc., which is known as source code defect analysis ({SCDA}), has attracted the attention of many researchers in the academic field. Since artificial intelligence ({AI}) technology has achieved excellent results in the field of image processing and natural language processing, researchers have tried to use deep learning algorithms in {AI} to automatically extract and analyze features of source code. Therefore, we review the recent deep learning-based source code defect analysis methods, including abstract syntax tree-based methods, program dependency graph-based methods, and other deep learning-based methods. Compared to traditional methods, the deep learning-based code defect analysis methods can realize the automatic extraction of source code defect features. This means that there is no longer a need for human experts to pre-define code features, which avoids errors caused by humans to a certain extent. The application research of {AI} in the source code defect analysis is an interesting and challenging development direction, and we believe it has broad development prospects.},
	eventtitle = {2020 5th International Conference on Computer and Communication Systems ({ICCCS})},
	pages = {167--171},
	booktitle = {2020 5th International Conference on Computer and Communication Systems ({ICCCS})},
	author = {Guan, Z. and Wang, X. and Xin, W. and Wang, J. and Zhang, L.},
	date = {2020-05},
	keywords = {low-prio, uni, \_tablet},
	file = {Guan et al_2020_A Survey on Deep Learning-Based Source Code Defect Analysis.pdf:/data/zotero/storage/TJDT7ILJ/Guan et al_2020_A Survey on Deep Learning-Based Source Code Defect Analysis.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/PTHRLF7E/9118556.html:text/html}
}

@article{bilgin_vulnerability_2020,
	title = {Vulnerability Prediction From Source Code Using Machine Learning},
	volume = {8},
	issn = {2169-3536},
	url = {10/ghxd5v},
	doi = {10/ghxd5v},
	abstract = {As the role of information and communication technologies gradually increases in our lives, software security becomes a major issue to provide protection against malicious attempts and to avoid ending up with noncompensable damages to the system. With the advent of data-driven techniques, there is now a growing interest in how to leverage machine learning ({ML}) as a software assurance method to build trustworthy software systems. In this study, we examine how to predict software vulnerabilities from source code by employing {ML} prior to their release. To this end, we develop a source code representation method that enables us to perform intelligent analysis on the Abstract Syntax Tree ({AST}) form of source code and then investigate whether {ML} can distinguish vulnerable and nonvulnerable code fragments. To make a comprehensive performance evaluation, we use a public dataset that contains a large amount of function-level real source code parts mined from open-source projects and carefully labeled according to the type of vulnerability if they have any.We show the effectiveness of our proposed method for vulnerability prediction from source code by carrying out exhaustive and realistic experiments under different regimes in comparison with state-of-art methods.},
	pages = {150672--150684},
	journaltitle = {{IEEE} Access},
	author = {Bilgin, Z. and Ersoy, M. A. and Soykan, E. U. and Tomur, E. and Çomak, P. and Karaçay, L.},
	date = {2020},
	note = {Conference Name: {IEEE} Access},
	keywords = {uni, \_tablet, boring},
	file = {Bilgin et al_2020_Vulnerability Prediction From Source Code Using Machine Learning.pdf:/data/zotero/storage/3XBFWTYF/Bilgin et al_2020_Vulnerability Prediction From Source Code Using Machine Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/X36KNIR7/9167194.html:text/html}
}

@inproceedings{russell_automated_2018,
	title = {Automated Vulnerability Detection in Source Code Using Deep Representation Learning},
	url = {10/ggssk7},
	doi = {10/ggssk7},
	shorttitle = {Draper},
	abstract = {Increasing numbers of software vulnerabilities are discovered every year whether they are reported publicly or discovered internally in proprietary code. These vulnerabilities can pose serious risk of exploit and result in system compromise, information leaks, or denial of service. We leveraged the wealth of C and C++ open-source code available to develop a largescale function-level vulnerability detection system using machine learning. To supplement existing labeled vulnerability datasets, we compiled a vast dataset of millions of open-source functions and labeled it with carefully-selected findings from three different static analyzers that indicate potential exploits. Using these datasets, we developed a fast and scalable vulnerability detection tool based on deep feature representation learning that directly interprets lexed source code. We evaluated our tool on code from both real software packages and the {NIST} {SATE} {IV} benchmark dataset. Our results demonstrate that deep feature representation learning on source code is a promising approach for automated software vulnerability detection.},
	eventtitle = {2018 17th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	pages = {757--762},
	booktitle = {2018 17th {IEEE} International Conference on Machine Learning and Applications ({ICMLA})},
	author = {Russell, R. and Kim, L. and Hamilton, L. and Lazovich, T. and Harer, J. and Ozdemir, O. and Ellingwood, P. and {McConley}, M.},
	date = {2018-12},
	keywords = {uni, \_tablet},
	file = {Russell et al_2018_Automated Vulnerability Detection in Source Code Using Deep Representation.pdf:/data/zotero/storage/AEJPSZDB/Russell et al_2018_Automated Vulnerability Detection in Source Code Using Deep Representation.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/S76LASA2/8614145.html:text/html}
}

@article{chawla_smote_2002,
	title = {{SMOTE}: synthetic minority over-sampling technique},
	volume = {16},
	url = {10/gd226v},
	doi = {10/gd226v},
	shorttitle = {{SMOTE}},
	pages = {321--357},
	journaltitle = {Journal of artificial intelligence research},
	author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
	date = {2002},
	keywords = {uni, \_tablet},
	file = {Chawla et al_2002_SMOTE.pdf:/data/zotero/storage/G2PRA58W/Chawla et al_2002_SMOTE.pdf:application/pdf;Snapshot:/data/zotero/storage/MM3VGYCU/10302.html:text/html}
}

@article{chen_slide_2020,
	title = {{SLIDE} : In Defense of Smart Algorithms over Hardware Acceleration for Large-Scale Deep Learning Systems},
	url = {http://arxiv.org/abs/1903.03129},
	shorttitle = {{SLIDE}},
	abstract = {Deep Learning ({DL}) algorithms are the central focus of modern machine learning systems. As data volumes keep growing, it has become customary to train large neural networks with hundreds of millions of parameters to maintain enough capacity to memorize these volumes and obtain state-of-the-art accuracy. To get around the costly computations associated with large models and data, the community is increasingly investing in specialized hardware for model training. However, specialized hardware is expensive and hard to generalize to a multitude of tasks. The progress on the algorithmic front has failed to demonstrate a direct advantage over powerful hardware such as {NVIDIA}-V100 {GPUs}. This paper provides an exception. We propose {SLIDE} (Sub-{LInear} Deep learning Engine) that uniquely blends smart randomized algorithms, with multi-core parallelism and workload optimization. Using just a {CPU}, {SLIDE} drastically reduces the computations during both training and inference outperforming an optimized implementation of Tensorflow ({TF}) on the best available {GPU}. Our evaluations on industry-scale recommendation datasets, with large fully connected architectures, show that training with {SLIDE} on a 44 core {CPU} is more than 3.5 times (1 hour vs. 3.5 hours) faster than the same network trained using {TF} on Tesla V100 at any given accuracy level. On the same {CPU} hardware, {SLIDE} is over 10x faster than {TF}. We provide codes and scripts for reproducibility.},
	journaltitle = {{arXiv}:1903.03129 [cs]},
	author = {Chen, Beidi and Medini, Tharun and Farwell, James and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
	urldate = {2021-04-09},
	date = {2020-02-29},
	eprinttype = {arxiv},
	eprint = {1903.03129},
	keywords = {out-of-scope, ⛔ No {DOI} found, \_tablet},
	file = {Chen et al_2020_SLIDE.pdf:/data/zotero/storage/ZQVJTBVK/Chen et al_2020_SLIDE.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/G33WEX89/1903.html:text/html}
}

@article{daghaghi_accelerating_2021,
	title = {Accelerating {SLIDE} Deep Learning on Modern {CPUs}: Vectorization, Quantizations, Memory Optimizations, and More},
	url = {http://arxiv.org/abs/2103.10891},
	shorttitle = {Accelerating {SLIDE} Deep Learning on Modern {CPUs}},
	abstract = {Deep learning implementations on {CPUs} (Central Processing Units) are gaining more traction. Enhanced {AI} capabilities on commodity x86 architectures are commercially appealing due to the reuse of existing hardware and virtualization ease. A notable work in this direction is the {SLIDE} system. {SLIDE} is a C++ implementation of a sparse hash table based back-propagation, which was shown to be significantly faster than {GPUs} in training hundreds of million parameter neural models. In this paper, we argue that {SLIDE}'s current implementation is sub-optimal and does not exploit several opportunities available in modern {CPUs}. In particular, we show how {SLIDE}'s computations allow for a unique possibility of vectorization via {AVX} (Advanced Vector Extensions)-512. Furthermore, we highlight opportunities for different kinds of memory optimization and quantizations. Combining all of them, we obtain up to 7x speedup in the computations on the same hardware. Our experiments are focused on large (hundreds of millions of parameters) recommendation and {NLP} models. Our work highlights several novel perspectives and opportunities for implementing randomized algorithms for deep learning on modern {CPUs}. We provide the code and benchmark scripts at https://github.com/{RUSH}-{LAB}/{SLIDE}},
	journaltitle = {{arXiv}:2103.10891 [cs]},
	author = {Daghaghi, Shabnam and Meisburger, Nicholas and Zhao, Mengnan and Wu, Yong and Gobriel, Sameh and Tai, Charlie and Shrivastava, Anshumali},
	urldate = {2021-04-09},
	date = {2021-03-05},
	eprinttype = {arxiv},
	eprint = {2103.10891},
	keywords = {out-of-scope, ⛔ No {DOI} found, \_tablet},
	file = {Daghaghi et al_2021_Accelerating SLIDE Deep Learning on Modern CPUs.pdf:/data/zotero/storage/SCY5SCA4/Daghaghi et al_2021_Accelerating SLIDE Deep Learning on Modern CPUs.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/A5H82MGN/2103.html:text/html}
}

@article{lin_cross-project_2018,
	title = {Cross-Project Transfer Representation Learning for Vulnerable Function Discovery},
	volume = {14},
	issn = {1941-0050},
	url = {10/gdwfhd},
	doi = {10/gdwfhd},
	abstract = {Machine learning is now widely used to detect security vulnerabilities in the software, even before the software is released. But its potential is often severely compromised at the early stage of a software project when we face a shortage of high-quality training data and have to rely on overly generic hand-crafted features. This paper addresses this cold-start problem of machine learning, by learning rich features that generalize across similar projects. To reach an optimal balance between feature-richness and generalizability, we devise a data-driven method including the following innovative ideas. First, the code semantics are revealed through serialized abstract syntax trees ({ASTs}), with tokens encoded by Continuous Bag-of-Words neural embeddings. Next, the serialized {ASTs} are fed to a sequential deep learning classifier (Bi-{LSTM}) to obtain a representation indicative of software vulnerability. Finally, the neural representation obtained from existing software projects is then transferred to the new project to enable early vulnerability detection even with a small set of training labels. To validate this vulnerability detection approach, we manually labeled 457 vulnerable functions and collected 30 000+ nonvulnerable functions from six open-source projects. The empirical results confirmed that the trained model is capable of generating representations that are indicative of program vulnerability and is adaptable across multiple projects. Compared with the traditional code metrics, our transfer-learned representations are more effective for predicting vulnerable functions, both within a project and across multiple projects.},
	pages = {3289--3297},
	number = {7},
	journaltitle = {{IEEE} Transactions on Industrial Informatics},
	author = {Lin, Guanjun and Zhang, Jun and Luo, Wei and Pan, Lei and Xiang, Yang and De Vel, Olivier and Montague, Paul},
	date = {2018-07},
	note = {Conference Name: {IEEE} Transactions on Industrial Informatics},
	keywords = {uni, \_tablet},
	file = {Lin et al_2018_Cross-Project Transfer Representation Learning for Vulnerable Function Discovery.pdf:/data/zotero/storage/ZGM27JVV/Lin et al_2018_Cross-Project Transfer Representation Learning for Vulnerable Function Discovery.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/KHHVTDWP/8329207.html:text/html}
}

@article{li_sysevr_2021,
	title = {{SySeVR}: A Framework for Using Deep Learning to Detect Software Vulnerabilities},
	issn = {1941-0018},
	url = {10/gjwbqv},
	doi = {10/gjwbqv},
	shorttitle = {{SySeVR}},
	abstract = {The detection of software vulnerabilities (or vulnerabilities for short) is an important problem that has yet to be tackled, as manifested by the many vulnerabilities reported on a daily basis. This calls for machine learning methods for vulnerability detection. Deep learning is attractive for this purpose because it alleviates the requirement to manually define features. Despite the tremendous success of deep learning in other application domains, its applicability to vulnerability detection is not systematically understood. In order to fill this void, we propose the first systematic framework for using deep learning to detect vulnerabilities in C/C++ programs with source code. The framework, dubbed Syntax-based, Semantics-based, and Vector Representations ({SySeVR}), focuses on obtaining program representations that can accommodate syntax and semantic information pertinent to vulnerabilities. Our experiments with 4 software products demonstrate the usefulness of the framework: we detect 15 vulnerabilities that are not reported in the National Vulnerability Database. Among these 15 vulnerabilities, 7 are unknown and have been reported to the vendors, and the other 8 have been “silently” patched by the vendors when releasing newer versions of the pertinent software products.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Dependable and Secure Computing},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Jin, Hai and Zhu, Yawei and Chen, Zhaoxuan},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Dependable and Secure Computing},
	keywords = {uni, \_tablet},
	file = {Li et al_2021_SySeVR.pdf:/data/zotero/storage/Y5CZXMWB/Li et al_2021_SySeVR.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/JH256RPT/9321538.html:text/html}
}

@article{li_vuldeelocator_2021,
	title = {{VulDeeLocator}: A Deep Learning-based Fine-grained Vulnerability Detector},
	issn = {1941-0018},
	url = {10/gjwbq7},
	doi = {10/gjwbq7},
	shorttitle = {{VulDeeLocator}},
	abstract = {Automatically detecting software vulnerabilities is an important problem that has attracted much attention from the academic research community. However, existing vulnerability detectors still cannot achieve the vulnerability detection capability and the locating precision that would warrant their adoption for real-world use. In this paper, we present a vulnerability detector that can simultaneously achieve a high detection capability and a high locating precision, dubbed Vulnerability Deep learning-based Locator ({VulDeeLocator}).In the course of designing {VulDeeLocator}, we encounter difficulties including how to accommodate semantic relations between the definitions of types as well as macros and their uses across files, how to accommodate accurate control flows and variable define-use relations, and how to achieve high locating precision. We solve these difficulties by using two innovative ideas: (i) leveraging intermediate code to accommodate extra semantic information, and (ii) using the notion of granularity refinement to pin down locations of vulnerabilities. When applied to 200 files randomly selected from three real-world software products, {VulDeeLocator} detects 18 confirmed vulnerabilities (i.e., true-positives). Among them, 16 vulnerabilities correspond to known vulnerabilities; the other two are not reported in the National Vulnerability Database ({NVD}) but have been silently patched by the vendor of Libav when releasing newer versions.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Dependable and Secure Computing},
	author = {Li, Zhen and Zou, Deqing and Xu, Shouhuai and Chen, Zhaoxuan and Zhu, Yawei and Jin, Hai},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Dependable and Secure Computing},
	keywords = {uni, \_tablet},
	file = {Li et al_2021_VulDeeLocator.pdf:/data/zotero/storage/2V5CCD4G/Li et al_2021_VulDeeLocator.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/GJ3VPHWJ/9416836.html:text/html}
}

@article{looks_deep_2017,
	title = {Deep Learning with Dynamic Computation Graphs},
	url = {http://arxiv.org/abs/1702.02181},
	abstract = {Neural networks that compute over graph structures are a natural fit for problems in a variety of domains, including natural language (parse trees) and cheminformatics (molecular graphs). However, since the computation graph has a different shape and size for every input, such networks do not directly support batched training or inference. They are also difficult to implement in popular deep learning libraries, which are based on static data-flow graphs. We introduce a technique called dynamic batching, which not only batches together operations between different input graphs of dissimilar shape, but also between different nodes within a single input graph. The technique allows us to create static graphs, using popular libraries, that emulate dynamic computation graphs of arbitrary shape and size. We further present a high-level library of compositional blocks that simplifies the creation of dynamic graph models. Using the library, we demonstrate concise and batch-wise parallel implementations for a variety of models from the literature.},
	journaltitle = {{arXiv}:1702.02181 [cs, stat]},
	author = {Looks, Moshe and Herreshoff, Marcello and Hutchins, {DeLesley} and Norvig, Peter},
	urldate = {2021-05-23},
	date = {2017-02-21},
	eprinttype = {arxiv},
	eprint = {1702.02181},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Looks et al_2017_Deep Learning with Dynamic Computation Graphs.pdf:/data/zotero/storage/DG33DUFL/Looks et al_2017_Deep Learning with Dynamic Computation Graphs.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/FG8II4NH/1702.html:text/html}
}

@article{khomenko_accelerating_2016,
	title = {Accelerating recurrent neural network training using sequence bucketing and multi-{GPU} data parallelization},
	url = {10/gj5rx8},
	doi = {10/gj5rx8},
	abstract = {An efficient algorithm for recurrent neural network training is presented. The approach increases the training speed for tasks where a length of the input sequence may vary significantly. The proposed approach is based on the optimal batch bucketing by input sequence length and data parallelization on multiple graphical processing units. The baseline training performance without sequence bucketing is compared with the proposed solution for a different number of buckets. An example is given for the online handwriting recognition task using an {LSTM} recurrent neural network. The evaluation is performed in terms of the wall clock time, number of epochs, and validation loss value.},
	pages = {100--103},
	journaltitle = {2016 {IEEE} First International Conference on Data Stream Mining \& Processing ({DSMP})},
	author = {Khomenko, Viacheslav and Shyshkov, Oleg and Radyvonenko, Olga and Bokhan, Kostiantyn},
	urldate = {2021-05-23},
	date = {2016-08},
	eprinttype = {arxiv},
	eprint = {1708.05604},
	keywords = {\_tablet},
	file = {Khomenko et al_2016_Accelerating recurrent neural network training using sequence bucketing and.pdf:/data/zotero/storage/EM8QMYWM/Khomenko et al_2016_Accelerating recurrent neural network training using sequence bucketing and.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/VNK3MQPN/1708.html:text/html}
}

@inproceedings{nguyen_reliability_2013,
	location = {Hangzhou, China},
	title = {The (un)reliability of {NVD} vulnerable versions data: an empirical experiment on Google Chrome vulnerabilities},
	isbn = {978-1-4503-1767-2},
	url = {10/gkf35c},
	doi = {10/gkf35c},
	series = {{ASIA} {CCS} '13},
	shorttitle = {The (un)reliability of {NVD} vulnerable versions data},
	abstract = {{NVD} is one of the most popular databases used by researchers to conduct empirical research on data sets of vulnerabilities. Our recent analysis on Chrome vulnerability data reported by {NVD} has revealed an abnormally phenomenon in the data where almost vulnerabilities were originated from the first versions. This inspires our experiment to validate the reliability of the {NVD} vulnerable version data. In this experiment, we verify for each version of Chrome that {NVD} claims vulnerable is actually vulnerable. The experiment revealed several errors in the vulnerability data of Chrome. Furthermore, we have also analyzed how these errors might impact the conclusions of an empirical study on foundational vulnerability. Our results show that different conclusions could be obtained due to the data errors.},
	pages = {493--498},
	booktitle = {Proceedings of the 8th {ACM} {SIGSAC} symposium on Information, computer and communications security},
	publisher = {Association for Computing Machinery},
	author = {Nguyen, Viet Hung and Massacci, Fabio},
	urldate = {2021-06-07},
	date = {2013-05-08},
	keywords = {uni, \_tablet},
	file = {Nguyen_Massacci_2013_The (un)reliability of NVD vulnerable versions data.pdf:/data/zotero/storage/MAQQLLVL/Nguyen_Massacci_2013_The (un)reliability of NVD vulnerable versions data.pdf:application/pdf}
}

@article{nguyen_automatic_2016,
	title = {An automatic method for assessing the versions affected by a vulnerability},
	volume = {21},
	issn = {1573-7616},
	url = {10/gbgdzg},
	doi = {10/gbgdzg},
	abstract = {Vulnerability data sources are used by academics to build models, and by industry and government to assess compliance. Errors in such data sources therefore not only are threats to validity in scientific studies, but also might cause organizations, which rely on retro versions of software, to lose compliance. In this work, we propose an automated method to determine the code evidence for the presence of vulnerabilities in retro software versions. The method scans the code base of each retro version of software for the code evidence to determine whether a retro version is vulnerable or not. It identifies the lines of code that were changed to fix vulnerabilities. If an earlier version contains these deleted lines, it is highly likely that this version is vulnerable. To show the scalability of the method we performed a large scale experiments on Chrome and Firefox (spanning 7,236 vulnerable files and approximately 9,800 vulnerabilities) on the National Vulnerability Database ({NVD}). The elimination of spurious vulnerability claims (e.g. entries to a vulnerability database such as {NVD}) found by our method may change the conclusions of studies on the prevalence of foundational vulnerabilities.},
	pages = {2268--2297},
	number = {6},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Nguyen, Viet Hung and Dashevskyi, Stanislav and Massacci, Fabio},
	urldate = {2021-06-07},
	date = {2016-12-01},
	langid = {english},
	keywords = {uni, \_tablet, cve},
	file = {Nguyen et al_2016_An automatic method for assessing the versions affected by a vulnerability.pdf:/data/zotero/storage/YVX5Z7RY/Nguyen et al_2016_An automatic method for assessing the versions affected by a vulnerability.pdf:application/pdf}
}

@inproceedings{massacci_which_2010,
	location = {Bolzano, Italy},
	title = {Which is the right source for vulnerability studies? an empirical analysis on Mozilla Firefox},
	isbn = {978-1-4503-0340-8},
	url = {10/bg22rw},
	doi = {10/bg22rw},
	series = {{MetriSec} '10},
	shorttitle = {Which is the right source for vulnerability studies?},
	abstract = {Recent years have seen a trend towards the notion of quantitative security assessment and the use of empirical methods to analyze or predict vulnerable components. Many papers focused on vulnerability discovery models based upon either a public vulnerability databases (e.g., {CVE}, {NVD}), or vendor ones (e.g., {MFSA}). Some combine these databases. Most of these works address a knowledge problem: can we understand the empirical causes of vulnerabilities? Can we predict them? Still, if the data sources do not completely capture the phenomenon we are interested in predicting, then our predictor might be optimal with respect to the data we have but unsatisfactory in practice. In our work, we focus on a more fundamental question: the quality of vulnerability database. We provide an analytical comparison of different security metric papers and the relative data sources. We also show, based on experimental data for Mozilla Firefox, how using different data sources might lead to completely different results.},
	pages = {1--8},
	booktitle = {Proceedings of the 6th International Workshop on Security Measurements and Metrics},
	publisher = {Association for Computing Machinery},
	author = {Massacci, Fabio and Nguyen, Viet Hung},
	urldate = {2021-06-07},
	date = {2010-09-15},
	keywords = {uni, \_tablet},
	file = {Massacci_Nguyen_2010_Which is the right source for vulnerability studies.pdf:/data/zotero/storage/B29HZXKV/Massacci_Nguyen_2010_Which is the right source for vulnerability studies.pdf:application/pdf}
}

@inproceedings{meneely_when_2013,
	title = {When a Patch Goes Bad: Exploring the Properties of Vulnerability-Contributing Commits},
	url = {10/gkf42t},
	doi = {10/gkf42t},
	shorttitle = {When a Patch Goes Bad},
	abstract = {Security is a harsh reality for software teams today. Developers must engineer secure software by preventing vulnerabilities, which are design and coding mistakes that have security consequences. Even in open source projects, vulnerable source code can remain unnoticed for years. In this paper, we traced 68 vulnerabilities in the Apache {HTTP} server back to the version control commits that contributed the vulnerable code originally. We manually found 124 Vulnerability-Contributing Commits ({VCCs}), spanning 17 years. In this exploratory study, we analyzed these {VCCs} quantitatively and qualitatively with the over-arching question: "What could developers have looked for to identify security concerns in this commit?" Specifically, we examined the size of the commit via code churn metrics, the amount developers overwrite each others' code via interactive churn metrics, exposure time between {VCC} and fix, and dissemination of the {VCC} to the development community via release notes and voting mechanisms. Our results show that {VCCs} are large: more than twice as much code churn on average than non-{VCCs}, even when normalized against lines of code. Furthermore, a commit was twice as likely to be a {VCC} when the author was a new developer to the source code. The insight from this study can help developers understand how vulnerabilities originate in a system so that security-related mistakes can be prevented or caught in the future.},
	eventtitle = {2013 {ACM} / {IEEE} International Symposium on Empirical Software Engineering and Measurement},
	pages = {65--74},
	booktitle = {2013 {ACM} / {IEEE} International Symposium on Empirical Software Engineering and Measurement},
	author = {Meneely, Andrew and Srinivasan, Harshavardhan and Musa, Ayemi and Tejeda, Alberto Rodríguez and Mokary, Matthew and Spates, Brian},
	date = {2013-10},
	note = {{ISSN}: 1949-3789},
	keywords = {uni, \_tablet},
	file = {Meneely et al_2013_When a Patch Goes Bad.pdf:/data/zotero/storage/GY8QPSTY/Meneely et al_2013_When a Patch Goes Bad.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/IJ3MWUAH/6681339.html:text/html}
}

@article{sanguino_software_2017,
	title = {Software Vulnerability Analysis Using {CPE} and {CVE}},
	url = {http://arxiv.org/abs/1705.05347},
	abstract = {In this paper, we analyze the Common Platform Enumeration ({CPE}) dictionary and the Common Vulnerabilities and Exposures ({CVE}) feeds. These repositories are widely used in Vulnerability Management Systems ({VMSs}) to check for known vulnerabilities in software products. The analysis shows, among other issues, a lack of synchronization between both datasets that can lead to incorrect results output by {VMSs} relying on those datasets. To deal with these problems, we developed a method that recommends to a user a prioritized list of {CPE} identifiers for a given software product. The user can then assign (and, if necessary, adapt) the most suitable {CPE} identifier to the software so that regular (e.g., daily) checks can find known vulnerabilities for this software in the {CVE} feeds. Our evaluation of this method shows that this interaction is indeed necessary because a fully automated {CPE} assignment is prone to errors due to the {CPE} and {CVE} shortcomings. We implemented an open-source {VMS} that employs the proposed method and published it on {GitHub}.},
	journaltitle = {{arXiv}:1705.05347 [cs]},
	author = {Sanguino, Luis Alberto Benthin and Uetz, Rafael},
	urldate = {2021-06-07},
	date = {2017-05-15},
	eprinttype = {arxiv},
	eprint = {1705.05347},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Sanguino_Uetz_2017_Software Vulnerability Analysis Using CPE and CVE.pdf:/data/zotero/storage/FMD68NI7/Sanguino_Uetz_2017_Software Vulnerability Analysis Using CPE and CVE.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/NFXIUT5X/1705.html:text/html}
}

@inproceedings{stuckman_mining_2014,
	title = {Mining Security Vulnerabilities from Linux Distribution Metadata},
	url = {10/gkf495},
	doi = {10/gkf495},
	abstract = {Security vulnerability research has long been hindered by the difficulty in obtaining structured, detailed data on individual vulnerabilities in sufficient quantities for analysis. We mined vulnerabilities from historical change log data from Linux distribution packages, tapping a yet-unexplored source of security data. Change logs provide a unified view of a application's evolution, version branching structure, and vulnerability patching history, allowing for the large-scale compilation of data on susceptible (and, in some cases, non-susceptible) versions of the original application for each vulnerability. We then compiled vulnerability datasets for multiple releases of Debian and Ubuntu Linux, analyzing trends in vulnerability patching over time. Patching practices in Debian and Ubuntu were similar, and patch rates stayed constant throughout each distribution's lifetime.},
	eventtitle = {2014 {IEEE} International Symposium on Software Reliability Engineering Workshops},
	pages = {323--328},
	booktitle = {2014 {IEEE} International Symposium on Software Reliability Engineering Workshops},
	author = {Stuckman, Jeffrey and Purtilo, James},
	date = {2014-11},
	keywords = {uni, \_tablet},
	file = {Stuckman_Purtilo_2014_Mining Security Vulnerabilities from Linux Distribution Metadata.pdf:/data/zotero/storage/PPUPCVLR/Stuckman_Purtilo_2014_Mining Security Vulnerabilities from Linux Distribution Metadata.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/VUISANH4/6983861.html:text/html}
}

@article{anwar_cleaning_2020,
	title = {Cleaning the {NVD}: Comprehensive Quality Assessment, Improvements, and Analyses},
	url = {http://arxiv.org/abs/2006.15074},
	shorttitle = {Cleaning the {NVD}},
	abstract = {Vulnerability databases are vital sources of information on emergent software security concerns. Security professionals, from system administrators to developers to researchers, heavily depend on these databases to track vulnerabilities and analyze security trends. How reliable and accurate are these databases though? In this paper, we explore this question with the National Vulnerability Database ({NVD}), the U.S. government's repository of vulnerability information that arguably serves as the industry standard. Through a systematic investigation, we uncover inconsistent or incomplete data in the {NVD} that can impact its practical uses, affecting information such as the vulnerability publication dates, names of vendors and products affected, vulnerability severity scores, and vulnerability type categorizations. We explore the extent of these discrepancies and identify methods for automated corrections. Finally, we demonstrate the impact that these data issues can pose by comparing analyses using the original and our rectified versions of the {NVD}. Ultimately, our investigation of the {NVD} not only produces an improved source of vulnerability information, but also provides important insights and guidance for the security community on the curation and use of such data sources.},
	journaltitle = {{arXiv}:2006.15074 [cs]},
	author = {Anwar, Afsah and Abusnaina, Ahmed and Chen, Songqing and Li, Frank and Mohaisen, David},
	urldate = {2021-06-07},
	date = {2020-06-26},
	eprinttype = {arxiv},
	eprint = {2006.15074},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Anwar et al_2020_Cleaning the NVD.pdf:/data/zotero/storage/QQSQUIM8/Anwar et al_2020_Cleaning the NVD.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/2G7XKHM2/2006.html:text/html}
}

@article{ghaffarian_software_2017,
	title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
	volume = {50},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3092566},
	doi = {10/gftfv7},
	shorttitle = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques},
	abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
	pages = {56:1--56:36},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
	urldate = {2021-06-08},
	date = {2017-08-25},
	keywords = {uni, \_tablet},
	file = {Ghaffarian_Shahriari_2017_Software Vulnerability Analysis and Discovery Using Machine-Learning and.pdf:/data/zotero/storage/A7MXT8K9/Ghaffarian_Shahriari_2017_Software Vulnerability Analysis and Discovery Using Machine-Learning and.pdf:application/pdf}
}

@inproceedings{fan_cc_2020,
	location = {Seoul, Republic of Korea},
	title = {A C/C++ Code Vulnerability Dataset with Code Changes and {CVE} Summaries},
	isbn = {978-1-4503-7517-7},
	url = {10/gjscq5},
	doi = {10/gjscq5},
	series = {{MSR} '20},
	abstract = {We collected a large C/C++ code vulnerability dataset from open-source Github projects, namely Big-Vul. We crawled the public Common Vulnerabilities and Exposures ({CVE}) database and {CVE}-related source code repositories. Specifically, we collected the descriptive information of the vulnerabilities from the {CVE} database, e.g., {CVE} {IDs}, {CVE} severity scores, and {CVE} summaries. With the {CVE} information and its related published Github code repository links, we downloaded all of the code repositories and extracted vulnerability related code changes. In total, Big-Vul contains 3,754 code vulnerabilities spanning 91 different vulnerability types. All these code vulnerabilities are extracted from 348 Github projects. All information is stored in the {CSV} format. We linked the code changes with the {CVE} descriptive information. Thus, our Big-Vul can be used for various research topics, e.g., detecting and fixing vulnerabilities, analyzing the vulnerability related code changes. Big-Vul is publicly available on Github.},
	pages = {508--512},
	booktitle = {Proceedings of the 17th International Conference on Mining Software Repositories},
	publisher = {Association for Computing Machinery},
	author = {Fan, Jiahao and Li, Yi and Wang, Shaohua and Nguyen, Tien N.},
	urldate = {2021-06-08},
	date = {2020-06-29},
	keywords = {uni, \_tablet},
	file = {Fan et al_2020_A C-C++ Code Vulnerability Dataset with Code Changes and CVE Summaries.pdf:/data/zotero/storage/84L7A2I2/Fan et al_2020_A C-C++ Code Vulnerability Dataset with Code Changes and CVE Summaries.pdf:application/pdf}
}

@article{ban_performance_2019,
	title = {A performance evaluation of deep-learnt features for software vulnerability detection},
	volume = {31},
	rights = {© 2018 John Wiley \& Sons, Ltd.},
	issn = {1532-0634},
	url = {10/gkgd5f},
	doi = {10/gkgd5f},
	abstract = {Software vulnerability is a critical issue in the realm of cyber security. In terms of techniques, machine learning ({ML}) has been successfully used in many real-world problems such as software vulnerability detection, malware detection and function recognition, for high-quality feature representation learning. In this paper, we propose a performance evaluation study on {ML} based solutions for software vulnerability detection, conducting three experiments: machine learning-based techniques for software vulnerability detection based on the scenario of single type of vulnerability and multiple types of vulnerabilities per dataset; machine learning-based techniques for cross-project software vulnerability detection; and software vulnerability detection when facing the class imbalance problem with varying imbalance ratios. Experimental results show that it is possible to employ software vulnerability detection based on {ML} techniques. However, {ML}-based techniques suffer poor performance on both cross-project and class imbalance problem in software vulnerability detection.},
	pages = {e5103},
	number = {19},
	journaltitle = {Concurrency and Computation: Practice and Experience},
	author = {Ban, Xinbo and Liu, Shigang and Chen, Chao and Chua, Caslon},
	urldate = {2021-06-08},
	date = {2019},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.5103},
	keywords = {uni, \_tablet},
	file = {Snapshot:/data/zotero/storage/JJHUKSAZ/cpe.html:text/html;Ban et al_2019_A performance evaluation of deep-learnt features for software vulnerability.pdf:/data/zotero/storage/XRCXHRLK/Ban et al_2019_A performance evaluation of deep-learnt features for software vulnerability.pdf:application/pdf}
}

@inproceedings{zheng_d2a_2021,
	title = {D2A: A Dataset Built for {AI}-Based Vulnerability Detection Methods Using Differential Analysis},
	url = {10/gkgd53},
	doi = {10/gkgd53},
	shorttitle = {D2A},
	abstract = {Static analysis tools are widely used for vulnerability detection as they understand programs with complex behavior and millions of lines of code. Despite their popularity, static analysis tools are known to generate an excess of false positives. The recent ability of Machine Learning models to understand programming languages opens new possibilities when applied to static analysis. However, existing datasets to train models for vulnerability identification suffer from multiple limitations such as limited bug context, limited size, and synthetic and unrealistic source code. We propose D2A, a differential analysis based approach to label issues reported by static analysis tools. The D2A dataset is built by analyzing version pairs from multiple open source projects. From each project, we select bug fixing commits and we run static analysis on the versions before and after such commits. If some issues detected in a before-commit version disappear in the corresponding after-commit version, they are very likely to be real bugs that got fixed by the commit. We use D2A to generate a large labeled dataset to train models for vulnerability identification. We show that the dataset can be used to build a classifier to identify possible false alarms among the issues reported by static analysis, hence helping developers prioritize and investigate potential true positives first.},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	pages = {111--120},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	author = {Zheng, Yunhui and Pujar, Saurabh and Lewis, Burn and Buratti, Luca and Epstein, Edward and Yang, Bo and Laredo, Jim and Morari, Alessandro and Su, Zhong},
	date = {2021-05},
	keywords = {uni, \_tablet},
	file = {Zheng et al_2021_D2A.pdf:/data/zotero/storage/66IDYMC5/Zheng et al_2021_D2A.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/A4BVAH7V/9402126.html:text/html}
}

@inproceedings{wijayasekara_mining_2012,
	title = {Mining Bug Databases for Unidentified Software Vulnerabilities},
	url = {10/gkgd54},
	doi = {10/gkgd54},
	abstract = {Identifying software vulnerabilities is becoming more important as critical and sensitive systems increasingly rely on complex software systems. It has been suggested in previous work that some bugs are only identified as vulnerabilities long after the bug has been made public. These vulnerabilities are known as hidden impact vulnerabilities. This paper discusses existing bug data mining classifiers and present an analysis of vulnerability databases showing the necessity to mine common publicly available bug databases for hidden impact vulnerabilities. We present a vulnerability analysis from January 2006 to April 2011 for two well known software packages: Linux kernel and {MySQL}. We show that 32\% (Linux) and 62\% ({MySQL}) of vulnerabilities discovered in this time period were hidden impact vulnerabilities. We also show that the percentage of hidden impact vulnerabilities has increased from 25\% to 36\% in Linux and from 59\% to 65\% in {MySQL} in the last two years. We then propose a hidden impact vulnerability identification methodology based on text mining classifier for bug databases. Finally, we discuss potential challenges faced by a development team when using such a classifier.},
	eventtitle = {2012 5th International Conference on Human System Interactions},
	pages = {89--96},
	booktitle = {2012 5th International Conference on Human System Interactions},
	author = {Wijayasekara, Dumidu and Manic, Milos and Wright, Jason L. and {McQueen}, Miles},
	date = {2012-06},
	note = {{ISSN}: 2158-2254},
	keywords = {uni, \_tablet},
	file = {Wijayasekara et al_2012_Mining Bug Databases for Unidentified Software Vulnerabilities.pdf:/data/zotero/storage/TC8MH4TQ/Wijayasekara et al_2012_Mining Bug Databases for Unidentified Software Vulnerabilities.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/NRNSYWB9/6473768.html:text/html}
}

@article{zou_vuldeepecker_2019,
	title = {μ{VulDeePecker}: A Deep Learning-Based System for Multiclass Vulnerability Detection},
	issn = {1941-0018},
	url = {10/gkgd74},
	doi = {10/gkgd74},
	shorttitle = {μ{VulDeePecker}},
	abstract = {Fine-grained software vulnerability detection is an important and challenging problem. Ideally, a detection system (or detector) not only should be able to detect whether or not a program contains vulnerabilities, but also should be able to pinpoint the type of a vulnerability in question. Existing vulnerability detection methods based on deep learning can detect the presence of vulnerabilities (i.e., addressing the binary classification or detection problem), but cannot pinpoint types of vulnerabilities (i.e., incapable of addressing multiclass classification). In this paper, we propose the first deep learning-based system for multiclass vulnerability detection, dubbed μ{VulDeePecker}. The key insight underlying μ{VulDeePecker} is the concept of code attention, which can capture information that can help pinpoint types of vulnerabilities, even when the samples are small. For this purpose, we create a dataset from scratch and use it to evaluate the effectiveness of μ{VulDeePecker}. Experimental results show that μ{VulDeePecker} is effective for multiclass vulnerability detection and that accommodating control-dependence (other than data-dependence) can lead to higher detection capabilities.},
	pages = {1--1},
	journaltitle = {{IEEE} Transactions on Dependable and Secure Computing},
	author = {Zou, Deqing and Wang, Sujuan and Xu, Shouhuai and Li, Zhen and Jin, Hai},
	date = {2019},
	note = {Conference Name: {IEEE} Transactions on Dependable and Secure Computing},
	keywords = {uni, \_tablet},
	file = {Zou et al_2019_μVulDeePecker.pdf:/data/zotero/storage/Z7PW8HBN/Zou et al_2019_μVulDeePecker.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/YRZG26T2/8846081.html:text/html}
}

@article{wu_cve-assisted_2020,
	title = {{CVE}-assisted large-scale security bug report dataset construction method},
	volume = {160},
	issn = {01641212},
	url = {10.1016/j.jss.2019.110456},
	doi = {10/gkgfm5},
	pages = {110456},
	journaltitle = {Journal of Systems and Software},
	shortjournal = {Journal of Systems and Software},
	author = {Wu, Xiaoxue and Zheng, Wei and Chen, Xiang and Wang, Fang and Mu, Dejun},
	urldate = {2021-06-08},
	date = {2020-02},
	langid = {english},
	file = {Wu et al_2020_CVE-assisted large-scale security bug report dataset construction method.pdf:/data/zotero/storage/CWBBH4NZ/Wu et al_2020_CVE-assisted large-scale security bug report dataset construction method.pdf:application/pdf}
}

@inproceedings{ponta_manually-curated_2019,
	title = {A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software},
	url = {10/ghbg26},
	doi = {10/ghbg26},
	abstract = {Advancing our understanding of software vulnerabilities, automating their identification, the analysis of their impact, and ultimately their mitigation is necessary to enable the development of software that is more secure. While operating a vulnerability assessment tool, which we developed, and that is currently used by hundreds of development units at {SAP}, we manually collected and curated a dataset of vulnerabilities of open-source software, and the commits fixing them. The data were obtained both from the National Vulnerability Database ({NVD}), and from project-specific web resources, which we monitor on a continuous basis. From that data, we extracted a dataset that maps 624 publicly disclosed vulnerabilities affecting 205 distinct opensource Java projects, used in {SAP} products or internal tools, onto the 1282 commits that fix them. Out of 624 vulnerabilities, 29 do not have a {CVE} (Common Vulnerability and Exposure) identifier at all, and 46, which do have such identifier assigned by a numbering authority, are not available in the {NVD} yet. The dataset is released under an open-source license, together with supporting scripts that allow researchers to automatically retrieve the actual content of the commits from the corresponding repositories, and to augment the attributes available for each instance. Moreover, these scripts allow to complement the dataset with additional instances that are not security fixes (which is useful, for example, in machine learning applications). Our dataset has been successfully used to train classifiers that could automatically identify security-relevant commits in code repositories. The release of this dataset and the supporting code as open-source will allow future research to be based on data of industrial relevance; it also represents a concrete step towards making the maintenance of this dataset a shared effort involving open-source communities, academia, and the industry.},
	eventtitle = {2019 {IEEE}/{ACM} 16th International Conference on Mining Software Repositories ({MSR})},
	pages = {383--387},
	booktitle = {2019 {IEEE}/{ACM} 16th International Conference on Mining Software Repositories ({MSR})},
	author = {Ponta, Serena Elisa and Plate, Henrik and Sabetta, Antonino and Bezzi, Michele and Dangremont, Cédric},
	date = {2019-05},
	note = {{ISSN}: 2574-3864},
	keywords = {uni, \_tablet},
	file = {Ponta et al_2019_A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software.pdf:/data/zotero/storage/NX5LBVKV/Ponta et al_2019_A Manually-Curated Dataset of Fixes to Vulnerabilities of Open-Source Software.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/THF7FRZG/8816802.html:text/html}
}

@article{geng_empirical_2020,
	title = {An Empirical Study on Benchmarks of Artificial Software Vulnerabilities},
	url = {http://arxiv.org/abs/2003.09561},
	abstract = {Recently, various techniques (e.g., fuzzing) have been developed for vulnerability detection. To evaluate those techniques, the community has been developing benchmarks of artificial vulnerabilities because of a shortage of ground-truth. However, people have concerns that such vulnerabilities cannot represent reality and may lead to unreliable and misleading results. Unfortunately, there lacks research on handling such concerns. In this work, to understand how close these benchmarks mirror reality, we perform an empirical study on three artificial vulnerability benchmarks - {LAVA}-M, Rode0day and {CGC} (2669 bugs) and various real-world memory-corruption vulnerabilities (80 {CVEs}). Furthermore, we propose a model to depict the properties of memory-corruption vulnerabilities. Following this model, we conduct intensive experiments and data analyses. Our analytic results reveal that while artificial benchmarks attempt to approach the real world, they still significantly differ from reality. Based on the findings, we propose a set of strategies to improve the quality of artificial benchmarks.},
	journaltitle = {{arXiv}:2003.09561 [cs]},
	author = {Geng, Sijia and Li, Yuekang and Du, Yunlan and Xu, Jun and Liu, Yang and Mao, Bing},
	urldate = {2021-06-08},
	date = {2020-03-20},
	eprinttype = {arxiv},
	eprint = {2003.09561},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Geng et al_2020_An Empirical Study on Benchmarks of Artificial Software Vulnerabilities.pdf:/data/zotero/storage/X9VAXVF6/Geng et al_2020_An Empirical Study on Benchmarks of Artificial Software Vulnerabilities.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/57EB3WVA/2003.html:text/html}
}

@inproceedings{hogan_challenges_2019,
	title = {The Challenges of Labeling Vulnerability-Contributing Commits},
	url = {10/gkgfnb},
	doi = {10/gkgfnb},
	abstract = {Software projects developed using version control are enhanced incrementally through commits, some of which inevitably introduce security vulnerabilities. The features of these vulnerability-contributing commits ({VCCs}) could be used to train a {VCC} detector or to inform software development best-practices. Previous work has attempted to label {VCCs} in open-source software projects for this purpose. We present a manual approach to {VCC} labeling using the fix commits listed in Common Vulnerabilities and Exposures ({CVEs}). We show that a published automated method of {VCC} labeling disagrees with our manual method on 42\% of {VCCs}. We argue that the automated method, while effective in scaling {VCC} labeling, is therefore not sufficiently accurate. Finally, we discuss the benefits and drawbacks of trying to predict vulnerable software components rather than {VCCs}.},
	eventtitle = {2019 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	pages = {270--275},
	booktitle = {2019 {IEEE} International Symposium on Software Reliability Engineering Workshops ({ISSREW})},
	author = {Hogan, Kevin and Warford, Noel and Morrison, Robert and Miller, David and Malone, Sean and Purtilo, James},
	date = {2019-10},
	keywords = {uni, \_tablet},
	file = {Hogan et al_2019_The Challenges of Labeling Vulnerability-Contributing Commits.pdf:/data/zotero/storage/23F9B8UR/Hogan et al_2019_The Challenges of Labeling Vulnerability-Contributing Commits.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/5V8B4P7A/8990315.html:text/html}
}

@article{wang_systematic_2020,
	title = {A systematic review of fuzzing based on machine learning techniques},
	volume = {15},
	issn = {1932-6203},
	url = {10/ghmdpr},
	doi = {10/ghmdpr},
	abstract = {Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzz testing faces many challenges, such as how to mutate input seed files, how to increase code coverage, and how to bypass the format verification effectively. Therefore machine learning techniques have been introduced as a new method into fuzz testing to alleviate these challenges. This paper reviews the research progress of using machine learning techniques for fuzz testing in recent years, analyzes how machine learning improves the fuzzing process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies five different stages in which machine learning has been used. Then this paper systematically studies machine learning-based fuzzing models from five dimensions of selection of machine learning algorithms, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Secondly, this paper assesses the performance of the machine learning techniques in existing research for fuzz testing. The results of the evaluation prove that machine learning techniques have an acceptable capability of prediction for fuzzing. Finally, the capability of discovering vulnerabilities both traditional fuzzers and machine learning-based fuzzers is analyzed. The results depict that the introduction of machine learning techniques can improve the performance of fuzzing. We hope to provide researchers with a systematic and more in-depth understanding of fuzzing based on machine learning techniques and provide some references for this field through analysis and summarization of multiple dimensions.},
	pages = {e0237749},
	number = {8},
	journaltitle = {{PLOS} {ONE}},
	shortjournal = {{PLOS} {ONE}},
	author = {Wang, Yan and Jia, Peng and Liu, Luping and Huang, Cheng and Liu, Zhonglin},
	urldate = {2021-06-08},
	date = {2020-08-18},
	langid = {english},
	note = {Publisher: Public Library of Science},
	keywords = {uni, \_tablet, fuzz},
	file = {Wang et al_2020_A systematic review of fuzzing based on machine learning techniques.pdf:/data/zotero/storage/KGZ4VLNT/Wang et al_2020_A systematic review of fuzzing based on machine learning techniques.pdf:application/pdf;Snapshot:/data/zotero/storage/HL7BYTTG/article.html:text/html}
}

@inproceedings{she_neuzz_2019,
	title = {{NEUZZ}: Efficient Fuzzing with Neural Program Smoothing},
	url = {10/ggdcc3},
	doi = {10/ggdcc3},
	shorttitle = {{NEUZZ}},
	abstract = {Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the target program's discrete branching behavior. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly increase the efficiency of the fuzzing process. Our extensive evaluations demonstrate that {NEUZZ} significantly outperforms 10 state-of-the-art graybox fuzzers on 10 popular real-world programs both at finding new bugs and achieving higher edge coverage. {NEUZZ} found 31 previously unknown bugs (including two {CVEs}) that other fuzzers failed to find in 10 real-world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers over 24 hour runs. Furthermore, {NEUZZ} also outperformed existing fuzzers on both {LAVA}-M and {DARPA} {CGC} bug datasets.},
	eventtitle = {2019 {IEEE} Symposium on Security and Privacy ({SP})},
	pages = {803--817},
	booktitle = {2019 {IEEE} Symposium on Security and Privacy ({SP})},
	author = {She, Dongdong and Pei, Kexin and Epstein, Dave and Yang, Junfeng and Ray, Baishakhi and Jana, Suman},
	date = {2019-05},
	note = {{ISSN}: 2375-1207},
	keywords = {uni, \_tablet, fuzz},
	file = {She et al_2019_NEUZZ.pdf:/data/zotero/storage/NSLVD45S/She et al_2019_NEUZZ.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/JFZCTFV3/8835342.html:text/html}
}

@article{wang_neufuzz_2019,
	title = {{NeuFuzz}: Efficient Fuzzing With Deep Neural Network},
	volume = {7},
	issn = {2169-3536},
	url = {10/gf9rtm},
	doi = {10/gf9rtm},
	shorttitle = {{NeuFuzz}},
	abstract = {Coverage-guided graybox fuzzing is one of the most popular and effective techniques for discovering vulnerabilities due to its nature of high speed and scalability. However, the existing techniques generally focus on code coverage but not on vulnerable code. These techniques aim to cover as many paths as possible rather than to explore paths that are more likely to be vulnerable. When selecting the seeds to test, the existing fuzzers usually treat all seed inputs equally, ignoring the fact that paths exercised by different seed inputs are not equally vulnerable. This results in wasting time testing uninteresting paths rather than vulnerable paths, thus reducing the efficiency of vulnerability detection. In this paper, we present a solution, {NeuFuzz}, using the deep neural network to guide intelligent seed selection during graybox fuzzing to alleviate the aforementioned limitation. In particular, the deep neural network is used to learn the hidden vulnerability pattern from a large number of vulnerable and clean program paths to train a prediction model to classify whether paths are vulnerable. The fuzzer then prioritizes seed inputs that are capable of covering the likely to be vulnerable paths and assigns more mutation energy (i.e., the number of inputs to be generated) to these seeds. We implemented a prototype of {NeuFuzz} based on an existing fuzzer {PTfuzz} and evaluated it on two different test suites: {LAVA}-M and nine real-world applications. The experimental results showed that {NeuFuzz} can find more vulnerabilities than the existing fuzzers in less time. We have found 28 new security bugs in these applications, 21 of which have been assigned as {CVE} {IDs}.},
	pages = {36340--36352},
	journaltitle = {{IEEE} Access},
	author = {Wang, Yunchao and Wu, Zehui and Wei, Qiang and Wang, Qingxian},
	date = {2019},
	note = {Conference Name: {IEEE} Access},
	keywords = {uni, \_tablet, fuzz},
	file = {Wang et al_2019_NeuFuzz.pdf:/data/zotero/storage/MLU667LE/Wang et al_2019_NeuFuzz.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/DM2H22LU/8672949.html:text/html}
}

@inproceedings{he_learning_2019,
	location = {London, United Kingdom},
	title = {Learning to Fuzz from Symbolic Execution with Application to Smart Contracts},
	isbn = {978-1-4503-6747-9},
	url = {10/ggftn4},
	doi = {10/ggftn4},
	series = {{CCS} '19},
	abstract = {Fuzzing and symbolic execution are two complementary techniques for discovering software vulnerabilities. Fuzzing is fast and scalable, but can be ineffective when it fails to randomly select the right inputs. Symbolic execution is thorough but slow and often does not scale to deep program paths with complex path conditions. In this work, we propose to learn an effective and fast fuzzer from symbolic execution, by phrasing the learning task in the framework of imitation learning. During learning, a symbolic execution expert generates a large number of quality inputs improving coverage on thousands of programs. Then, a fuzzing policy, represented with a suitable architecture of neural networks, is trained on the generated dataset. The learned policy can then be used to fuzz new programs. We instantiate our approach to the problem of fuzzing smart contracts, a domain where contracts often implement similar functionality (facilitating learning) and security is of utmost importance. We present an end-to-end system, {ILF} (for Imitation Learning based Fuzzer), and an extensive evaluation over {\textgreater}18K contracts. Our results show that {ILF} is effective: (i) it is fast, generating 148 transactions per second, (ii) it outperforms existing fuzzers (e.g., achieving 33\% more coverage), and (iii) it detects more vulnerabilities than existing fuzzing and symbolic execution tools for Ethereum.},
	pages = {531--548},
	booktitle = {Proceedings of the 2019 {ACM} {SIGSAC} Conference on Computer and Communications Security},
	publisher = {Association for Computing Machinery},
	author = {He, Jingxuan and Balunović, Mislav and Ambroladze, Nodar and Tsankov, Petar and Vechev, Martin},
	urldate = {2021-06-08},
	date = {2019-11-06},
	keywords = {\_tablet, fuzz, blockchain},
	file = {He et al_2019_Learning to Fuzz from Symbolic Execution with Application to Smart Contracts.pdf:/data/zotero/storage/6IWHKZ2C/He et al_2019_Learning to Fuzz from Symbolic Execution with Application to Smart Contracts.pdf:application/pdf}
}

@article{drozd_fuzzergym_2018,
	title = {{FuzzerGym}: A Competitive Framework for Fuzzing and Learning},
	url = {http://arxiv.org/abs/1807.07490},
	shorttitle = {{FuzzerGym}},
	abstract = {Fuzzing is a commonly used technique designed to test software by automatically crafting program inputs. Currently, the most successful fuzzing algorithms emphasize simple, low-overhead strategies with the ability to efficiently monitor program state during execution. Through compile-time instrumentation, these approaches have access to numerous aspects of program state including coverage, data flow, and heterogeneous fault detection and classification. However, existing approaches utilize blind random mutation strategies when generating test inputs. We present a different approach that uses this state information to optimize mutation operators using reinforcement learning ({RL}). By integrating {OpenAI} Gym with {libFuzzer} we are able to simultaneously leverage advancements in reinforcement learning as well as fuzzing to achieve deeper coverage across several varied benchmarks. Our technique connects the rich, efficient program monitors provided by {LLVM} Santizers with a deep neural net to learn mutation selection strategies directly from the input data. The cross-language, asynchronous architecture we developed enables us to apply any {OpenAI} Gym compatible deep reinforcement learning algorithm to any fuzzing problem with minimal slowdown.},
	journaltitle = {{arXiv}:1807.07490 [cs]},
	author = {Drozd, William and Wagner, Michael D.},
	urldate = {2021-06-08},
	date = {2018-07-19},
	eprinttype = {arxiv},
	eprint = {1807.07490},
	keywords = {⛔ No {DOI} found, fuzz},
	file = {Drozd_Wagner_2018_FuzzerGym.pdf:/data/zotero/storage/DYJ4SXEW/Drozd_Wagner_2018_FuzzerGym.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/MPTLZN9I/1807.html:text/html}
}

@article{ding_empirical_2021,
	title = {An Empirical Study of {OSS}-Fuzz Bugs},
	url = {http://arxiv.org/abs/2103.11518},
	abstract = {Continuous fuzzing is an increasingly popular technique for automated quality and security assurance. Google maintains {OSS}-Fuzz: a continuous fuzzing service for open source software. We conduct the first empirical study of {OSS}-Fuzz, analyzing 23,907 bugs found in 316 projects. We examine the characteristics of fuzzer-found faults, the lifecycles of such faults, and the evolution of fuzzing campaigns over time. We find that {OSS}-Fuzz is often effective at quickly finding bugs, and developers are often quick to patch them. However, flaky bugs, timeouts, and out of memory errors are problematic, people rarely file {CVEs} for security vulnerabilities, and fuzzing campaigns often exhibit punctuated equilibria, where developers might be surprised by large spikes in bugs found. Our findings have implications on future fuzzing research and practice.},
	journaltitle = {{arXiv}:2103.11518 [cs]},
	author = {Ding, Zhen Yu and Goues, Claire Le},
	urldate = {2021-06-08},
	date = {2021-03-21},
	eprinttype = {arxiv},
	eprint = {2103.11518},
	keywords = {⛔ No {DOI} found, uni, \_tablet, fuzz},
	file = {Ding_Goues_2021_An Empirical Study of OSS-Fuzz Bugs.pdf:/data/zotero/storage/D24UVEZP/Ding_Goues_2021_An Empirical Study of OSS-Fuzz Bugs.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DZ9IK67G/2103.html:text/html}
}

@inproceedings{babic_fudge_2019,
	location = {Tallinn, Estonia},
	title = {{FUDGE}: fuzz driver generation at scale},
	isbn = {978-1-4503-5572-8},
	url = {10/gh6h37},
	doi = {10/gh6h37},
	series = {{ESEC}/{FSE} 2019},
	shorttitle = {{FUDGE}},
	abstract = {At Google we have found tens of thousands of security and robustness bugs by fuzzing C and C++ libraries. To fuzz a library, a fuzzer requires a fuzz driver—which exercises some library code—to which it can pass inputs. Unfortunately, writing fuzz drivers remains a primarily manual exercise, a major hindrance to the widespread adoption of fuzzing. In this paper, we address this major hindrance by introducing the Fudge system for automated fuzz driver generation. Fudge automatically generates fuzz driver candidates for libraries based on existing client code. We have used Fudge to generate thousands of new drivers for a wide variety of libraries. Each generated driver includes a synthesized C/C++ program and a corresponding build script, and is automatically analyzed for quality. Developers have integrated over 200 of these generated drivers into continuous fuzzing services and have committed to address reported security bugs. Further, several of these fuzz drivers have been upstreamed to open source projects and integrated into the {OSS}-Fuzz fuzzing infrastructure. Running these fuzz drivers has resulted in over 150 bug fixes, including the elimination of numerous exploitable security vulnerabilities.},
	pages = {975--985},
	booktitle = {Proceedings of the 2019 27th {ACM} Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Babić, Domagoj and Bucur, Stefan and Chen, Yaohui and Ivančić, Franjo and King, Tim and Kusano, Markus and Lemieux, Caroline and Szekeres, László and Wang, Wei},
	urldate = {2021-06-08},
	date = {2019-08-12},
	keywords = {\_tablet, fuzz},
	file = {Babić et al_2019_FUDGE.pdf:/data/zotero/storage/I3GFZYKZ/Babić et al_2019_FUDGE.pdf:application/pdf}
}

@inproceedings{liu_large-scale_2020,
	title = {A Large-Scale Empirical Study on Vulnerability Distribution within Projects and the Lessons Learned},
	abstract = {The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.},
	eventtitle = {2020 {IEEE}/{ACM} 42nd International Conference on Software Engineering ({ICSE})},
	pages = {1547--1559},
	booktitle = {2020 {IEEE}/{ACM} 42nd International Conference on Software Engineering ({ICSE})},
	author = {Liu, Bingchang and Meng, Guozhu and Zou, Wei and Gong, Qi and Li, Feng and Lin, Min and Sun, Dandan and Huo, Wei and Zhang, Chao},
	date = {2020-10},
	note = {{ISSN}: 1558-1225},
	keywords = {uni, \_tablet},
	file = {Liu et al_2020_A Large-Scale Empirical Study on Vulnerability Distribution within Projects and.pdf:/data/zotero/storage/AQ58P7LV/Liu et al_2020_A Large-Scale Empirical Study on Vulnerability Distribution within Projects and.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/WNPETSK3/9283967.html:text/html}
}

@inproceedings{ispoglou_fuzzgen_2020,
	title = {{FuzzGen}: Automatic Fuzzer Generation},
	isbn = {978-1-939133-17-5},
	url = {https://www.usenix.org/conference/usenixsecurity20/presentation/ispoglou},
	shorttitle = {{FuzzGen}},
	eventtitle = {29th \{{USENIX}\} Security Symposium (\{{USENIX}\} Security 20)},
	pages = {2271--2287},
	author = {Ispoglou, Kyriakos and Austin, Daniel and Mohan, Vishwath and Payer, Mathias},
	urldate = {2021-06-08},
	date = {2020},
	langid = {english},
	keywords = {fuzz},
	file = {Ispoglou et al_2020_FuzzGen.pdf:/data/zotero/storage/T2JVCIDC/Ispoglou et al_2020_FuzzGen.pdf:application/pdf;Snapshot:/data/zotero/storage/JCSRBASA/ispoglou.html:text/html}
}

@inproceedings{zhang_intelligen_2021,
	title = {{IntelliGen}: Automatic Driver Synthesis for Fuzz Testing},
	url = {10/gkgf5t},
	doi = {10/gkgf5t},
	shorttitle = {{IntelliGen}},
	abstract = {Fuzzing is a technique widely used in vulnerability detection. The process usually involves writing effective fuzz driver programs, which, when done manually, can be extremely labor intensive. Previous attempts at automation leave much to be desired, in either degree of automation or quality of output.In this paper, we propose {IntelliGen}, a framework that constructs valid fuzz drivers automatically. First, {IntelliGen} determines a set of entry functions and evaluates their respective chance of exhibiting a vulnerability. Then, {IntelliGen} generates fuzz drivers for the entry functions through hierarchical parameter replacement and type inference. We implemented {IntelliGen} and evaluated its effectiveness on real-world programs selected from the Android Open-Source Project, Google’s fuzzer-test-suite and industrial collaborators. {IntelliGen} covered on average 1.08 ×-2.03× more basic blocks and 1.36×-2.06× more paths over state-of-the-art fuzz driver synthesizers {FUDGE} and {FuzzGen}. {IntelliGen} performed on par with manually written drivers and found 10 more bugs.},
	eventtitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	pages = {318--327},
	booktitle = {2021 {IEEE}/{ACM} 43rd International Conference on Software Engineering: Software Engineering in Practice ({ICSE}-{SEIP})},
	author = {Zhang, Mingrui and Liu, Jianzhong and Ma, Fuchen and Zhang, Huafeng and Jiang, Yu},
	date = {2021-05},
	keywords = {fuzz},
	file = {Zhang et al_2021_IntelliGen.pdf:/data/zotero/storage/K3K4K9ZE/Zhang et al_2021_IntelliGen.pdf:application/pdf;IEEE Xplore Abstract Record:/data/zotero/storage/3KJ87V6X/9402152.html:text/html}
}

@inproceedings{hulin_autoctf_2017,
	title = {{AutoCTF}: Creating Diverse Pwnables via Automated Bug Injection},
	url = {https://www.usenix.org/conference/woot17/workshop-program/presentation/hulin},
	shorttitle = {{AutoCTF}},
	eventtitle = {11th \{{USENIX}\} Workshop on Offensive Technologies (\{{WOOT}\} 17)},
	author = {Hulin, Patrick and Davis, Andy and Sridhar, Rahul and Fasano, Andrew and Gallagher, Cody and Sedlacek, Aaron and Leek, Tim and Dolan-Gavitt, Brendan},
	urldate = {2021-06-08},
	date = {2017},
	langid = {english},
	keywords = {⛔ No {DOI} found, fuzz},
	file = {Hulin et al_2017_AutoCTF.pdf:/data/zotero/storage/RXBEAANI/Hulin et al_2017_AutoCTF.pdf:application/pdf;Snapshot:/data/zotero/storage/PR4SC5XN/hulin.html:text/html}
}

@article{bundt_evaluating_2021,
	title = {Evaluating Synthetic Bugs},
	author = {Bundt, Joshua and Fasano, Andrew and Dolan-Gavitt, Brendan and Robertson, William and Leek, Tim},
	date = {2021},
	keywords = {⛔ No {DOI} found, fuzz},
	file = {Bundt et al_2021_Evaluating Synthetic Bugs.pdf:/data/zotero/storage/YX23MWEA/Bundt et al_2021_Evaluating Synthetic Bugs.pdf:application/pdf}
}

@article{fioraldi_program_2020,
	title = {Program State Abstraction for Feedback-Driven Fuzz Testing using Likely Invariants},
	url = {http://arxiv.org/abs/2012.11182},
	abstract = {Fuzz testing proved its great effectiveness in finding software bugs in the latest years, however, there are still open challenges. Coverage-guided fuzzers suffer from the fact that covering a program point does not ensure the trigger of a fault. Other more sensitive techniques that in theory should cope with this problem, such as the coverage of the memory values, easily lead to path explosion. In this thesis, we propose a new feedback for Feedback-driven Fuzz testing that combines code coverage with the "shape" of the data. We learn likely invariants for each basic block in order to divide into regions the space described by the variables used in the block. The goal is to distinguish in the feedback when a block is executed with values that fall in different regions of the space. This better approximates the program state coverage and, on some targets, improves the ability of the fuzzer in finding faults. We developed a prototype using {LLVM} and {AFL}++ called {InvsCov}.},
	journaltitle = {{arXiv}:2012.11182 [cs]},
	author = {Fioraldi, Andrea},
	urldate = {2021-06-08},
	date = {2020-12-21},
	eprinttype = {arxiv},
	eprint = {2012.11182},
	keywords = {⛔ No {DOI} found, fuzz},
	file = {Fioraldi_2020_Program State Abstraction for Feedback-Driven Fuzz Testing using Likely.pdf:/data/zotero/storage/V9PAY8EQ/Fioraldi_2020_Program State Abstraction for Feedback-Driven Fuzz Testing using Likely.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/V7SF2LPA/2012.html:text/html}
}

@article{hazimeh_magma_2020,
	title = {Magma: A Ground-Truth Fuzzing Benchmark},
	volume = {4},
	url = {10/gjbb43},
	doi = {10/gjbb43},
	shorttitle = {Magma},
	abstract = {High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count---perhaps the most commonly-used performance metric---is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers ({AFL}, {AFLFast}, {AFL}++, {FairFuzz}, {MOpt}-{AFL}, honggfuzz, and {SymCC}-{AFL}) against Magma over 200,000 {CPU}-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.},
	pages = {49:1--49:29},
	number = {3},
	journaltitle = {Proceedings of the {ACM} on Measurement and Analysis of Computing Systems},
	shortjournal = {Proc. {ACM} Meas. Anal. Comput. Syst.},
	author = {Hazimeh, Ahmad and Herrera, Adrian and Payer, Mathias},
	urldate = {2021-06-08},
	date = {2020-11-30},
	keywords = {to-read, \_tablet, fuzz},
	file = {Hazimeh et al_2020_Magma.pdf:/data/zotero/storage/IIM4SRW2/Hazimeh et al_2020_Magma.pdf:application/pdf}
}

@report{cheikes_common_2011,
	title = {Common Platform Enumeration: Naming Specification Version 2.3},
	url = {https://csrc.nist.gov/publications/detail/nistir/7695/final},
	shorttitle = {Common Platform Enumeration},
	abstract = {This report defines the Common Platform Enumeration ({CPE}) Naming version 2.3 specification. The {CPE} Naming specification is a part of a stack of {CPE} specifications that support a variety of use cases relating to {IT} product description and naming. The {CPE} Naming specification defines the logical structure of names for {IT} product classes and the procedures for binding and unbinding these names to and from machine-readable encodings. This report also defines and explains the requirements that {IT} products must meet for conformance with the {CPE} Naming version 2.3 specification.},
	number = {{NIST} Internal or Interagency Report ({NISTIR}) 7695},
	institution = {National Institute of Standards and Technology},
	author = {Cheikes, Brant and Waltermire, David and Scarfone, Karen},
	urldate = {2021-06-10},
	date = {2011-08-19},
	langid = {english},
	doi = {10.6028/NIST.IR.7695},
	keywords = {uni},
	file = {Full Text PDF:/data/zotero/storage/SZYDFDK4/Cheikes et al. - 2011 - Common Platform Enumeration Naming Specification .pdf:application/pdf;Snapshot:/data/zotero/storage/9USI3P6T/final.html:text/html}
}

@article{chirkova_empirical_2021,
	title = {Empirical Study of Transformers for Source Code},
	url = {10.1145/3468264.3468611},
	doi = {10.1145/3468264.3468611},
	abstract = {Initially developed for natural language processing ({NLP}), Transformers are now widely used for source code processing, due to the format similarity between source code and text. In contrast to natural language, source code is strictly structured, i.e., it follows the syntax of the programming language. Several recent works develop Transformer modifications for capturing syntactic information in source code. The drawback of these works is that they do not compare to each other and consider different tasks. In this work, we conduct a thorough empirical study of the capabilities of Transformers to utilize syntactic information in different tasks. We consider three tasks (code completion, function naming and bug fixing) and re-implement different syntax-capturing modifications in a unified framework. We show that Transformers are able to make meaningful predictions based purely on syntactic information and underline the best practices of taking the syntactic information into account for improving the performance of the model.},
	journaltitle = {{arXiv}:2010.07987 [cs]},
	author = {Chirkova, Nadezhda and Troshin, Sergey},
	urldate = {2021-07-12},
	date = {2021-06-24},
	eprinttype = {arxiv},
	eprint = {2010.07987},
	keywords = {⚠️ Invalid {DOI}, uni, \_tablet},
	file = {Chirkova_Troshin_2021_Empirical Study of Transformers for Source Code.pdf:/data/zotero/storage/4TN5LTGE/Chirkova_Troshin_2021_Empirical Study of Transformers for Source Code.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/AURIS8CX/2010.html:text/html}
}

@article{roziere_dobf_2021,
	title = {{DOBF}: A Deobfuscation Pre-Training Objective for Programming Languages},
	url = {http://arxiv.org/abs/2102.07492},
	shorttitle = {{DOBF}},
	abstract = {Recent advances in self-supervised learning have dramatically improved the state of the art on a wide variety of tasks. However, research in language model pre-training has mostly focused on natural languages, and it is unclear whether models like {BERT} and its variants provide the best pre-training when applied to other modalities, such as source code. In this paper, we introduce a new pre-training objective, {DOBF}, that leverages the structural aspect of programming languages and pre-trains a model to recover the original version of obfuscated source code. We show that models pre-trained with {DOBF} significantly outperform existing approaches on multiple downstream tasks, providing relative improvements of up to 13\% in unsupervised code translation, and 24\% in natural language code search. Incidentally, we found that our pre-trained model is able to de-obfuscate fully obfuscated source files, and to suggest descriptive variable names.},
	journaltitle = {{arXiv}:2102.07492 [cs]},
	author = {Roziere, Baptiste and Lachaux, Marie-Anne and Szafraniec, Marc and Lample, Guillaume},
	urldate = {2021-07-12},
	date = {2021-02-16},
	eprinttype = {arxiv},
	eprint = {2102.07492},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Roziere et al_2021_DOBF.pdf:/data/zotero/storage/9BNZJ6TN/Roziere et al_2021_DOBF.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/2S49XJS9/2102.html:text/html}
}

@article{schuster_you_2020,
	title = {You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion},
	url = {http://arxiv.org/abs/2007.02220},
	shorttitle = {You Autocomplete Me},
	abstract = {Code autocompletion is an integral feature of modern code editors and {IDEs}. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. We demonstrate that neural code autocompleters are vulnerable to poisoning attacks. By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), or else by directly fine-tuning the autocompleter on these files (model poisoning), the attacker can influence its suggestions for attacker-chosen contexts. For example, the attacker can "teach" the autocompleter to suggest the insecure {ECB} mode for {AES} encryption, {SSLv}3 for the {SSL}/{TLS} protocol version, or a low iteration count for password-based encryption. Moreover, we show that these attacks can be targeted: an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and {GPT}-2. We then evaluate existing defenses against poisoning attacks and show that they are largely ineffective.},
	journaltitle = {{arXiv}:2007.02220 [cs]},
	author = {Schuster, Roei and Song, Congzheng and Tromer, Eran and Shmatikov, Vitaly},
	urldate = {2021-07-12},
	date = {2020-10-08},
	eprinttype = {arxiv},
	eprint = {2007.02220},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Schuster et al_2020_You Autocomplete Me.pdf:/data/zotero/storage/TDJJJQ5L/Schuster et al_2020_You Autocomplete Me.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/DZ7MGJVD/2007.html:text/html}
}

@article{wainakh_idbench_2021,
	title = {{IdBench}: Evaluating Semantic Representations of Identifier Names in Source Code},
	url = {http://arxiv.org/abs/1910.05177},
	shorttitle = {{IdBench}},
	abstract = {Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents {IdBench}, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use {IdBench} to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.},
	journaltitle = {{arXiv}:1910.05177 [cs, stat]},
	author = {Wainakh, Yaza and Rauf, Moiz and Pradel, Michael},
	urldate = {2021-07-12},
	date = {2021-01-14},
	eprinttype = {arxiv},
	eprint = {1910.05177},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Wainakh et al_2021_IdBench.pdf:/data/zotero/storage/KQ8D62VC/Wainakh et al_2021_IdBench.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/GL4NJPPB/1910.html:text/html}
}

@article{rabin_memorization_2021,
	title = {Memorization and Generalization in Neural Code Intelligence Models},
	url = {http://arxiv.org/abs/2106.08704},
	abstract = {Deep Neural Networks ({DNN}) are increasingly commonly used in software engineering and code intelligence tasks. These are powerful tools that are capable of learning highly generalizable patterns from large datasets through millions of parameters. At the same time, training {DNNs} means walking a knife's edges, because their large capacity also renders them prone to memorizing data points. While traditionally thought of as an aspect of over-training, recent work suggests that the memorization risk manifests especially strongly when the training datasets are noisy and memorization is the only recourse. Unfortunately, most code intelligence tasks rely on rather noise-prone and repetitive data sources, such as {GitHub}, which, due to their sheer size, cannot be manually inspected and evaluated. We evaluate the memorization and generalization tendencies in neural code intelligence models through a case study across several benchmarks and model families by leveraging established approaches from other fields that use {DNNs}, such as introducing targeted noise into the training dataset. In addition to reinforcing prior general findings about the extent of memorization in {DNNs}, our results shed light on the impact of noisy dataset in training.},
	journaltitle = {{arXiv}:2106.08704 [cs]},
	author = {Rabin, Md Rafiqul Islam and Hussain, Aftab and Hellendoorn, Vincent J. and Alipour, Mohammad Amin},
	urldate = {2021-07-12},
	date = {2021-06-16},
	eprinttype = {arxiv},
	eprint = {2106.08704},
	keywords = {⛔ No {DOI} found, uni, \_tablet},
	file = {Rabin et al_2021_Memorization and Generalization in Neural Code Intelligence Models.pdf:/data/zotero/storage/EYX4SHDA/Rabin et al_2021_Memorization and Generalization in Neural Code Intelligence Models.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/5TKFHTAW/2106.html:text/html}
}

@article{lin_deep_2021,
	title = {Deep neural-based vulnerability discovery demystified: data, model and performance},
	issn = {1433-3058},
	url = {10/gj3nxh},
	doi = {10/gj3nxh},
	shorttitle = {Deep neural-based vulnerability discovery demystified},
	abstract = {Detecting source-code level vulnerabilities at the development phase is a cost-effective solution to prevent potential attacks from happening at the software deployment stage. Many machine learning, including deep learning-based solutions, have been proposed to aid the process of vulnerability discovery. However, these approaches were mainly evaluated on self-constructed/-collected datasets. It is difficult to evaluate the effectiveness of proposed approaches due to lacking a unified baseline dataset. To bridge this gap, we construct a function-level vulnerability dataset from scratch, providing in source-code-label pairs. To evaluate the constructed dataset, a function-level vulnerability detection framework is built to incorporate six mainstream neural network models as vulnerability detectors. We perform experiments to investigate the performance behaviors of the neural model-based detectors using source code as raw input with continuous Bag-of-Words neural embeddings. Empirical results reveal that the variants of recurrent neural networks and convolutional neural network perform well on our dataset, as the former is capable of handling contextual information and the latter learns features from small context windows. In terms of generalization ability, the fully connected network outperforms the other network architectures. The performance evaluation can serve as a reference benchmark for neural model-based vulnerability detection at function-level granularity. Our dataset can serve as ground truth for {ML}-based function-level vulnerability detection and a baseline for evaluating relevant approaches.},
	journaltitle = {Neural Computing and Applications},
	shortjournal = {Neural Comput \& Applic},
	author = {Lin, Guanjun and Xiao, Wei and Zhang, Leo Yu and Gao, Shang and Tai, Yonghang and Zhang, Jun},
	urldate = {2021-09-08},
	date = {2021-05-17},
	langid = {english},
	keywords = {uni},
	file = {Lin et al_2021_Deep neural-based vulnerability discovery demystified.pdf:/data/zotero/storage/JZ55ZWMU/Lin et al_2021_Deep neural-based vulnerability discovery demystified.pdf:application/pdf}
}

@inproceedings{zhuang_smart_2020,
	title = {Smart Contract Vulnerability Detection using Graph Neural Network.},
	pages = {3283--3290},
	booktitle = {{IJCAI}},
	author = {Zhuang, Yuan and Liu, Zhenguang and Qian, Peng and Liu, Qi and Wang, Xiang and He, Qinming},
	date = {2020},
	keywords = {⛔ No {DOI} found},
	file = {Zhuang et al_2020_Smart Contract Vulnerability Detection using Graph Neural Network.pdf:/data/zotero/storage/ET5247G9/Zhuang et al_2020_Smart Contract Vulnerability Detection using Graph Neural Network.pdf:application/pdf}
}

@article{liu_smart_2021,
	title = {Smart Contract Vulnerability Detection: From Pure Neural Network to Interpretable Graph Feature and Expert Pattern Fusion},
	shorttitle = {Smart Contract Vulnerability Detection},
	journaltitle = {{arXiv} preprint {arXiv}:2106.09282},
	author = {Liu, Zhenguang and Qian, Peng and Wang, Xiang and Zhu, Lei and He, Qinming and Ji, Shouling},
	date = {2021},
	keywords = {⛔ No {DOI} found},
	file = {Liu et al_2021_Smart Contract Vulnerability Detection.pdf:/data/zotero/storage/G6J86IRK/Liu et al_2021_Smart Contract Vulnerability Detection.pdf:application/pdf;Snapshot:/data/zotero/storage/XRI7DX7Y/2106.html:text/html}
}

@article{liu_combining_2021,
	title = {Combining graph neural networks with expert knowledge for smart contract vulnerability detection},
	doi = {10/gmvtg6},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Liu, Zhenguang and Qian, Peng and Wang, Xiaoyang and Zhuang, Yuan and Qiu, Lin and Wang, Xun},
	date = {2021},
	note = {Publisher: {IEEE}},
	file = {Liu et al_2021_Combining graph neural networks with expert knowledge for smart contract.pdf:/data/zotero/storage/828DRQSD/Liu et al_2021_Combining graph neural networks with expert knowledge for smart contract.pdf:application/pdf;Snapshot:/data/zotero/storage/D2DM5HV9/9477066.html:text/html}
}

@article{croft_data_2021,
	title = {Data Preparation for Software Vulnerability Prediction: A Systematic Literature Review},
	url = {http://arxiv.org/abs/2109.05740},
	shorttitle = {Data Preparation for Software Vulnerability Prediction},
	abstract = {Software Vulnerability Prediction ({SVP}) is a data-driven technique for software quality assurance that has recently gained considerable attention in the Software Engineering research community. However, the difficulties of preparing Software Vulnerability ({SV}) related data remains as the main barrier to industrial adoption. Despite this problem, there have been no systematic efforts to analyse the existing {SV} data preparation techniques and challenges. Without such insights, we are unable to overcome the challenges and advance this research domain. Hence, we are motivated to conduct a Systematic Literature Review ({SLR}) of {SVP} research to synthesize and gain an understanding of the data considerations, challenges and solutions that {SVP} researchers provide. From our set of primary studies, we identify the main practices for each data preparation step. We then present a taxonomy of 16 key data challenges relating to six themes, which we further map to six categories of solutions. However, solutions are far from complete, and there are several ill-considered issues. We also provide recommendations for future areas of {SV} data research. Our findings help illuminate the key {SV} data practices and considerations for {SVP} researchers and practitioners, as well as inform the validity of the current {SVP} approaches.},
	journaltitle = {{arXiv}:2109.05740 [cs]},
	author = {Croft, Roland and Xie, Yongzheng and Babar, M. Ali},
	urldate = {2021-09-23},
	date = {2021-09-13},
	eprinttype = {arxiv},
	eprint = {2109.05740},
	keywords = {⛔ No {DOI} found, uni},
	file = {Croft et al_2021_Data Preparation for Software Vulnerability Prediction.pdf:/data/zotero/storage/AZQGJNP5/Croft et al_2021_Data Preparation for Software Vulnerability Prediction.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/AS8GTX4K/2109.html:text/html}
}

@inproceedings{li_acgvd_2021,
	location = {Cham},
	title = {{ACGVD}: Vulnerability Detection Based on Comprehensive Graph via Graph Neural Network with Attention},
	isbn = {978-3-030-86890-1},
	url = {10/gmwd22},
	doi = {10/gmwd22},
	series = {Lecture Notes in Computer Science},
	shorttitle = {{ACGVD}},
	abstract = {Vulnerability is one of the main causes of network intrusion. An effective way to mitigate security threats is to find and repair vulnerabilities as soon as possible. Traditional vulnerability detection methods are limited by expert knowledge. Existing deep learning-based methods neglect the connection between semantic graphs and cannot effectively deal with the structure information. Graph neural network brings new insight into vulnerability detection. However, benign nodes on the graph account for a large proportion, resulting in vulnerability information could be disturbed by them. To address the limitations of existing vulnerability detection approaches, in this paper, we propose {ACGVD}, a vulnerability detection method by constructing a graph network with attention. We first combine multiple semantic graphs together to form a more comprehensive graph. We then adopt the Graph neural network instead of the sequence-based model to automatically analyze the comprehensive graph. In order to solve the problem that the vulnerability information could be covered up, we add a double-level attention mechanism to the graph model. We also add a novel classification layer to extract the high-level features of the code. To make the experiment more realistic, the model is trained over the latest published real-world dataset. The experiment results demonstrate that compared with state-of-the-art methods, our model {ACGVD} achieves 5.01\%, 13.89\%, and 8.27\% improvement in accuracy, recall and F1-score, respectively.},
	pages = {243--259},
	booktitle = {Information and Communications Security},
	publisher = {Springer International Publishing},
	author = {Li, Min and Li, Chunfang and Li, Shuailou and Wu, Yanna and Zhang, Boyang and Wen, Yu},
	editor = {Gao, Debin and Li, Qi and Guan, Xiaohong and Liao, Xiaofeng},
	date = {2021},
	langid = {english},
	keywords = {uni},
	file = {Li et al_2021_ACGVD.pdf:/data/zotero/storage/6ZU7RMQF/Li et al_2021_ACGVD.pdf:application/pdf}
}

@article{coimbra_using_2021,
	title = {On using distributed representations of source code for the detection of C security vulnerabilities},
	url = {http://arxiv.org/abs/2106.01367},
	abstract = {This paper presents an evaluation of the code representation model Code2vec when trained on the task of detecting security vulnerabilities in C source code. We leverage the open-source library astminer to extract path-contexts from the abstract syntax trees of a corpus of labeled C functions. Code2vec is trained on the resulting path-contexts with the task of classifying a function as vulnerable or non-vulnerable. Using the {CodeXGLUE} benchmark, we show that the accuracy of Code2vec for this task is comparable to simple transformer-based methods such as pre-trained {RoBERTa}, and outperforms more naive {NLP}-based methods. We achieved an accuracy of 61.43\% while maintaining low computational requirements relative to larger models.},
	journaltitle = {{arXiv}:2106.01367 [cs]},
	author = {Coimbra, David and Reis, Sofia and Abreu, Rui and Păsăreanu, Corina and Erdogmus, Hakan},
	urldate = {2021-09-23},
	date = {2021-06-01},
	eprinttype = {arxiv},
	eprint = {2106.01367},
	keywords = {⛔ No {DOI} found, uni},
	file = {Coimbra et al_2021_On using distributed representations of source code for the detection of C.pdf:/data/zotero/storage/I9FYNY6L/Coimbra et al_2021_On using distributed representations of source code for the detection of C.pdf:application/pdf;arXiv.org Snapshot:/data/zotero/storage/BAXBAE38/2106.html:text/html}
}

@article{sun_vdsimilar_2021,
	title = {{VDSimilar}: Vulnerability detection based on code similarity of vulnerabilities and patches},
	volume = {110},
	issn = {0167-4048},
	url = {10/gmwd3s},
	doi = {10/gmwd3s},
	shorttitle = {{VDSimilar}},
	abstract = {Vulnerability detection using machine learning is a hot topic in improving software security. However, existing works formulate detection as a classification problem, which requires a large set of labelled data while capturing semantical and syntactic similarity. In this work, we argue that similarity in the view of vulnerability is the key in detecting vulnerabilities. We prepare a relatively smaller data set composed of both vulnerabilities and associated patches, and attempt to realize security similarity from (i) the similarity between pair of vulnerabilities and (ii) the difference between a pair of vulnerability and patch. To achieve this, we setup the detection model using the Siamese network cooperated with {BiLSTM} and Attention to deal with source code, Attention network to improve the detection accuracy. On a data set of 876 vulnerabilities and patches of {OpenSSL} and Linux, the proposed model ({VDSimilar}) achieves about 97.17\% in {AUC} value of {OpenSSL} (where the Attention network contributes 1.21\% than {BiLSTM} in Siamese), which is more outstanding than the most advanced methods based on deep learning.},
	pages = {102417},
	journaltitle = {Computers \& Security},
	shortjournal = {Computers \& Security},
	author = {Sun, Hao and Cui, Lei and Li, Lun and Ding, Zhenquan and Hao, Zhiyu and Cui, Jiancong and Liu, Peng},
	urldate = {2021-09-23},
	date = {2021-11-01},
	langid = {english},
	keywords = {uni},
	file = {ScienceDirect Snapshot:/data/zotero/storage/JHU7RHLC/S0167404821002418.html:text/html}
}